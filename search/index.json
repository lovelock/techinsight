[{"content":"引言 在流处理领域，Apache Flink 以其强一致性（Exactly-Once）闻名。但在并行度高达 2000+ 的大规模工业场景中，盲目追求极致一致性会导致吞吐量骤降、作业“假死”。本文将结合生产一线调优经验，深度探讨 Flink 在检查点（Checkpoint）、延迟表现及故障恢复中的权衡细节。\n一、 性能死结：2000 并行度下的“全连接阻塞” 当并行度从 40 扩展到 2000 时，性能损耗并非线性增加，而是呈几何倍数增长，其核心原因在于 Shuffle 网络栈与屏障对齐（Barrier Alignment） 的耦合。\n1. 全连接（All-to-All）的网络压力 在使用 keyBy 或 rebalance 时，Flink 会构建一个 $2000 \\times 2000$ 的逻辑网络连接。这意味着每个下游 Task 都在同时处理来自 2000 个上游通道的输入。\nExactly-Once 的代价： 在 EO 模式下，算子必须集齐全部 2000 个上游的 Barrier 才能触发快照。 木桶效应的极限： 只要 2000 个并行 Subtask 中有 1 个 发生毫秒级的 GC 或网络抖动，下游算子的所有通道都会因等待对齐而被迫缓存数据到内存（Buffer 积压）。 2. 架构降级：走向 At-Least-Once 切换至 At-Least-Once (ALO) 后，Flink 内部发生了本质变化：\n首位触发机制： 算子不再等待 2000 个 Barrier 全数到齐，而是收到 第 1 个 Barrier 立即开始异步快照。 零阻塞处理： 快照期间，算子照常消费所有通道的数据，彻底消除了由于“等待对齐”导致的 CPU 空转。 二、 监控幻象：锯齿状 Lag 与位移提交细节 在监控面板上看到的 15s 周期性 Lag 锯齿，本质上是 异步快照机制与 Kafka Offset 提交逻辑 的时间差。\n1. 提交触发的链路细节 Flink 对 Kafka 位移的提交严格遵循以下序列：\nBarrier 抵达： Source 算子记录当前各 Partition 的 Offset。 异步上传： 各个 Task 将状态（含 Offset）写入 HDFS/S3。 JobManager 确认： 只有当 所有 2000 个 Task 都汇报 CP 成功后，JM 才会向 Source 发出 notifyCheckpointComplete 指令。 Kafka Commit： 收到指令后，Source 才会真正执行 consumer.commitOffsets()。 2. 为什么会有 15s 的锯齿？ 因为 Kafka Broker 侧的监控（如 CMAK）只能看到第 4 步发生的物理提交。如果你的 CP 间隔是 15s，即便数据在第 1s 就被 Flink 处理完了，Kafka 监控也会显示 Lag 持续增长了 14s，直到下一轮提交。\n深度指标： 应关注 currentFetchEventTimeLag，它记录的是数据被读入 Flink 内存的即时延迟，而非汇报延迟。 三、 状态存续：内存攒批与故障恢复的权衡 为了控制下游 MySQL 或 Kafka 的写入频率，我们通常在 processElement 中实现 1s/100条 的攒批逻辑。但 2000 并行度下，如何确保这批数据在崩溃时不丢失？\n1. 托管状态：ListState 的底层保证 直接使用 Java ArrayList 会导致 Failover 时数据丢失。正确的姿势是实现 CheckpointedFunction：\n快照阶段（snapshotState）： 将内存 ArrayList 的数据 Copy 到 Flink 托管的 ListState。 恢复阶段（initializeState）： 如果作业重启，Flink 会自动从检查点拉回这部分数据，重新填充内存列表。 状态分布： 在 2000 并行度下，这种状态是典型的 Operator State，它不依赖 keyBy，在扩缩容时会均匀地在 Subtask 间重新分配。 2. 恢复一致性分析 在 ALO 模式下，故障恢复后的行为特征如下：\n不丢数据： 依靠 ListState 找回了快照时刻积压在内存的 100 条数据。 数据重复： 既然是 ALO，Source 会回溯到上一个快照的 Offset。那些在快照触发后、崩溃发生前已经发往下游的数据，会被再次处理。 对策： 下游 Sink（如 MySQL）需通过 INSERT ... ON DUPLICATE KEY UPDATE 实现幂等，从而在 ALO 的高性能基础上获得最终一致性。 四、 2000 并行度下的最佳实践建议 配置调优： CheckpointingMode.AT_LEAST_ONCE（必选）。 setMinPauseBetweenCheckpoints(10s)：确保在 2000 个并行任务写入 IO 后，给 HDFS 和 CPU 留出恢复时间。 分发策略： 弃用 keyBy 转向 rebalance()，消除哈希计算开销，解决大 Key 倾斜。 Sink 优化： Kafka Sink 应调整 linger.ms 和 batch.size，让客户端在物理层攒批，而非仅依靠 Flink 算子。 结语 分布式系统的优化是一场关于“透明性”的博弈。通过舍弃昂贵的“强一致性屏障”，利用托管状态保证“不丢”，并配合下游幂等实现“不乱”，我们才能在 2000 并行度的重载下，构建出既实时又稳定的流处理系统。\n","date":"2026-02-24T17:24:49+08:00","permalink":"https://techinsight.pages.dev/p/%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%84-flink2000-%E5%B9%B6%E8%A1%8C%E5%BA%A6%E4%B8%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%BB%B6%E8%BF%9F%E4%B8%8E%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%9A%E5%BC%88/","title":"深度解构 Flink：2000 并行度下的性能、延迟与一致性博弈"},{"content":"在计算机世界里，“1+1=2”是所有人公认的规则，但多字节数据该如何存进内存，却曾让不同厂商“各执一词”。这个看似微小的差异，就是字节序（Endianness）——它像CPU的“方言”，虽不常被用户感知，却在网络传输、文件解析、硬件交互中扮演着关键角色。本文将从本质、历史、实践到应用，彻底讲透字节序的来龙去脉。\n一、字节序是什么？多字节数据的“排列法则” 字节序的核心，是多字节数据（如16位整数、32位浮点数）在内存中的存储顺序。计算机最小存储单位是字节（8位），但单个字节只能表示0-255的数值，要存储更大的数据（如65535），就需要多个字节组合。此时，“先存高位字节还是低位字节”，就成了字节序要解决的问题。\n目前主流的字节序分为两类：\n小端序（Little-Endian）：“低位字节在前”，即数值的低位字节存到内存的低地址，高位字节存到高地址。\n例：16位整数0x1234（十进制4660），小端序下内存布局为[0x34, 0x12]（低地址存低位0x34，高地址存高位0x12）。 大端序（Big-Endian）：“高位字节在前”，即数值的高位字节存到内存的低地址，低位字节存到高地址。\n例：同样是0x1234，大端序下内存布局为[0x12, 0x34]（低地址存高位0x12，高地址存低位0x34）。 举个生活化的例子：如果把“1234”看作一个4字节数据，小端序会按“34 12”的顺序写在纸上，大端序则按“12 34”的顺序写——两种写法的“数值”相同，但“排列顺序”完全相反。\n二、字节序的演变：从硬件差异到行业妥协 字节序并非“技术选择”，而是早期计算机硬件厂商的“路径依赖”。它的演变史，本质是不同架构CPU的“话语权争夺”与后期行业的“标准化妥协”。\n1. 起源：CPU架构的“各自为战” 字节序的分歧，最早源于20世纪70年代的CPU设计：\n小端序的崛起：英特尔（Intel）在1978年推出的8086处理器，首次采用小端序。当时8086需要兼容更早的8位处理器8080，小端序能让8位程序直接访问16位数据的低位字节，减少兼容性成本。后续x86架构（如386、奔腾）延续了这一设计，而AMD的x86_64架构也兼容小端序，最终让小端序成为PC、服务器的主流（目前全球90%以上的个人计算机和服务器采用x86/x86_64架构）。 大端序的坚守：同期的摩托罗拉（Motorola）68000处理器（用于早期苹果Mac、工作站）、IBM的System/360大型机，以及后来的PowerPC架构（早期Mac、游戏主机），均采用大端序。大端序的优势是“符合人类阅读习惯”——数值的高位在前，与我们书写“1234”的顺序一致，早期在嵌入式设备、通信设备中应用广泛。 2. 现状：小端为主，大端局部留存 随着x86架构的普及，小端序成为消费级硬件的绝对主流，但大端序并未消失：\n嵌入式领域：部分ARM架构的芯片（如用于路由器、物联网设备的ARM Cortex-M系列）支持大端序模式； 专用设备：大型机（如IBM zSeries）、部分网络设备仍沿用大端序； 标准化场景：网络传输、文件格式（如JPEG、PNG）、数据存储（如数据库），均以大端序为“标准字节序”，避免跨设备解析错误。 三、用Rust理解字节序：从检测到转换的实践 Rust作为一门“注重安全性与跨平台性”的语言，不仅提供了检测系统字节序的工具，还封装了字节序转换的标准方法，能让我们直观看到字节序的差异。\n1. 第一步：检测当前系统的字节序 Rust通过cfg!(target_endian)宏直接判断系统字节序，无需复杂的内存操作：\n1 2 3 4 5 6 7 8 fn main() { println!(\u0026#34;当前系统字节序：\u0026#34;); match cfg!(target_endian) { \u0026#34;little\u0026#34; =\u0026gt; println!(\u0026#34;✅ 小端序（x86/x86_64架构常见）\u0026#34;), \u0026#34;big\u0026#34; =\u0026gt; println!(\u0026#34;✅ 大端序（部分ARM/大型机常见）\u0026#34;), _ =\u0026gt; println!(\u0026#34;❓ 未知字节序（极罕见）\u0026#34;), } } 在PC上运行会输出“小端序”，而在支持大端序的ARM设备上则输出“大端序”。\n2. 第二步：观察内存中的字节布局 要看到“数值在内存中的真实排列”，可以通过std::mem::transmute将整数转换为字节数组（注意：transmute是 unsafe 操作，需确保类型大小一致）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 use std::mem; fn main() { // 定义一个16位整数：0x12（高位）、0x34（低位） let num: u16 = 0x1234; println!(\u0026#34;原始数值：0x{:X}（十进制：{}）\u0026#34;, num, num); // 将整数转换为字节数组，查看内存布局 let bytes: [u8; 2] = unsafe { mem::transmute(num) }; println!(\u0026#34;内存中的字节排列：[0x{:X}, 0x{:X}]\u0026#34;, bytes[0], bytes[1]); // 解释布局含义 if cfg!(target_endian = \u0026#34;little\u0026#34;) { println!(\u0026#34;📌 小端序：低位字节0x34存低地址（数组第1位），高位字节0x12存高地址（数组第2位）\u0026#34;); } else { println!(\u0026#34;📌 大端序：高位字节0x12存低地址（数组第1位），低位字节0x34存高地址（数组第2位）\u0026#34;); } } 在PC上运行的输出如下，清晰展示了小端序的“低位在前”：\n1 2 3 原始数值：0x1234（十进制：4660） 内存中的字节排列：[0x34, 0x12] 📌 小端序：低位字节0x34存低地址（数组第1位），高位字节0x12存高地址（数组第2位） 3. 第三步：字节序转换的核心逻辑 Rust标准库为所有整数类型（u16/i32/u64等）提供了4个核心方法，解决跨平台数据交互问题：\nto_be()：将当前数值转换为“按大端序存储”的新值； to_le()：将当前数值转换为“按小端序存储”的新值； from_be()：将“大端序存储的数值”转换为主机字节序（当前系统的字节序）； from_le()：将“小端序存储的数值”转换为主机字节序。 下面的代码演示了“小端系统向大端格式转换”的过程，关键是观察“内存布局的变化”：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 use std::mem; fn main() { let original: u16 = 0x1234; println!(\u0026#34;原始数值：0x{:X}\u0026#34;, original); // 转换为大端序格式 let be_num = original.to_be(); // 转换为小端序格式（小端系统上等于原始值） let le_num = original.to_le(); // 打印数值：注意be_num的数值“看似反了”，但内存布局是对的 println!(\u0026#34;大端序转换后（数值）：0x{:X}\u0026#34;, be_num); // 小端系统上输出0x3412 println!(\u0026#34;小端序转换后（数值）：0x{:X}\u0026#34;, le_num); // 小端系统上输出0x1234 // 查看内存布局：be_num的内存是大端序的[0x12, 0x34] let be_bytes: [u8; 2] = unsafe { mem::transmute(be_num) }; println!(\u0026#34;大端序转换后（内存）：[0x{:X}, 0x{:X}]\u0026#34;, be_bytes[0], be_bytes[1]); } 输出结果：\n1 2 3 4 原始数值：0x1234 大端序转换后（数值）：0x3412 小端序转换后（数值）：0x1234 大端序转换后（内存）：[0x12, 0x34] 这里的关键认知是：to_be()的目的不是“修改数值”，而是“让数值的内存布局符合大端序”。小端系统上be_num的数值是0x3412，但它在内存中是[0x12, 0x34]——这正是大端序的标准布局，能被其他设备正确解析。\n四、为什么网络传输必须用大端序？ 既然小端序是主流，为什么TCP/IP、UDP等网络协议，都强制要求用大端序（也称“网络字节序”）传输数据？答案是“避免跨设备的解析混乱”，本质是行业的“标准化妥协”。\n1. 核心原因：消除“方言壁垒” 网络传输的核心是“跨设备通信”——发送方可能是小端序的PC，接收方可能是大端序的路由器；或者发送方是嵌入式设备，接收方是服务器。如果没有统一的字节序标准：\n小端序设备发送0x1234（内存[0x34, 0x12]），大端序设备会按“大端规则”解析为0x3412，数值直接错误； 大端序设备发送0x1234（内存[0x12, 0x34]），小端序设备会按“小端规则”解析为0x3412，同样错误。 而规定“网络传输必须用大端序”后，所有设备都遵循同一规则：\n发送方：无论自身是大端还是小端，先将数据转换为大端序格式； 接收方：收到数据后，先按大端序解析，再转换为自身的字节序。 这样一来，无论设备的“方言”是什么，都能通过“标准字节序”实现正确通信。\n2. 历史选择：大端序的“天然优势” 早期网络设备（如路由器、交换机）多采用大端序架构（如摩托罗拉68000），大端序成为网络协议的默认选择。此外，大端序的“高位在前”符合人类对数值的阅读习惯，便于开发者调试（如抓包时直接读取数值，无需反转字节）。\n虽然现在小端序设备占主流，但“网络字节序为大端”的标准已延续数十年，所有网络协议、编程语言（包括Rust）都已适配这一规则，无需也无法轻易更改。\n结语：字节序——隐藏的“跨平台密码” 字节序是计算机硬件发展的“历史遗留产物”，它看似微小，却深刻影响着跨设备数据交互。理解字节序，不仅能避免网络编程、文件解析中的“诡异bug”，更能让我们看清：计算机世界的“统一”，往往是在“差异”基础上通过标准化实现的。\n对于Rust开发者而言，无需手动处理复杂的字节反转逻辑——标准库的to_be()/from_be()等方法，已为我们封装了所有细节。但背后的原理仍需掌握：当你调用to_be()时，本质是在为数据“贴上网络标准的标签”，确保它能在不同“方言”的设备间顺畅通行。\n","date":"2025-11-02T11:56:37+08:00","permalink":"https://techinsight.pages.dev/p/%E5%AD%97%E8%8A%82%E5%BA%8F%E9%9A%90%E8%97%8F%E5%9C%A8cpu%E6%B7%B1%E5%A4%84%E7%9A%84%E5%AD%98%E5%82%A8%E5%AF%86%E7%A0%81%E4%BB%8E%E5%8E%9F%E7%90%86%E5%88%B0rust%E5%AE%9E%E8%B7%B5/","title":"字节序：隐藏在CPU深处的“存储密码”，从原理到Rust实践"},{"content":"在 Rust 开发中，文件读取是极为常见的操作场景，而文本文件与字节流（二进制文件）的读取逻辑因数据格式的本质差异，存在截然不同的处理思路与注意事项。文本文件以人类可理解的字符序列为核心，需关注编码解析与字符完整性；二进制文件则以原始字节为载体，需保障数据结构的精确还原与读写一致性。本文将从底层原理出发，系统剖析两类文件读取的核心注意事项，结合 Rust 标准库 API 与实践案例，为开发者提供全面的操作指南，确保在不同场景下实现安全、高效的文件读取。\n一、读取文本文件：聚焦编码解析与字符完整性 文本文件的本质是 “按特定编码规则存储的字符序列”（如 UTF-8、GBK、UTF-16 等），读取时的核心挑战在于正确解析编码格式与保障字符不被截断。Rust 标准库虽默认以 UTF-8 处理文本，但实际开发中需应对多样编码场景，同时规避因文件损坏、读取不完整导致的字符解析错误。\n1. 编码处理：从默认 UTF-8 到多编码兼容 Rust 标准库（std::fs）的文本读取 API（如read_to_string）默认假设文件为 UTF-8 编码，若文件采用其他编码（如 Windows 系统常见的 GBK、UTF-16），直接读取会触发编码错误，这是文本文件读取的首要注意事项。\n（1）默认 UTF-8 读取的风险与处理 使用std::fs::read_to_string读取非 UTF-8 编码文件时，会返回io::Error（错误类型为InvalidData），导致程序崩溃或数据读取失败。例如，读取 GBK 编码的中文文本文件：\n1 2 3 4 5 6 7 8 9 use std::fs; fn main() { // 读取 GBK 编码的文本文件，默认 UTF-8 解析会失败 match fs::read_to_string(\u0026#34;gbk_text.txt\u0026#34;) { Ok(content) =\u0026gt; println!(\u0026#34;内容：{}\u0026#34;, content), Err(e) =\u0026gt; eprintln!(\u0026#34;读取错误：{}\u0026#34;, e), // 输出 \u0026#34;读取错误：invalid utf-8 sequence of 2 bytes from index 0\u0026#34; } } 注意事项：\n不可想当然地假设文本文件为 UTF-8 编码，尤其是在跨平台场景（如 Windows 生成的文件可能为 GBK 或 UTF-16）或处理第三方文件时，需先明确文件编码格式。\n若无法确定编码，需通过文件头标识（如 UTF-8 BOM、UTF-16 BOM）或外部配置获取编码信息，避免盲目读取。\n（2）多编码支持：借助第三方库实现兼容 Rust 标准库未提供多编码解析能力，需依赖第三方库（如encoding_rs、chardet）实现 GBK、Shift-JIS 等编码的解析。其中encoding_rs是 Rust 生态中主流的编码处理库，支持绝大多数常见编码，且性能高效、无 unsafe 代码。\n实践案例：读取 GBK 编码文本文件\n在Cargo.toml中添加依赖： 1 2 [dependencies] encoding_rs = \u0026#34;0.8\u0026#34; 实现 GBK 编码解析： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 use std::fs; use encoding_rs::GBK; fn read_gbk_file(path: \u0026amp;str) -\u0026gt; Result\u0026lt;String, Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { // 1. 先以字节流形式读取文件（避免编码解析干扰） let bytes = fs::read(path)?; // 2. 使用 GBK 编码解析字节流，忽略无效字节（或根据需求处理） let (content, _, had_errors) = GBK.decode(\u0026amp;bytes); // 3. 处理编码错误（可选：根据业务需求决定是否终止程序） if had_errors { eprintln!(\u0026#34;警告：文件存在无效 GBK 编码字符，已忽略\u0026#34;); } Ok(content.into_owned()) } fn main() { match read_gbk_file(\u0026#34;gbk_text.txt\u0026#34;) { Ok(content) =\u0026gt; println!(\u0026#34;GBK 文本内容：{}\u0026#34;, content), Err(e) =\u0026gt; eprintln!(\u0026#34;读取错误：{}\u0026#34;, e), } } 注意事项：\n解析编码时需明确 “错误处理策略”：是忽略无效字节（如上述案例）、替换为占位符（如�），还是直接返回错误终止程序，需根据业务场景（如日志分析、用户文档读取）决定。\n对于 UTF-16 编码文件（常见于 Windows 系统），需区分 “大端序（UTF-16BE）” 与 “小端序（UTF-16LE）”，可通过文件头的 BOM（字节顺序标记，0xFEFF）自动判断编码端序，避免字符错乱。\n2. 字符完整性：规避部分读取导致的截断问题 文本文件的读取可能因 “部分读取”（如使用read方法读取固定大小缓冲区）导致字符被截断 —— 尤其是 UTF-8 编码中，一个字符可能占用 1~4 字节，若缓冲区恰好截断某个字符的中间字节，会导致后续解析失败。\n（1）避免使用固定缓冲区直接读取文本 Rust 的std::fs::File类型实现了Read trait，其read方法会尝试读取指定大小的字节到缓冲区，但无法保证读取的字节恰好构成完整字符。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 use std::fs::File; use std::io::Read; fn main() { let mut file = File::open(\u0026#34;utf8_text.txt\u0026#34;).unwrap(); let mut buf = [0u8; 3]; // 缓冲区大小为 3 字节（可能截断 UTF-8 字符） let mut content = String::new(); // 部分读取可能导致字符截断 loop { match file.read(\u0026amp;mut buf) { Ok(0) =\u0026gt; break, // 读取结束 Ok(n) =\u0026gt; { // 若 buf[0..n] 恰好截断 UTF-8 字符，from_utf8 会报错 let s = String::from_utf8(buf[0..n].to_vec()).unwrap(); content.push_str(\u0026amp;s); } Err(e) =\u0026gt; panic!(\u0026#34;读取错误：{}\u0026#34;, e), } } } 上述代码中，若文件包含占用 4 字节的 UTF-8 字符（如 emoji 表情😀），缓冲区大小为 3 字节时，会截断该字符的最后 1 字节，导致from_utf8调用失败。\n注意事项：\n除非明确知道文本文件的编码为 “固定宽度编码”（如 UTF-32），否则不要使用read方法结合固定缓冲区读取文本，应优先使用专为文本设计的 API。 （2）使用文本友好型 API 保障字符完整性 Rust 标准库与第三方库提供了多种 “字符感知” 的文本读取 API，可自动处理字符截断问题，核心包括：\nstd::fs::read_to_string：一次性读取整个文件到字符串，内部自动处理 UTF-8 编码与字符完整性，适合中小型文本文件（避免内存溢出）。\nstd::io::BufRead trait：提供按行读取（lines方法）或按字符读取（chars方法）的能力，内部通过缓冲区自动缓存不完整字符，确保每次读取的单元（行 / 字符）完整。\n实践案例：按行读取大型文本文件\n对于大型文本文件（如日志文件、CSV 数据），一次性读取会占用大量内存，此时应使用BufRead::lines按行读取，既保障字符完整性，又降低内存开销：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 use std::fs::File; use std::io::{BufRead, BufReader}; fn read_large_text_file(path: \u0026amp;str) -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let file = File::open(path)?; let reader = BufReader::new(file); // 包装为带缓冲区的读取器（默认缓冲区大小 8KB，可自定义） // 按行读取，自动处理换行符与字符完整性 for (line_num, line_result) in reader.lines().enumerate() { let line = line_result?; // 处理可能的 IO 错误（如文件中途损坏） println!(\u0026#34;第 {} 行：{}\u0026#34;, line_num + 1, line); } Ok(()) } fn main() { if let Err(e) = read_large_text_file(\u0026#34;large_log.txt\u0026#34;) { eprintln!(\u0026#34;读取错误：{}\u0026#34;, e); } } 注意事项：\nBufRead::lines会自动忽略行尾的换行符（\\n或\\r\\n），若需保留原始换行符，需使用read_line方法手动处理。\n对于超大型文件（如 GB 级），需注意内存占用：BufReader的缓冲区大小默认为 8KB，可通过BufReader::with_capacity调整（如设置为 64KB），平衡 IO 次数与内存开销。\n3. 其他关键注意事项 （1）文件权限与路径处理 读取文件前需确保程序拥有文件的 “读权限”，否则会返回PermissionDenied错误。在跨平台场景中，需注意不同系统的权限模型（如 Linux 的rwx权限、Windows 的 ACL 权限）。\n使用std::path::Path或PathBuf处理文件路径，避免硬编码字符串路径（如\u0026quot;C:\\\\text.txt\u0026quot;仅适用于 Windows，\u0026quot;/home/text.txt\u0026quot;仅适用于 Linux），确保路径解析的跨平台兼容性。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 use std::path::Path; use std::fs; fn main() { let path = Path::new(\u0026#34;text.txt\u0026#34;); // 跨平台路径表示 if path.exists() \u0026amp;\u0026amp; path.is_file() { // 先检查文件是否存在且为普通文件 let content = fs::read_to_string(path).unwrap(); println!(\u0026#34;内容：{}\u0026#34;, content); } else { eprintln!(\u0026#34;错误：文件不存在或不是普通文件\u0026#34;); } } （2）文件编码标识的处理 部分文本文件会在开头添加 “编码标识”（如 UTF-8 BOM、UTF-16 BOM），虽然 Rust 标准库的read_to_string会自动忽略 UTF-8 BOM（0xEFBBBF），但其他编码的 BOM 需手动处理（如 UTF-16 BOM 用于判断端序）。\n若文件无编码标识，且无法通过外部信息确定编码，可使用chardet库自动检测编码（基于字节频率分析），但检测结果存在一定误差，需在关键场景中结合人工确认。\n二、读取字节流（二进制文件）：聚焦数据结构还原与读写一致性 字节流（二进制文件）的本质是 “按特定格式存储的原始字节序列”（如图片、音频、自定义二进制协议数据），读取时的核心挑战在于精确还原数据结构与保障字节级别的完整性。与文本文件不同，二进制文件的读取无需编码解析，但需严格遵循数据的存储格式（如结构体对齐、字节序），否则会导致数据解析错误。\n1. 数据结构对齐与字节序：避免内存布局不匹配 二进制文件中的数据通常按特定 “结构体” 格式存储（如自定义的Header、Record结构），而 Rust 结构体的内存布局可能因 “对齐优化” 与文件中的存储格式不一致，直接使用std::mem::transmute或read_exact读取结构体可能导致数据错乱。\n（1）结构体对齐的问题与解决 Rust 编译器会自动对结构体成员进行内存对齐（如i32成员通常对齐到 4 字节边界），以提升访问性能，但二进制文件中的数据可能按 “紧凑格式” 存储（无对齐填充字节），此时直接映射结构体将读取到无效的填充字节。例如：\n1 2 3 4 5 6 7 8 9 10 // Rust 中的结构体（存在对齐填充） #[derive(Debug)] struct FileHeader { magic: u16, // 2 字节 version: u32, // 4 字节（编译器会在 magic 后添加 2 字节填充，使 version 对齐到 4 字节边界） length: u64, // 8 字节 } // 二进制文件中实际的存储格式（紧凑，无填充）：magic(2) + version(4) + length(8) = 14 字节 // 而 Rust 结构体的大小为 2 + 2（填充） + 4 + 8 = 16 字节，直接读取会导致 version 数据错误 注意事项：\n不可直接将二进制文件的字节序列强制转换为 Rust 结构体，需禁用结构体的自动对齐优化，或手动按字段顺序读取字节并转换。 （2）禁用自动对齐：使用repr(packed)属性 通过#[repr(packed)]属性可强制 Rust 结构体按 “紧凑格式” 布局，消除填充字节，使其与二进制文件的存储格式一致。但需注意，repr(packed)会禁用内存对齐，可能导致访问性能下降，且需使用unsafe代码读取（因未对齐内存访问在 Rust 中属于未定义行为）。\n实践案例：读取紧凑格式的二进制结构体\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 use std::fs::File; use std::io::Read; use std::mem; // 禁用自动对齐，按紧凑格式布局（与二进制文件一致） #[repr(packed)] #[derive(Debug)] struct FileHeader { magic: u16, version: u32, length: u64, } fn read_file_header(path: \u0026amp;str) -\u0026gt; Result\u0026lt;FileHeader, Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let mut file = File::open(path)?; let mut buf = [0u8; mem::size_of::\u0026lt;FileHeader\u0026gt;()]; // 缓冲区大小 = 结构体大小（14 字节） // 读取恰好足够的字节（若文件长度不足，会返回错误） file.read_exact(\u0026amp;mut buf)?; // unsafe：将字节缓冲区强制转换为结构体（因 repr(packed) 禁用了对齐，需确保安全） let header = unsafe { mem::transmute::\u0026lt;[u8; mem::size_of::\u0026lt;FileHeader\u0026gt;()], FileHeader\u0026gt;(buf) }; Ok(header) } fn main() { match read_file_header(\u0026#34;data.bin\u0026#34;) { Ok(header) =\u0026gt; println!(\u0026#34;文件头：{:?}\u0026#34;, header), Err(e) =\u0026gt; eprintln!(\u0026#34;读取错误：{}\u0026#34;, e), } } 注意事项：\nrepr(packed)仅适用于 “结构体字段类型固定、存储格式紧凑” 的场景，若二进制文件的字段顺序与结构体不一致，仍需手动读取每个字段。\n使用unsafe代码时需格外谨慎，确保缓冲区大小与结构体大小完全一致，且文件中的数据格式与结构体定义匹配，避免内存越界或数据解析错误。\n（3）处理字节序（大小端）问题 二进制文件中的多字节数据（如u16、u32）可能采用 “大端序（Big-Endian）” 或 “小端序（Little-Endian）” 存储，而 Rust 程序默认使用 “主机字节序”（即运行平台的字节序，如 x86_64 平台为小端序），若文件字节序与主机字节序不一致，直接读取会导致数据错误（如文件中 0x1234 存储为大端序，直接读取为小端序会变成 0x3412）。\n解决方法：使用byteorder库处理字节序\nbyteorder是 Rust 生态中处理字节序的主流库，支持在读取多字节数据时显式指定字节序（大端序 / 小端序），无需手动转换字节。\n在Cargo.toml中添加依赖： 1 2 [dependencies] byteorder = \u0026#34;1.5\u0026#34; 实践案例：读取大端序的二进制数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 use std::fs::File; use std::io::{Read, Seek, SeekFrom}; use byteorder::{BigEndian, ReadBytesExt}; #[derive(Debug)] struct SensorData { id: u16, // 大端序存储 temperature: f32, // 大端序存储（IEEE 754 单精度浮点数） timestamp: u64, // 大端序存储 } fn read_sensor_data(path: \u0026amp;str) -\u0026gt; Result\u0026lt;Vec\u0026lt;SensorData\u0026gt;, Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let mut file = File::open(path)?; let file_size = file.seek(SeekFrom::End(0))?; } ","date":"2025-11-01T23:13:20+08:00","permalink":"https://techinsight.pages.dev/p/%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90-rust-%E4%B8%AD%E8%AF%BB%E5%8F%96%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E4%B8%8E%E5%AD%97%E8%8A%82%E6%B5%81%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%E7%9A%84%E6%A0%B8%E5%BF%83%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","title":"深度解析 Rust 中读取文本文件与字节流（二进制文件）的核心注意事项"},{"content":"在 Rust 的集合类型体系中，Array（数组）、Vector（向量）与 Slice（切片）是承载连续数据的核心结构。三者看似都用于存储有序元素，却因底层设计、所有权模型与使用场景的差异，呈现出截然不同的特性。对于 Rust 开发者而言，理解它们的本质区别与关联，不仅是写出高效代码的关键，更是掌握 Rust 内存安全与性能平衡设计哲学的核心。本文将从底层实现出发，逐层剖析三者的特性、交互逻辑与适用场景，带你构建对 Rust 连续数据结构的深度认知。\n一、Array：编译期确定大小的 “静态容器” Array（数组）是 Rust 中最基础的连续数据结构，其设计核心是 “编译期固定大小”，这一特性决定了它的内存布局、所有权模型与使用边界。\n1. 底层本质：栈上的固定大小连续内存块 在 Rust 中，Array 的类型表示为[T; N]，其中T是元素类型，N是一个非负整数常量（必须在编译期确定）。其底层实现是一段连续的内存空间，直接分配在栈上（而非堆上），元素紧密排列，无额外内存开销（如长度、容量指针等）。例如，let arr: [i32; 3] = [1, 2, 3];在栈上的内存布局为 3 个i32类型的连续空间，总大小为3 * 4 = 12字节（假设i32为 4 字节）。\n这种 “栈上固定大小” 的设计带来两个关键特性：\n内存高效：无额外元数据，内存利用率达到 100%，适合存储小规模、固定数量的数据（如 RGB 颜色值[u8; 3]、矩阵的行[f64; 4]）。\n编译期安全检查：由于大小固定，编译器可在编译期验证数组访问的合法性。例如，arr[5]会直接触发编译错误（索引超出数组长度 3），避免了 C/C++ 中数组越界的运行时风险。\n2. 核心特性：不可变性与所有权的 “简单性” Array 的特性与其 “静态大小” 的设计强绑定，主要体现在不可变性、所有权与方法支持上：\n默认不可变，显式可变：与 Rust 中变量默认不可变一致，Array 默认无法修改元素（如arr[0] = 4会报错），需通过mut关键字显式声明可变数组：let mut arr: [i32; 3] = [1, 2, 3]; arr[0] = 4;。但需注意，数组的大小始终不可变，即使是可变数组，也无法追加、删除元素或改变长度。\n所有权的 “值语义”：Array 是 “拥有型类型”（Owned Type），但其所有权转移遵循 “值语义”—— 当数组被赋值给新变量或传递给函数时，会触发完整的数据复制（而非引用传递）。例如：\n1 2 let arr1 = [1, 2, 3]; let arr2 = arr1; // 复制 arr1 的所有元素到 arr2，arr1 仍可使用（因 i32 实现 Copy 特性） 若数组元素类型未实现Copy特性（如String），则赋值后原数组会失去所有权（无法再使用），这与 Rust 基本的所有权规则一致。\n有限的方法支持：由于大小固定，Array 仅提供少量基础方法（如len()获取长度、iter()生成迭代器），不支持动态集合的操作（如push()、pop()）。若需更灵活的操作，需先将其转换为 Slice 或 Vector。 3. 适用场景：小规模、固定大小的确定性数据 Array 的设计决定了它更适合存储 “数量已知且固定” 的小规模数据，典型场景包括：\n存储固定维度的数据（如 2D 坐标[f64; 2]、RGBA 颜色[u8; 4]）。\n作为函数参数传递少量固定数据（如配置项、状态码列表），避免堆内存分配开销。\n作为底层容器，为 Slice 或 Vector 提供初始数据（如let vec: Vec\u0026lt;i32\u0026gt; = arr.into();将数组转换为向量）。\n二、Vector：堆上动态扩容的 “可变集合” Vector（向量）是 Rust 中最常用的动态连续数据结构，其设计核心是 “堆上存储 + 动态扩容”，弥补了 Array 大小固定的局限性，同时保持了连续内存的访问效率。\n1. 底层实现：堆内存缓冲区 + 栈上元数据 Vector 的类型表示为Vec\u0026lt;T\u0026gt;，其底层由两部分组成：\n栈上元数据：包含三个字段 ——ptr（指向堆内存缓冲区的指针）、len（当前存储的元素数量）、cap（缓冲区的总容量，即最多可存储的元素数量），总大小为3 * usize（在 64 位系统上为 24 字节）。\n堆上缓冲区：一段连续的内存空间，用于存储实际的T类型元素，缓冲区大小由cap决定，且会根据元素的添加 / 删除自动扩容或收缩（按需调整）。\n例如，let mut vec: Vec\u0026lt;i32\u0026gt; = Vec::with_capacity(3); vec.push(1); vec.push(2);的底层状态为：\n栈上元数据：ptr指向堆上 3 个i32大小的缓冲区，len = 2，cap = 3。\n堆上缓冲区：前两个位置存储1和2，第三个位置空闲（待填充）。\n当len达到cap（如继续vec.push(3); vec.push(4);）时，Vector 会触发扩容机制：\n计算新容量（通常为当前容量的 2 倍，避免频繁扩容）。\n在堆上分配一块新的、更大的缓冲区。\n将旧缓冲区的元素复制到新缓冲区。\n释放旧缓冲区的内存，更新ptr和cap字段。\n这种扩容机制保证了push()操作的平均时间复杂度为 O (1)，但需注意：扩容时的复制操作会带来短暂的性能开销，且若元素类型未实现Copy特性，复制过程会触发所有权转移（可能影响性能）。\n2. 核心特性：动态性、所有权与内存安全 Vector 的特性围绕 “动态管理连续内存” 展开，同时遵循 Rust 的内存安全规则：\n完全动态的元素操作：支持push()（尾部添加）、pop()（尾部删除）、insert()（指定位置插入）、remove()（指定位置删除）等操作，可灵活调整元素数量。需注意，insert()和remove()会导致指定位置后的元素 “平移”（内存复制），时间复杂度为 O (n)，因此不适合频繁在中间位置修改数据（此时应优先选择LinkedList或VecDeque）。\n所有权的 “容器语义”：Vector 拥有其堆上缓冲区的所有权，同时也拥有所有元素的所有权。当 Vector 超出作用域时，会先自动销毁所有元素（调用元素的Drop方法），再释放堆上缓冲区的内存，彻底避免内存泄漏。例如，Vec\u0026lt;String\u0026gt;被销毁时，会先逐个销毁内部的String（释放其堆内存），再释放 Vector 自身的堆缓冲区。\n切片化与借用规则：Vector 可通过\u0026amp;vec或vec.as_slice()转换为\u0026amp;[T]（Slice），此时会生成一个指向 Vector 堆缓冲区的 “视图”，且遵循 Rust 的借用规则 —— 若存在\u0026amp;[T]（不可变切片），则 Vector 无法被修改（避免切片指向无效数据）；若存在\u0026amp;mut [T]（可变切片），则 Vector 无法被其他方式借用（避免数据竞争）。\n3. 适用场景：大规模、动态变化的连续数据 Vector 凭借动态性与高效性，成为 Rust 中处理连续数据的 “首选工具”，典型场景包括：\n存储数量不确定的数据（如用户输入列表、日志条目、网络请求响应体）。\n作为动态数组使用，替代其他语言中的ArrayList或std::vector（C++）。\n作为底层容器，为其他数据结构提供连续内存支持（如HashMap的桶数组、VecDeque的环形缓冲区）。\n三、Slice：跨越容器的 “数据视图” Slice（切片）是 Rust 中最特殊的连续数据结构，其设计核心是 “无所有权的视图”—— 它不存储数据，仅指向其他容器（Array、Vector、甚至另一个 Slice）中的一段连续数据，是连接 Array 与 Vector 的 “桥梁”。\n1. 底层本质：指针 + 长度的 “轻量级视图” Slice 的类型表示为\u0026amp;[T]（不可变切片）或\u0026amp;mut [T]（可变切片），其底层是一个 “胖指针”（Fat Pointer），包含两个字段：\nptr：指向目标容器中连续数据的起始位置的指针（与容器数据的内存地址一致）。\nlen：切片包含的元素数量（usize类型，需小于等于目标容器的长度）。\n例如：\n对 Arraylet arr = [1, 2, 3, 4];取切片let slice = \u0026amp;arr[1..3];，切片的ptr指向arr[1]的地址，len = 2（包含2和3）。\n对 Vectorlet vec = vec![1, 2, 3, 4];取切片let slice = \u0026amp;vec[2..];，切片的ptr指向vec堆缓冲区中3的地址，len = 2（包含3和4）。\nSlice 的 “无所有权” 特性意味着：\n切片不管理数据的生命周期，数据的生命周期由其指向的容器（Array/Vector）决定。\n切片的生命周期不能超过目标容器的生命周期（否则会出现 “悬垂切片”，触发编译器错误）。例如：\n1 2 3 4 5 6 let slice: \u0026amp;[i32]; { let arr = [1, 2, 3]; slice = \u0026amp;arr; // 错误：arr 的生命周期在块结束后结束，slice 会成为悬垂引用 } 2. 核心特性：无所有权、借用约束与通用性 Slice 的特性完全围绕 “视图” 角色展开，是 Rust 实现 “泛型连续数据操作” 的关键：\n无所有权与不可复制性：Slice 不拥有数据，因此既不支持Copy特性（无法直接赋值复制），也不支持Clone特性（除非元素类型支持Clone，且需显式调用clone()复制数据，而非切片本身）。切片的传递本质是 “引用传递”，仅复制胖指针（开销极小，64 位系统上为 16 字节），不复制底层数据。\n严格的借用规则：作为引用类型，Slice 遵循 Rust 的借用规则：\n1 2 3 let mut vec = vec![1, 2, 3]; let slice1 = \u0026amp;vec[..]; // 不可变切片 // let slice2 = \u0026amp;mut vec[..]; // 错误：不可变切片与可变切片不能共存 不可变切片（\u0026amp;[T]）：允许同时存在多个不可变切片，但禁止同时存在可变切片或对容器的可变借用（避免数据竞争）。\n可变切片（\u0026amp;mut [T]）：禁止同时存在其他任何切片或借用（可变引用独占数据），确保修改操作的安全性。\n例如：\n泛型操作的 “统一接口”：由于 Slice 可从 Array 和 Vector 中生成，且提供了丰富的通用方法（如sort()、iter()、split()、join()），它成为了 Rust 中 “处理连续数据” 的统一接口。例如，一个接受\u0026amp;[T]参数的函数，既能处理 Array 的切片，也能处理 Vector 的切片，无需为两种容器单独实现： 1 2 3 4 5 6 7 8 9 10 // 计算切片中所有元素的和（泛型函数，支持 Array 和 Vector 的切片） fn sum_slice\u0026lt;T: std::ops::Add\u0026lt;Output = T\u0026gt; + Default\u0026gt;(slice: \u0026amp;[T]) -\u0026gt; T { slice.iter().fold(T::default(), |acc, \u0026amp;x| acc + x) } let arr = [1, 2, 3]; let vec = vec![4, 5, 6]; println!(\u0026#34;Array sum: {}\u0026#34;, sum_slice(\u0026amp;arr)); // 输出 6 println!(\u0026#34;Vector sum: {}\u0026#34;, sum_slice(\u0026amp;vec)); // 输出 15 3. 适用场景：泛化连续数据操作与安全访问 Slice 的 “视图” 角色使其在以下场景中不可或缺：\n函数参数的泛化：当函数需要处理 “连续数据” 但不关心数据的具体容器类型（Array/Vector）时，使用\u0026amp;[T]作为参数，提高函数的通用性（如上述sum_slice函数）。\n安全的部分数据访问：无需复制数据，即可访问容器中的部分元素（如从 Vector 中提取子数组、处理文件的某段字节数据），减少内存开销。\n与其他数据结构的交互：许多 Rust 标准库函数（如std::io::Read::read()、std::fs::read_to_string()）返回或接受 Slice，作为数据传输的 “中间载体”。\n四、三者的关联与差异：一张表看懂核心区别 为了更清晰地对比 Array、Vector 与 Slice 的核心差异，我们通过下表从底层存储、大小特性、所有权、性能等维度进行总结：\n特性维度 Array（[T; N]） Vector（Vec\u0026lt;T\u0026gt;） Slice（\u0026amp;[T]/\u0026amp;mut [T]） 底层存储位置 栈（Stack） 元数据在栈，数据在堆（Heap） 无存储，指向其他容器的内存（栈 / 堆） 大小特性 编译期固定（N为常量） 运行期动态变化（支持扩容 / 收缩） 运行期确定（由切片范围决定） 所有权 拥有数据（值语义，赋值会复制） 拥有数据（容器语义，管理堆内存） 无所有权（仅作为数据视图） 内存开销 无额外开销（仅存储元素） 栈上 3 个usize（元数据） 栈上 2 个usize（胖指针） 元素修改 支持（需mut），但大小不可变 支持动态添加 / 删除 / 修改 不可变切片禁止修改，可变切片支持修改 生命周期 与变量作用域一致（栈内存自动释放） 与变量作用域一致（堆内存自动释放） 依赖目标容器的生命周期（不可独立存在） 典型方法 len()、iter() push()、pop()、insert()、sort() sort()、split()、join()、iter() 适用场景 小规模、固定大小数据 大规模、动态变化数据 泛化连续数据操作、部分数据访问 五、实践指南：如何选择合适的连续数据结构？ 在实际开发中，选择 Array、Vector 还是 Slice，需结合数据规模、变化频率、内存开销等因素综合判断，以下是具体的选择指南：\n优先选择 Array 的场景： 数据数量在编译期已知且固定（如 RGB 颜色、坐标点）。\n数据规模较小（避免栈溢出，栈大小通常有限，如 Linux 默认栈大小为 8MB）。\n需极致的内存效率（无额外元数据开销）或避免堆内存分配（如嵌入式开发、高性能场景）。\n优先选择 Vector 的场景： 数据数量在运行期确定或动态变化（如用户输入列表、日志集合）。\n数据规模较大（堆内存可支持更大的存储容量）。\n需频繁添加 / 删除元素（尤其是尾部操作，push()/pop()效率高）。\n优先选择 Slice 的场景： 编写泛化函数（需同时支持 Array 和 Vector 作为输入）。 ","date":"2025-11-01T22:26:20+08:00","permalink":"https://techinsight.pages.dev/p/%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90-rust-%E4%B8%AD%E7%9A%84-slicearray-%E4%B8%8E-vector%E7%89%B9%E6%80%A7%E5%85%B3%E8%81%94%E4%B8%8E%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0/","title":"深度解析 Rust 中的 Slice、Array 与 Vector：特性、关联与底层实现"},{"content":"对于刚接触 Rust 的开发者而言，字符串类型往往是一道 “入门难关”。不同于许多编程语言中单一的字符串类型，Rust 提供了String、str以及一系列相关衍生类型，它们各自承载着不同的设计目标与使用场景，且内部实现细节时常会影响到实际编码，容易引发类型错误。本文将系统梳理这些字符串类型的核心区别与关联，并结合 C/C++ 字符串处理的痛点，详解 Rust 如何优雅解决 \u0026lsquo;\\0\u0026rsquo;（即 0x00）截断问题。\n一、核心字符串类型：str与String的 “孪生差异” 在 Rust 的字符串体系中，str与String是最基础且最常用的两个类型，二者名字相似却本质不同，理解它们的差异是掌握 Rust 字符串的关键。\n1. str：高性能、不可变的 “字符串切片” str（发音为 “string slice”）是 Rust 中最底层的字符串类型，它本质上是一段UTF-8 编码的字节序列的视图，自身并不存储数据，而是指向内存中已存在的字符串数据（如程序静态区的字面量、String内部的字节缓冲区等）。其核心特性可概括为三点：\n不可变性与高性能：str一旦创建（或指向特定数据），就无法修改其内容。这种不可变性带来了性能优势 —— 当str被复用（如传递给函数、赋值给变量）时，数据不会被复制，仅传递指向数据的 “视图”，减少了内存开销与操作系统的交互，因此运行效率极高。\n通常以引用形式存在（\u0026amp;str）：由于str本身不拥有数据，直接使用str类型的变量（如let s: str = \u0026quot;hello\u0026quot;;）在 Rust 中是不允许的 —— 编译器无法确定其大小（字符串长度不固定）。因此，str几乎总是以不可变引用\u0026amp;str的形式出现，\u0026amp;str也被直接称为 “字符串切片”。\n静态生命周期的字面量：我们在代码中直接写的字符串字面量（如\u0026quot;hello world\u0026quot;），其类型就是\u0026amp;'static str。这里的'static生命周期表示该字符串数据存储在程序的静态内存区（编译时确定，程序运行期间始终存在，且只读），无需开发者手动管理内存生命周期。\n2. String：可修改、拥有所有权的 “动态字符串” 如果说str是 “字符串的视图”，那么String就是 “字符串的所有者”。String是 Rust 提供的可动态修改的字符串类型，其设计更贴近其他编程语言（如 Java、Python）中的字符串，核心特性包括：\n所有权与可变性：String拥有其内部存储的 UTF-8 字节序列（本质上是对Vec\u0026lt;u8\u0026gt;的封装），因此具备可变性 —— 支持追加（push_str）、插入（insert）、截断（truncate）等修改操作，且修改时会自动管理内存（扩容、释放）。同时，作为 “owned type”（拥有型类型），String的生命周期由其所有者决定：当所有者超出作用域（如函数执行结束、变量被销毁）时，String会自动释放其占用的内存，避免内存泄漏。\n与\u0026amp;str的转换关系：String与\u0026amp;str是 Rust 字符串体系中最核心的转换对。由于String内部存储的是 UTF-8 字节序列，我们可以通过\u0026amp;*s（或s.as_str()）将String转换为\u0026amp;str（即获取String内部数据的 “视图”）；反之，若要将\u0026amp;str转换为String，则需要通过to_string()或String::from()方法 —— 这会复制\u0026amp;str指向的数据，生成一个新的String（因为\u0026amp;str不拥有数据，无法直接 “转化” 为拥有数据的String）。\n二、扩展字符串类型：应对特殊场景的 “工具集” 除了str与String，Rust 还提供了一系列针对特殊场景的字符串 / 字节类型，它们各自解决特定领域的问题，共同构成了完整的字符串处理体系。\n1. char：固定宽度的 “单个 Unicode 字符” char并非字符串类型，而是 Rust 中表示 “单个 Unicode 字符” 的类型，其核心特点与字符串类型形成鲜明对比：\n编码与宽度：char采用UCS-4/UTF-32 编码，每个char固定占用 4 个字节，无论其表示的是英文字母（如 \u0026lsquo;a\u0026rsquo;）还是复杂汉字（如 \u0026rsquo; 中\u0026rsquo;）。这与str/String的 UTF-8 编码不同 ——UTF-8 编码下，字符宽度可变（1~4 字节），英文字母占 1 字节，汉字占 3 字节。\n用途与转换代价：char的固定宽度使其更适合需要精确字符操作的场景（如字符遍历、字符匹配），编译器也能更轻松地推断其内存布局。但从str/String转换为char（如通过chars()方法迭代）会产生一定的性能代价 —— 需要解析 UTF-8 的可变宽度字节序列，将其拆分为单个char。\n2. [u8]与Vec\u0026lt;u8\u0026gt;： raw 字节的 “底层载体” [u8]（字节切片）与Vec\u0026lt;u8\u0026gt;（字节向量）是 Rust 中处理 “原始字节数据” 的类型，它们与字符串类型的关系可概括为 “底层实现与上层封装”：\n[u8]：字节切片：与str类似，[u8]是一段字节序列的 “视图”，不拥有数据，通常以引用形式\u0026amp;[u8]存在（如读取文件时获得的二进制数据）。[u8]本身不保证数据是 UTF-8 编码 —— 它只是纯粹的字节集合，可能存储图片、视频等二进制数据，也可能存储非 UTF-8 的文本（如 GBK 编码的中文）。\nVec\u0026lt;u8\u0026gt;：字节向量：与String类似，Vec\u0026lt;u8\u0026gt;是拥有数据的字节集合，支持动态修改（追加、删除字节），并自动管理内存。String本质上就是对Vec\u0026lt;u8\u0026gt;的封装 —— 它在Vec\u0026lt;u8\u0026gt;的基础上增加了 “数据必须是 UTF-8 编码” 的约束，并提供了字符串相关的方法（如contains()、replace()）。\n与字符串类型的对应关系：若用一句话概括它们的关联，即 “String是Vec\u0026lt;u8\u0026gt;的 UTF-8 封装，str是[u8]的 UTF-8 视图”。具体来说：\nstr ↔ [u8]：str是[u8]的 “UTF-8 约束版”，\u0026amp;str可以通过as_bytes()方法转换为\u0026amp;[u8]（获取 UTF-8 字节视图），而\u0026amp;[u8]需要通过std::str::from_utf8()方法验证 UTF-8 合法性后才能转换为\u0026amp;str。\nString ↔ Vec\u0026lt;u8\u0026gt;：String可以通过into_bytes()方法转换为Vec\u0026lt;u8\u0026gt;（转移所有权，String会被消耗），或通过as_bytes_mut()获取可变字节引用；Vec\u0026lt;u8\u0026gt;则可通过String::from_utf8()方法（验证 UTF-8）转换为String。\n3. std::ffi::OSString：适配系统的 “原生字符串” OSString是 Rust 为处理操作系统原生字符串设计的类型，主要用于与操作系统 API 交互（如获取环境变量、处理文件路径），其核心特点是 “兼容性优先”：\n编码无关性：不同于String强制 UTF-8 编码，OSString会根据操作系统的原生编码存储字符串（如 Windows 的 UTF-16、Linux 的 UTF-8），且不保证数据是 UTF-8 格式 —— 这意味着它可能包含无效的 UTF-8 字节，甚至包含\\0（0x00）字符。\n与String的转换：由于编码不确定性，OSString与String的转换需要显式处理：通过into_string()方法尝试转换，若内部数据是有效的 UTF-8，则返回Ok(String)；否则返回Err(OsString)，避免因编码错误导致程序崩溃。\n4. std::path::Path：专注路径的 “类型安全工具” Path是 Rust 中专门用于处理文件系统路径的类型，它本质上是对 “路径字符串” 的封装，核心优势是 “类型安全与跨平台适配”：\n路径语义保障：Path会自动处理不同操作系统的路径分隔符（如 Windows 的\\与 Linux 的/），并提供路径相关的方法（如parent()获取父目录、file_name()获取文件名、exists()判断路径是否存在），避免开发者手动拼接路径时出现的跨平台问题。\n与字符串类型的关联：Path可以通过as_os_str()方法转换为\u0026amp;OsStr（OSString的引用形式），也可以通过to_str()方法尝试转换为\u0026amp;str（若路径是有效的 UTF-8）—— 这既保证了路径处理的灵活性，又通过类型约束避免了将普通字符串误用作路径的错误。\n三、从 C/C++ 痛点看 Rust：如何解决 \u0026lsquo;\\0\u0026rsquo; 截断问题？ 在 C/C++ 中，字符串处理一直存在一个经典痛点：依赖 \u0026lsquo;\\0\u0026rsquo; 作为字符串结束标志。这一设计导致了诸多问题，而 Rust 通过底层设计的优化，从根源上解决了这一问题。\n1. C/C++ 的 \u0026lsquo;\\0\u0026rsquo; 痛点：为何需要 “指针 + 长度” 双参数？ 在 C/C++ 中，字符串本质上是char*（字符指针），通过在字符串末尾添加\\0（空字符，ASCII 码 0x00）来标记结束。这种设计的缺陷十分明显：\n\u0026rsquo;\\0\u0026rsquo; 截断风险：若字符串内部包含\\0字符（如二进制数据中的 0x00 字节），则字符串处理函数（如strlen()、strcpy()）会将其误认为结束标志，导致字符串被截断 —— 例如，字符串\u0026quot;a\\0b\u0026quot;在 C 中会被strlen()判定为长度 1，strcpy()只会复制'a'和\\0，丢失'b'。\n“指针 + 长度” 的妥协：为解决截断问题，C/C++ 中许多底层函数（如strncpy()、read()）不得不增加一个 “长度参数”，要求开发者显式传入字符串的实际长度，以避免\\0误判。但这种方式依赖开发者手动维护 “指针与长度的一致性”，容易因参数传递错误导致缓冲区溢出或数据丢失，安全性较低。\n2. Rust 的解决方案：以 “长度 + UTF-8 约束” 替代 \u0026lsquo;\\0\u0026rsquo; 标志 Rust 彻底抛弃了 “依赖 \u0026lsquo;\\0\u0026rsquo; 标记字符串结束” 的设计，通过 “显式长度存储 + 编码约束” 的组合，从根源上避免了 \u0026lsquo;\\0\u0026rsquo; 截断问题，具体实现可分为三层：\n（1）底层：所有字符串 / 字节类型均存储 “长度信息” 在 Rust 中，无论是\u0026amp;str、String，还是\u0026amp;[u8]、Vec\u0026lt;u8\u0026gt;，其内部都显式存储了数据的长度，而非依赖\\0判断结束：\n例如，\u0026amp;str的底层结构包含两个字段：*const u8（指向 UTF-8 字节序列的指针）和usize（字节序列的长度）；String则是对Vec\u0026lt;u8\u0026gt;的封装，而Vec\u0026lt;u8\u0026gt;同样存储了长度（len）和容量（cap）信息。\n这种设计意味着，Rust 的字符串处理函数（如len()、contains()、copy_from_slice()）无需扫描\\0，直接通过内部存储的长度即可确定数据范围，即使字符串内部包含\\0，也不会影响处理结果。例如：\n1 2 3 4 5 6 // 字符串内部包含\u0026#39;\\0\u0026#39;，但Rust能正确识别长度 let s = \u0026#34;a\\0b\u0026#34;; println!(\u0026#34;长度: {}\u0026#34;, s.len()); // 输出3（\u0026#39;a\u0026#39;占1字节，\u0026#39;\\0\u0026#39;占1字节，\u0026#39;b\u0026#39;占1字节） println!(\u0026#34;是否包含\u0026#39;b\u0026#39;? {}\u0026#34;, s.contains(\u0026#39;b\u0026#39;)); // 输出true （2）中层：UTF-8 编码约束排除 “无效 \u0026lsquo;\\0\u0026rsquo; 场景” 对于str和String，Rust 强制要求数据必须是合法的 UTF-8 编码，而 UTF-8 编码中，\\0（0x00）本身是一个合法的字符（表示空字符），但在正常文本场景中极少出现 —— 这进一步降低了 “\u0026rsquo;\\0\u0026rsquo; 导致逻辑错误” 的概率：\n若开发者尝试将包含无效 UTF-8 字节（如非 UTF-8 的\\0组合）的\u0026amp;[u8]转换为\u0026amp;str，Rust 会在编译期或运行期报错（如std::str::from_utf8()返回Err），避免非法数据进入字符串处理流程。 （3）高层：类型分化应对 “特殊 \u0026lsquo;\\0\u0026rsquo; 需求” 对于需要处理\\0的场景（如与 C/C++ 交互、处理二进制数据），Rust 提供了专门的类型，避免 “用通用字符串类型处理特殊数据”：\n与 C 交互：std::ffi::CString：若需要向 C 函数传递字符串（C 要求字符串以\\0结尾），可使用CString—— 它会自动在字符串末尾添加\\0，并确保内部不含其他\\0（若有则报错），避免 C 函数截断。例如： 1 2 3 4 5 6 7 use std::ffi::CString; // 创建C风格字符串，自动添加\u0026#39;\\0\u0026#39;，且内部无其他\u0026#39;\\0\u0026#39; let c_str = CString::new(\u0026#34;hello\u0026#34;).unwrap(); // 传递给C函数（获取*const c_char指针） let c_ptr = c_str.as_ptr(); 处理二进制数据：Vec\u0026lt;u8\u0026gt;/\u0026amp;[u8]：若数据中包含大量\\0（如二进制文件、网络流），应直接使用Vec\u0026lt;u8\u0026gt;或\u0026amp;[u8]，而非字符串类型 —— 这些字节类型不涉及编码约束，可直接存储和处理\\0，且通过长度信息确保数据完整性。 四、总结：Rust 字符串类型的选择指南 理解 Rust 字符串类型的关键，在于 “根据场景匹配设计目标”。以下是不同场景下的类型选择建议：\n需要动态修改文本：选择String（拥有所有权，支持 UTF-8 修改）。\n传递文本而不修改：选择\u0026amp;str（无复制开销，高性能）。\n处理单个 Unicode 字符：选择char（固定宽度，适合字符操作）。\n处理二进制数据 / 非 UTF-8 文本：选择Vec\u0026lt;u8\u0026gt;或\u0026amp;[u8]（无编码约束，灵活存储）。\n与操作系统 API 交互：选择OSString或\u0026amp;OsStr（适配系统原生编码）。\n处理文件路径：选择Path或PathBuf（跨平台适配，路径语义保障）。\n与 C/C++ 交互：选择CString（生成\\0结尾的 C 风格字符串）。\nRust 的字符串体系看似复杂，实则是 “精确应对不同场景” 的设计体现 —— 它通过类型分化解决了 “通用字符串类型无法兼顾性能、安全性与兼容性” 的问题，同时以 “显式长度 + 编码约束” 彻底摆脱了 C/C++ 中 \u0026lsquo;\\0\u0026rsquo; 截断的痛点，为开发者提供了更安全、更高效的字符串处理方案。\n","date":"2025-11-01T21:46:01+08:00","permalink":"https://techinsight.pages.dev/p/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90-rust-%E4%B8%AD%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E5%85%B3%E7%B3%BB%E5%8C%BA%E5%88%AB%E4%B8%8E-0x00-%E5%A4%84%E7%90%86%E6%96%B9%E6%A1%88/","title":"深入解析 Rust 中的字符串类型：关系、区别与 '0x00' 处理方案"},{"content":"在 Rust 中，for...in 循环是遍历集合的核心工具。它看似简单，却暗藏着 Rust 对内存安全和所有权的精妙设计。本文将聚焦 for...in 循环的本质，解析它如何通过迭代器实现灵活的集合遍历，以及不同迭代方式的选择逻辑。\n本质：迭代器的“语法糖” for...in 循环的核心逻辑是迭代一个迭代器（Iterator）。Rust 中所有能被 for...in 遍历的类型（如向量、数组、范围等），都实现了 IntoIterator 特性——这个特性的作用是将类型“转换为迭代器”。\n简单说，for item in 集合 本质上是在做：\n调用集合的 into_iter() 方法，将其转换为迭代器； 依次从迭代器中取出元素，赋值给循环变量 item； 直到迭代器耗尽，循环结束。 三种基础遍历方式：从引用到所有权 针对集合的不同使用场景（只读、修改、消耗），for...in 有三种典型写法，对应三种迭代器类型：\n1. 只读遍历：迭代不可变引用 当你需要访问集合元素但不修改它们时，用 \u0026amp;集合 触发不可变引用迭代：\n1 2 3 4 5 6 7 let vec = vec![1, 2, 3]; // 等价于 for item in vec.iter() for item in \u0026amp;vec { println!(\u0026#34;{}\u0026#34;, item); // item 类型是 \u0026amp;i32（不可变引用） } // 遍历后 vec 仍可使用（未被消耗） println!(\u0026#34;遍历后：{:?}\u0026#34;, vec); // [1, 2, 3] 这里的 \u0026amp;vec 会通过 IntoIterator 转换为 vec.iter() 返回的迭代器，每次迭代获取的是元素的不可变引用（\u0026amp;T）。由于只借用元素，原集合不会被修改或消耗。\n为什么用 iter() 而不是 iter_mut()？\niter() 产生的不可变引用可以共享，不影响其他地方对集合的只读访问。而 iter_mut() 产生的可变引用具有排他性——迭代期间整个集合会被独占借用，其他地方无法同时访问（包括读取）。因此，仅需读取时用 iter() 更灵活。\n2. 修改遍历：迭代可变引用 若要修改集合元素，用 \u0026amp;mut 集合 触发可变引用迭代：\n1 2 3 4 5 6 7 let mut vec = vec![1, 2, 3]; // 等价于 for item in vec.iter_mut() for item in \u0026amp;mut vec { *item += 1; // item 类型是 \u0026amp;mut i32（可变引用，需解引用修改） } // 遍历后 vec 已被修改 println!(\u0026#34;修改后：{:?}\u0026#34;, vec); // [2, 3, 4] \u0026amp;mut vec 会转换为 vec.iter_mut() 返回的迭代器，每次迭代获取元素的可变引用（\u0026amp;mut T）。通过可变引用，我们可以直接修改集合中的元素，且原集合仍可继续使用。\n3. 消耗遍历：迭代所有权 当你需要获取元素的所有权（例如将元素转移到其他地方），直接用 集合 触发所有权迭代：\n1 2 3 4 5 6 7 let vec = vec![1, 2, 3]; // 等价于 for item in vec.into_iter() for item in vec { println!(\u0026#34;{}\u0026#34;, item); // item 类型是 i32（直接拥有元素） } // 遍历后 vec 已被消耗，无法再使用 // println!(\u0026#34;{:?}\u0026#34;, vec); // 编译错误：vec 已被移动 直接使用 vec 时，会调用 vec.into_iter() 生成迭代器，每次迭代会将元素的所有权从集合中“取出”并转移给 item。遍历结束后，原集合因失去所有元素的所有权而被“消耗”，无法再被访问。\n为什么“不需要修改时，也可能需要消耗遍历？\n“不需要修改”仅意味着不需要改变元素的值，但可能需要转移元素的所有权：\n例如将元素存入另一个集合（需拥有所有权才能存入）； 对于整数等 Copy 类型，消耗遍历可避免反复解引用（直接使用值更简洁）； 处理临时集合时，“用完即弃”可节省内存。 示例：\n1 2 3 4 5 6 7 let vec = vec![1, 2, 3]; let mut new_vec = Vec::new(); // 消耗原集合，将元素所有权转移到新集合 for item in vec { new_vec.push(item); } println!(\u0026#34;新集合：{:?}\u0026#34;, new_vec); // [1, 2, 3] 特殊场景：范围迭代 除了集合，for...in 还能直接遍历“范围（Range）”，这是一种无需提前创建集合的轻量迭代方式：\n1 2 3 4 5 6 7 8 9 // 遍历 1 到 4（不包含 5） for i in 1..5 { println!(\u0026#34;{}\u0026#34;, i); // 1, 2, 3, 4 } // 遍历 1 到 5（包含 5） for i in 1..=5 { println!(\u0026#34;{}\u0026#34;, i); // 1, 2, 3, 4, 5 } 范围本质上也是一种迭代器，它通过计算生成序列，无需占用额外内存。\n总结：三种迭代方式的核心区别 迭代方式 元素类型 是否消耗原集合 适用场景 iter() \u0026amp;T（不可变引用） 不消耗 只读访问，需保留原集合，允许共享访问 iter_mut() \u0026amp;mut T（可变引用） 不消耗 需要修改元素，独占集合访问权 into_iter() T（所有权） 消耗 需获取元素所有权（转移、Copy 类型简化操作等），用完即弃集合 选择的核心逻辑是：是否需要修改元素，以及是否需要保留原集合。掌握这三种方式，就能灵活应对 Rust 中绝大多数集合遍历场景。\n","date":"2025-10-31T21:14:08+08:00","permalink":"https://techinsight.pages.dev/p/rust-%E4%B8%AD%E7%9A%84-for...in-%E5%BE%AA%E7%8E%AF%E7%AE%80%E6%B4%81%E8%BF%AD%E4%BB%A3%E7%9A%84%E8%89%BA%E6%9C%AF/","title":"Rust 中的 `for...in` 循环：简洁迭代的艺术"},{"content":"很多人觉得线性代数难，其实是一开始没搞懂 “核心概念” 和 “具体运算” 的对应关系。今天咱们不绕弯子，从最基础的矩阵开始，每个概念都配实打实的计算例子，帮你把入门知识学扎实。\n一、先搞懂 “矩阵”：就是按规矩排好的数字表格 1. 什么是矩阵？ 简单说，矩阵就是把一组数字按 “行数 × 列数” 的格式排成的表格，通常用大写字母（比如 A、B）表示。比如一个 “2 行 3 列” 的矩阵，就像下面这样：\n\\(A = \\begin{bmatrix} 1 \u0026 3 \u0026 5 \\\\ 2 \u0026 4 \u0026 6 \\end{bmatrix}\\)\n这里 “2 行 3 列” 可以记为 “2×3 矩阵”，表格里的每个数字叫 “元素”，比如第一行第二列的元素是 3，第二行第三列的元素是 6。\n2. 常见的特殊矩阵（记熟这些，后续运算少踩坑） 方阵：行数和列数相等的矩阵。比如 3×3 矩阵、2×2 矩阵，像下面这个 2×2 方阵：\n\\(B = \\begin{bmatrix} 2 \u0026 7 \\\\ 5 \u0026 9 \\end{bmatrix}\\)\n方阵是后续学 “行列式”“逆矩阵” 的基础，一定要先认熟。\n行矩阵 / 列矩阵：只有 1 行，或者只有 1 列的矩阵。\n行矩阵（比如表示 3 个数据的列表）：\\(C = \\begin{bmatrix} 4 \u0026 8 \u0026 10 \\end{bmatrix}\\)（1×3 矩阵）\n列矩阵（比如表示 2 个维度的向量）：\\(D = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\\)（2×1 矩阵）\n零矩阵：所有元素都是 0 的矩阵，记为 O。比如 2×3 零矩阵：\n\\(O = \\begin{bmatrix} 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \\end{bmatrix}\\)\n单位矩阵：只有 “主对角线”（从左上角到右下角）上的元素是 1，其余都是 0 的方阵，记为 E。比如 3×3 单位矩阵：\n\\(E = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix}\\)\n单位矩阵有点像 “乘法里的 1”，后续学矩阵乘法会用到。\n二、矩阵的基本运算：加法、数乘、乘法（每个都配例子） 学完矩阵的定义，下一步就是搞懂 “怎么对矩阵做运算”。这部分是线性代数的 “基本功”，例子一定要跟着算一遍。\n1. 矩阵加法：先满足 “同型”，再对应相加 首先要记住：只有同型矩阵才能相加（什么是同型矩阵？下一部分会讲，这里先理解 “行数、列数都相同”）。\n加法规则：两个矩阵对应位置的元素相加，结果放在同样位置。\n例子：计算 A + B（已知 A 和 B 都是 2×2 矩阵）\n已知：\\(A = \\begin{bmatrix} 1 \u0026 2 \\\\ 3 \u0026 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 \u0026 6 \\\\ 7 \u0026 8 \\end{bmatrix}\\)\n计算过程：\n第一行第一列：1 + 5 = 6\n第一行第二列：2 + 6 = 8\n第二行第一列：3 + 7 = 10\n第二行第二列：4 + 8 = 12\n所以结果：\\(A + B = \\begin{bmatrix} 6 \u0026 8 \\\\ 10 \u0026 12 \\end{bmatrix}\\)\n2. 矩阵数乘：用一个数乘遍所有元素 数乘规则：用一个常数（比如 k）乘以矩阵里的每一个元素，结果还是同样行数和列数的矩阵。\n例子：计算 2×A（已知 A 是 2×3 矩阵）\n已知：\\(A = \\begin{bmatrix} 1 \u0026 2 \u0026 3 \\\\ 4 \u0026 5 \u0026 6 \\end{bmatrix}\\)\n计算过程：用 2 乘每个元素\n第一行：1×2=2，2×2=4，3×2=6\n第二行：4×2=8，5×2=10，6×2=12\n所以结果：\\(2A = \\begin{bmatrix} 2 \u0026 4 \u0026 6 \\\\ 8 \u0026 10 \u0026 12 \\end{bmatrix}\\)\n3. 矩阵乘法：最容易错，一定要按 “规则” 来 矩阵乘法是重点，也是难点，先记住两个关键前提：\n只有 “前一个矩阵的列数 = 后一个矩阵的行数”，才能相乘；\n结果矩阵的行数 = 前一个矩阵的行数，结果矩阵的列数 = 后一个矩阵的列数。\n乘法规则：结果矩阵中 “第 i 行第 j 列的元素”，等于前一个矩阵第 i 行的每个元素，分别乘后一个矩阵第 j 列的对应元素，再把所有乘积加起来。\n例子：计算 A×B（已知 A 是 2×3 矩阵，B 是 3×2 矩阵）\n已知：\\(A = \\begin{bmatrix} 1 \u0026 2 \u0026 3 \\\\ 4 \u0026 5 \u0026 6 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 7 \u0026 8 \\\\ 9 \u0026 10 \\\\ 11 \u0026 12 \\end{bmatrix}\\)\n第一步：先确认能不能乘 ——A 的列数是 3，B 的行数是 3，满足条件；结果矩阵是 2×2 矩阵（A 的行数 2，B 的列数 2）。\n第二步：计算结果矩阵的每个元素：\n结果第一行第一列：A 第一行（1,2,3）× B 第一列（7,9,11）→ 1×7 + 2×9 + 3×11 = 7 + 18 + 33 = 58\n结果第一行第二列：A 第一行（1,2,3）× B 第二列（8,10,12）→ 1×8 + 2×10 + 3×12 = 8 + 20 + 36 = 64\n结果第二行第一列：A 第二行（4,5,6）× B 第一列（7,9,11）→ 4×7 + 5×9 + 6×11 = 28 + 45 + 66 = 139\n结果第二行第二列：A 第二行（4,5,6）× B 第二列（8,10,12）→ 4×8 + 5×10 + 6×12 = 32 + 50 + 72 = 154\n所以结果：\\(A×B = \\begin{bmatrix} 58 \u0026 64 \\\\ 139 \u0026 154 \\end{bmatrix}\\)\n提醒：矩阵乘法不满足 “交换律”，比如 A×B 和 B×A 不一定相等（甚至可能一个能乘，一个不能乘），计算时千万别搞反顺序。\n三、同型矩阵：理解 “为什么只有同型才能相加” 1. 什么是同型矩阵？ 两个矩阵如果 “行数相同，并且列数也相同”，就是同型矩阵。\n比如：\n矩阵 A（2×3）和矩阵 C（2×3）：同型（都是 2 行 3 列）；\n矩阵 B（2×2）和矩阵 D（2×1）：不同型（列数不一样）。\n2. 为什么同型矩阵才能相加？ 其实很简单：加法是 “对应元素相加”，如果行数或列数不一样，根本找不到 “对应的元素”。比如一个 2×3 矩阵和一个 3×2 矩阵，前者第一行有 3 个元素，后者第一行只有 2 个元素，怎么对应相加？\n所以记住：加法看同型，乘法看 “前列 = 后行”。\n四、行列式：只对方阵有用，是个 “具体的数” 1. 什么是行列式？ 行列式是针对 “方阵” 定义的一个数值，记为 det (A) 或者 | A|。简单说，就是把方阵里的元素按特定规则计算，最后得到一个数 —— 这个数能反映方阵的很多性质（比如后续会学的 “逆矩阵是否存在”）。\n2. 行列式怎么算？（从简单的 2 阶、3 阶开始） （1）2 阶方阵的行列式（最容易）\n规则：主对角线元素相乘，减去副对角线元素相乘。\n比如方阵\\(A = \\begin{bmatrix} a \u0026 b \\\\ c \u0026 d \\end{bmatrix}\\)，行列式计算：\\(|A| = ad - bc\\)\n例子：计算 2 阶方阵的行列式\n已知：\\(A = \\begin{bmatrix} 2 \u0026 5 \\\\ 3 \u0026 7 \\end{bmatrix}\\)\n计算过程：主对角线（2×7） - 副对角线（5×3）= 14 - 15 = -1\n所以 | A| = -1\n（2）3 阶方阵的行列式（按 “展开式” 算）\n规则：选一行（或一列），把每个元素乘以它的 “代数余子式”，再把所有结果加起来（这里先记方法，具体 “代数余子式” 不用深抠，先会算例子即可）。\n例子：计算 3 阶方阵的行列式\n已知：\\(B = \\begin{bmatrix} 1 \u0026 2 \u0026 3 \\\\ 4 \u0026 5 \u0026 6 \\\\ 7 \u0026 8 \u0026 9 \\end{bmatrix}\\)\n第一步：选第一行展开（1, 2, 3），计算每个元素的 “元素 × 代数余子式”：\n1× 代数余子式：1×[(5×9) - (6×8) ] = 1×(45 - 48) = -3\n2× 代数余子式：2×[- (4×9 - 6×7) ] = 2×[ - (36 - 42) ] = 2×6 = 12\n3× 代数余子式：3×[(4×8) - (5×7) ] = 3×(32 - 35) = 3×(-3) = -9\n第二步：把结果相加：-3 + 12 - 9 = 0\n所以 | B| = 0\n3. 行列式的关键作用：判断方阵是否 “可逆” 记住一个结论：如果一个方阵的行列式≠0，这个方阵就是 “可逆的”；如果行列式 = 0，就是 “不可逆的”（逆矩阵是什么？下一部分讲）。\n比如刚才的例子：|A|=-1≠0，所以 A 可逆；|B|=0，所以 B 不可逆。\n五、逆矩阵：方阵的 “倒数”，满足 “乘完等于单位矩阵” 1. 什么是逆矩阵？ 对于 n 阶方阵 A，如果存在一个 n 阶方阵 B，满足 “A×B = B×A = E”（E 是 n 阶单位矩阵），那么 B 就是 A 的逆矩阵，记为 A⁻¹。\n简单理解：逆矩阵就像 “倒数”，比如 2 的倒数是 1/2，因为 2×1/2=1；而 A 的逆矩阵 A⁻¹，就是和 A 相乘后等于 “单位矩阵 E”（相当于乘法里的 1）的矩阵。\n2. 怎么求逆矩阵？（以 2 阶方阵为例，用 “伴随矩阵法”） 2 阶方阵的逆矩阵有个简单公式：\n如果\\(A = \\begin{bmatrix} a \u0026 b \\\\ c \u0026 d \\end{bmatrix}\\)，且 | A|≠0（可逆），那么：\n\\(A^{-1} = \\frac{1}{|A|} \\times \\begin{bmatrix} d \u0026 -b \\\\ -c \u0026 a \\end{bmatrix}\\)\n（公式里的矩阵是 “伴随矩阵”，简单记为 “主对角线元素互换，副对角线元素变号”）\n例子：求 2 阶方阵的逆矩阵\n已知：\\(A = \\begin{bmatrix} 2 \u0026 5 \\\\ 3 \u0026 7 \\end{bmatrix}\\)（之前算过 | A|=-1≠0，可逆）\n第一步：计算 1/|A| = 1/(-1) = -1\n第二步：写伴随矩阵（主对角线互换，副对角线变号）：\\(\\begin{bmatrix} 7 \u0026 -5 \\\\ -3 \u0026 2 \\end{bmatrix}\\)\n第三步：两者相乘：\\(A^{-1} = -1 \\times \\begin{bmatrix} 7 \u0026 -5 \\\\ -3 \u0026 2 \\end{bmatrix} = \\begin{bmatrix} -7 \u0026 5 \\\\ 3 \u0026 -2 \\end{bmatrix}\\)\n验证：A×A⁻¹ 是否等于单位矩阵 E\n计算\\(A×A^{-1} = \\begin{bmatrix} 2 \u0026 5 \\\\ 3 \u0026 7 \\end{bmatrix} × \\begin{bmatrix} -7 \u0026 5 \\\\ 3 \u0026 -2 \\end{bmatrix}\\)\n按矩阵乘法规则计算：\n第一行第一列：2×(-7) + 5×3 = -14 + 15 = 1\n第一行第二列：2×5 + 5×(-2) = 10 - 10 = 0\n第二行第一列：3×(-7) + 7×3 = -21 + 21 = 0\n第二行第二列：3×5 + 7×(-2) = 15 - 14 = 1\n结果是\\(\\begin{bmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\end{bmatrix}\\)（2 阶单位矩阵 E），说明逆矩阵求对了。\n六、线性方程组：用矩阵表示，解的情况和行列式、逆矩阵有关 最后咱们把前面的知识串起来：线性代数很重要的一个用途，就是解 “线性方程组”（比如 2 个未知数、2 个方程的方程组）。\n1. 线性方程组的矩阵形式 比如下面这个 2 元一次方程组：\n\\(\\begin{cases} 2x + 5y = 1 \\\\ 3x + 7y = 2 \\end{cases}\\)\n可以写成 “矩阵乘法” 的形式：\\(A×X = b\\)\n其中：\nA 是 “系数矩阵”：\\(\\begin{bmatrix} 2 \u0026 5 \\\\ 3 \u0026 7 \\end{bmatrix}\\)（方程组中 x、y 的系数）\nX 是 “未知数向量”：\\(\\begin{bmatrix} x \\\\ y \\end{bmatrix}\\)（要解的 x 和 y）\nb 是 “常数项向量”：\\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\)（方程组等号右边的数）\n2. 怎么用逆矩阵解方程？ 如果 A 可逆（|A|≠0），那么在等式 A×X = b 两边同时左乘 A⁻¹，得到：\n\\(A^{-1}×A×X = A^{-1}×b\\)\n因为 A⁻¹×A = E（单位矩阵），而 E×X = X，所以：\\(X = A^{-1}×b\\)\n例子：用逆矩阵解刚才的方程组\n已知 A⁻¹ = \\(\\begin{bmatrix} -7 \u0026 5 \\\\ 3 \u0026 -2 \\end{bmatrix}\\)，b = \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\)\n计算 X = A⁻¹×b：\n\\(X = \\begin{bmatrix} -7 \u0026 5 \\\\ 3 \u0026 -2 \\end{bmatrix} × \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} (-7)×1 + 5×2 \\\\ 3×1 + (-2)×2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}\\)\n所以方程组的解是 x=3，y=-1（代入原方程组验证：2×3 +5×(-1)=6-5=1，3×3 +7×(-1)=9-7=2，完全正确）。\n入门总结：先抓 “核心逻辑”，再练 “具体计算” 线性代数入门不用怕，记住这 3 个关键点：\n矩阵是 “数字表格”，运算要按规则（加法看同型，乘法看前列 = 后行）；\n行列式是 “方阵的数值”，决定方阵是否可逆；\n逆矩阵是 “方阵的倒数”，能用来解线性方程组。\n接下来可以试着自己找几个矩阵，练一练加法、乘法和行列式计算，熟练之后再学更复杂的概念（比如向量、秩）就会轻松很多。\n","date":"2025-10-21T22:41:41+08:00","permalink":"https://techinsight.pages.dev/p/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E4%BB%8E%E7%9F%A9%E9%98%B5%E5%88%B0%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84%E7%94%A8%E4%BE%8B%E5%AD%90%E7%9C%8B%E6%87%82%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/","title":"线性代数快速入门:从矩阵到线性方程组，用例子看懂核心概念"},{"content":"Windows 11下的Windows Terminal堪堪一用，但在Windows 10 上官方的这些东西就完全没法用了，Wezterm还稍微有点定制性，花了点时间设置了一个自己需要的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 -- These are the basic\u0026#39;s for using wezterm. -- Mux is the mutliplexes for windows etc inside of the terminal -- Action is to perform actions on the terminal local wezterm = require \u0026#39;wezterm\u0026#39; local mux = wezterm.mux local act = wezterm.action -- These are vars to put things in later (i dont use em all yet) local config = {} local keys = {} local mouse_bindings = {} local launch_menu = { { label = \u0026#39;PowerShell 7\u0026#39;, domain = \u0026#39;DefaultDomain\u0026#39;, args = {\u0026#39;C:\\\\Program Files\\\\PowerShell\\\\7\\\\pwsh.exe\u0026#39;} }, { label = \u0026#39;WSL(Debian)\u0026#39;, args = {\u0026#39;wsl\u0026#39;, \u0026#39;-d\u0026#39;, \u0026#39;Debian\u0026#39;} }, { label = \u0026#39;Git Bash\u0026#39;, domain = \u0026#39;DefaultDomain\u0026#39;, args = {\u0026#39;C:\\\\Program Files\\\\Git\\\\bin\\\\bash.exe\u0026#39;} } } -- This is for newer wezterm vertions to use the config builder if wezterm.config_builder then config = wezterm.config_builder() end -- Default config settings -- These are the default config settins needed to use Wezterm -- Just add this and return config and that\u0026#39;s all the basics you need -- Color scheme, Wezterm has 100s of them you can see here: -- https://wezfurlong.org/wezterm/colorschemes/index.html config.color_scheme = \u0026#39;Oceanic Next (Gogh)\u0026#39; -- This is my chosen font, we will get into installing fonts on windows later config.font = wezterm.font(\u0026#39;FiraCode Nerd Font\u0026#39;) config.font_size = 12 config.launch_menu = launch_menu -- makes my cursor blink config.default_cursor_style = \u0026#39;BlinkingBar\u0026#39; config.disable_default_key_bindings = true -- this adds the ability to use ctrl+v to paste the system clipboard config.keys = { -- Ctrl+V 粘贴 { key = \u0026#39;V\u0026#39;, mods = \u0026#39;CTRL\u0026#39;, action = act.PasteFrom \u0026#39;Clipboard\u0026#39; }, -- Ctrl+Shift+N 打开环境选择菜单 { key = \u0026#39;t\u0026#39;, mods = \u0026#39;CTRL\u0026#39;, action = act.SpawnTab \u0026#39;CurrentPaneDomain\u0026#39; }, -- 新增一些常用快捷键 { key = \u0026#39;c\u0026#39;, mods = \u0026#39;CTRL\u0026#39;, action = act.CopyTo \u0026#39;ClipboardAndPrimarySelection\u0026#39; }, { key = \u0026#39;n\u0026#39;, mods = \u0026#39;CTRL\u0026#39;, action = act.SpawnWindow }, { key = \u0026#39;w\u0026#39;, mods = \u0026#39;CTRL\u0026#39;, action = act.CloseCurrentTab { confirm = true } }} config.mouse_bindings = mouse_bindings -- There are mouse binding to mimc Windows Terminal and let you copy -- To copy just highlight something and right click. Simple mouse_bindings = {{ event = { Down = { streak = 3, button = \u0026#39;Left\u0026#39; } }, action = wezterm.action.SelectTextAtMouseCursor \u0026#39;SemanticZone\u0026#39;, mods = \u0026#39;NONE\u0026#39; }, { event = { Down = { streak = 1, button = \u0026#34;Right\u0026#34; } }, mods = \u0026#34;NONE\u0026#34;, action = wezterm.action_callback(function(window, pane) local has_selection = window:get_selection_text_for_pane(pane) ~= \u0026#34;\u0026#34; if has_selection then window:perform_action(act.CopyTo(\u0026#34;ClipboardAndPrimarySelection\u0026#34;), pane) window:perform_action(act.ClearSelection, pane) else window:perform_action(act({ PasteFrom = \u0026#34;Clipboard\u0026#34; }), pane) end end) }} -- This is used to make my foreground (text, etc) brighter than my background config.foreground_text_hsb = { hue = 1.0, saturation = 1.2, brightness = 1.5 } -- IMPORTANT: Sets WSL2 Debian as the defualt when opening Wezterm -- config.default_domain = \u0026#39;WSL:Debian\u0026#39; config.default_domain = \u0026#39;local\u0026#39; -- 此处路径替换为你实际的 pwsh.exe 路径 config.default_prog = {\u0026#34;C:\\\\Program Files\\\\PowerShell\\\\7\\\\pwsh.exe\u0026#34;} return config 这样可以对【+】右键呼出菜单，选择想要的Shell了。\n","date":"2025-10-03T23:09:31+08:00","permalink":"https://techinsight.pages.dev/p/wezterm-config/","title":"Wezterm Config"},{"content":"最近实在是受不了Windows 11 的卡顿和延迟，先在台式机上安装了Windows 10验证了一番，确实是指哪打哪、干脆利落，于是就把笔记本上预装的Win11给抹掉安装Win10了，安装之后也遇到了一些问题，最主要的是WSL。\n因为之前在Win11上部署的一个Debian WSL里面安装了非常多的开发工具，已经50多G了，如果重新配置就太麻烦了，所以肯定还是要想办法保留，重装完之后我有一个ext4.vhdx文件等待导入。\n说起来Windows这系统开发得也真是草率，遇到的各种问题都不能简单明了地解释清楚，到处都是“未知问题”、“发现了一个错误”这种，看了也不知道怎么解决。\n首先就是要打开Hyper-V虚拟机的选项和Windows Subsystem Linux的功能，但谁能知道打开这功能之后还有一大堆问题等着呢。\n报有一个进程正在使用 执行wsl --import Debian D:\\WSL\\Debian D:\\WSL\\Deiban\\ext4.vhdx，立即就会报一个有其他进程正在使用文件还是什么的类似的报错，查了很多解决方案，有的说把这个文件复制到别处啥的，都没用。最后才发现是因为用了两个相同的目录，可能本质是因为要写入的目标文件也是ext4.vhdx，而这个文件正在被读取进程使用，但这个报错信息真的是没有任何价值。\n换了一个命令之后变成了未知错误 1 wsl --import Debian D:\\WSL\\Debian13 D:\\WSL\\Deiban\\ext4.vhdx 这样终于不再报上面那个错了，但仍然不行，报未知错误，比上面那个还离谱，而且是执行几分钟之后才报出来。都让我一度想放弃了。后来发现还可以执行一下wsl --update，执行了之后再执行上面这个命令，终于来了不一样的报错了。\n接近成功 1 wsl --import Debian D:\\WSL\\Debian13 D:\\WSL\\Deiban\\ext4.vhdx 这时候报的是vhdx文件需要用``\u0026ndash;vhd选项执行，因为默认要导入的是tar包。这时候我就理解了，因为正常的备份方式是先导出tar包，再导入，我这里使用的是vhdx，所以要指定不用tar包，所以在上面的命令上加上\u0026ndash;vhd`就可以了。\n1 wsl --import Debian D:\\WSL\\Debian13 D:\\WSL\\Deiban\\ext4.vhdx --vhd 前面可能遗漏了一个重要的点，因为我在此之前已经执行了wsl --set-default-version 2，让WSL默认使用WSL2了，而Windows 10 默认用的应该是版本1，所以在导入过程中没有遇到这个WSL版本的问题。\n这时候就出现了可爱的进度条，这时候我的眼泪都要下来了，太难了。狗日的Windows这报错信息比我自己写的还要烂，完全不能体现出具体是什么问题，没有任何价值。\n","date":"2025-10-03T12:59:02+08:00","permalink":"https://techinsight.pages.dev/p/%E6%8A%8A%E5%9C%A8win11%E4%B8%AD%E7%94%9F%E6%88%90%E7%9A%84wsl%E5%AF%BC%E5%85%A5%E5%88%B0%E6%96%B0%E5%AE%89%E8%A3%85%E7%9A%84win10%E4%B8%AD/","title":"把在Win11中生成的WSL导入到新安装的Win10中"},{"content":"目录 项目创建与配置\nRust 代码编写规范\n头文件生成与管理\n构建系统实现\n安装与分发\n调用示例\n跨平台注意事项\n最佳实践\n项目创建与配置 初始化项目 1 2 cargo new --lib rust_so_lib cd rust_so_lib Cargo.toml 配置 1 2 3 4 5 6 7 8 9 10 11 [package] name = \u0026#34;rust_so_lib\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; [lib] name = \u0026#34;rust_so_lib\u0026#34; crate-type = [\u0026#34;cdylib\u0026#34;]# 关键配置：生成动态链接库 [dependencies] libc = \u0026#34;0.2\u0026#34;# 用于C类型兼容 重要提示：确保不要同时使用 [workspace] 和 [lib] 配置，否则会导致编译错误。\nRust 代码编写规范 基础示例 (src/lib.rs) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 use std::os::raw::{c_int, c_char}; use std::ffi::{CString, CStr}; #[unsafe(no_mangle)] pub extern \u0026#34;C\u0026#34; fn add_numbers(a: c_int, b: c_int) -\u0026gt; c_int { a + b } #[unsafe(no_mangle)] pub extern \u0026#34;C\u0026#34; fn string_length(s: *const c_char) -\u0026gt; c_int { let c_str = unsafe { CStr::from_ptr(s) }; c_str.to_bytes().len() as c_int } #[unsafe(no_mangle)] pub extern \u0026#34;C\u0026#34; fn create_greeting(name: *const c_char) -\u0026gt; *mut c_char { let name_str = unsafe { CStr::from_ptr(name).to_string_lossy() }; let greeting = format!(\u0026#34;Hello, {}!\u0026#34;, name_str); CString::new(greeting).unwrap().into_raw() } #[unsafe(no_mangle)] pub extern \u0026#34;C\u0026#34; fn free_string(s: *mut c_char) { unsafe { CString::from_raw(s) }; } 关键编程要点 ABI 兼容性： 使用 extern \u0026quot;C\u0026quot; 确保C兼容调用约定 #[unsafe(no_mangle)] 禁止名称修饰 类型映射：\nRust 类型 C 类型 c_int int *const c_char const char* *mut c_char char* 内存安全：\nRust分配的内存应由Rust释放 提供明确的资源释放函数（如free_string） 头文件生成与管理 手动创建头文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // rust_so_lib.h #ifndef RUST_SO_LIB_H #define RUST_SO_LIB_H #ifdef __cplusplus extern \u0026#34;C\u0026#34; { #endif int add_numbers(int a, int b); int string_length(const char* s); char* create_greeting(const char* name); void free_string(char* s); #ifdef __cplusplus } #endif #endif // RUST_SO_LIB_H 使用 cbindgen 自动生成 安装工具：\n1 cargo install cbindgen 配置文件 (cbindgen.toml)：\n1 2 3 language = \u0026#34;C\u0026#34; include_guard = \u0026#34;RUST_SO_LIB_H\u0026#34; autogen_warning = \u0026#34;/* AUTO-GENERATED FILE, DO NOT EDIT */\u0026#34; 生成命令：\n1 cbindgen --config cbindgen.toml --crate rust_so_lib --output rust_so_lib.h 构建系统实现 完整 Makefile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # 项目配置 TARGET := rust_so_lib VERSION := 0.1.0 PREFIX ?= /usr/local LIBDIR := $(PREFIX)/lib INCLUDEDIR := $(PREFIX)/include # 构建目标 RUST_TARGET := release RUST_LIB := target/$(RUST_TARGET)/lib$(TARGET).so HEADER := $(TARGET).h .PHONY: all build install uninstall clean test all: build build: cargo build --$(RUST_TARGET) install: build install -d $(LIBDIR) $(INCLUDEDIR) install -m 755 $(RUST_LIB) $(LIBDIR)/lib$(TARGET).so.$(VERSION) ln -sf lib$(TARGET).so.$(VERSION) $(LIBDIR)/lib$(TARGET).so install -m 644 $(HEADER) $(INCLUDEDIR) ldconfig uninstall: rm -f $(LIBDIR)/lib$(TARGET).so* rm -f $(INCLUDEDIR)/$(HEADER) ldconfig clean: cargo clean rm -f *.o *.a *.so test_$(TARGET) test: $(RUST_LIB) $(CC) test.c -o test_$(TARGET) -l$(TARGET) -Ltarget/$(RUST_TARGET) -I. LD_LIBRARY_PATH=target/$(RUST_TARGET) ./test_$(TARGET) Makefile 功能说明 命令 功能描述 make 构建Release版本的动态库 make install 安装到系统目录 make uninstall 卸载已安装的库 make clean 清理所有生成文件 make test 编译并运行测试程序 安装与分发 标准安装流程 1 2 cargo build --release sudo make install 自定义安装路径 1 make install PREFIX=/custom/install/path 分发包内容 建议包含以下文件结构：\n1 2 3 4 5 6 7 dist/ ├── include/ │└── rust_so_lib.h# 头文件 ├── lib/ │└── librust_so_lib.so # 动态库 └── examples/# 调用示例 └── test.c 调用示例 C 语言示例 (test.c) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #include \u0026lt;stdio.h\u0026gt; #include \u0026#34;rust_so_lib.h\u0026#34; int main() { // 数值计算 printf(\u0026#34;3 + 5 = %d\\n\u0026#34;, add_numbers(3, 5)); // 字符串处理 const char* name = \u0026#34;Rust\u0026#34;; char* greeting = create_greeting(name); printf(\u0026#34;%s\\n\u0026#34;, greeting); printf(\u0026#34;String length: %d\\n\u0026#34;, string_length(greeting)); free_string(greeting); return 0; } 编译与运行 1 2 3 4 5 6 # 动态链接方式 gcc test.c -o test -lrust_so_lib -Ltarget/release LD_LIBRARY_PATH=target/release ./test # 静态链接方式 gcc test.c -o test target/release/librust_so_lib.so 跨平台注意事项 平台差异处理 1 2 3 4 5 6 7 8 9 10 11 12 # 在Makefile中添加平台检测 UNAME := $(shell uname) ifeq ($(UNAME), Darwin) LIB_EXT := dylib LDLIBS := -Wl,-rpath,@loader_path else ifeq ($(OS), Windows_NT) LIB_EXT := dll LDLIBS := -Wl,-rpath=\u0026#39;$$ORIGIN\u0026#39; else LIB_EXT := so LDLIBS := -Wl,-rpath=\\$$ORIGIN endif Windows 特殊处理 修改 Cargo.toml： 1 2 [target.\u0026#39;cfg(windows)\u0026#39;.dependencies] winapi = { version = \u0026#34;0.3\u0026#34;, features = [\u0026#34;winbase\u0026#34;] } 添加 DLL 导出属性： 1 2 3 4 #[cfg_attr(target_os = \u0026#34;windows\u0026#34;, link(name = \u0026#34;rust_so_lib\u0026#34;))] extern \u0026#34;C\u0026#34; { // 导出函数 } 最佳实践 接口设计原则 保持接口稳定： 使用版本号控制ABI兼容性\n避免频繁变更函数签名\n内存管理规范： 1 2 3 4 5 6 7 8 9 10 11 12 13 // 明确所有权转移 #[unsafe(no_mangle)] pub extern \u0026#34;C\u0026#34; fn create_buffer(size: usize) -\u0026gt; *mut u8 { let mut buf = Vec::with_capacity(size); let ptr = buf.as_mut_ptr(); std::mem::forget(buf); // 明确转移所有权 ptr } #[unsafe(no_mangle)] pub extern \u0026#34;C\u0026#34; fn free_buffer(ptr: *mut u8, size: usize) { unsafe { Vec::from_raw_parts(ptr, 0, size) }; } 错误处理模式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #[repr(C)] pub enum ErrorCode { Success = 0, InvalidInput = 1, MemoryError = 2, } #[unsafe(no_mangle)] pub extern \u0026#34;C\u0026#34; fn safe_operation(input: *const c_char) -\u0026gt; ErrorCode { if input.is_null() { return ErrorCode::InvalidInput; } // ...操作逻辑 ErrorCode::Success } 性能优化建议 批量数据处理： 1 2 3 4 5 6 7 8 9 10 #[unsafe(no_mangle)] pub extern \u0026#34;C\u0026#34; fn process_batch( inputs: *const c_int, count: usize, outputs: *mut c_int ) { let inputs = unsafe { std::slice::from_raw_parts(inputs, count) }; let outputs = unsafe { std::slice::from_raw_parts_mut(outputs, count) }; // 批量处理... } 零拷贝接口： 1 2 3 4 5 6 7 8 9 #[unsafe(no_mangle)] pub extern \u0026#34;C\u0026#34; fn get_data(buffer: *mut u8, max_size: usize) -\u0026gt; usize { let data = generate_data(); // Rust端生成数据 let copy_size = data.len().min(max_size); unsafe { std::ptr::copy_nonoverlapping(data.as_ptr(), buffer, copy_size); } copy_size } 通过本指南，您可以系统性地掌握使用 Rust 开发生产级动态库的全流程，从项目创建、代码编写、构建系统到分发部署的完整知识体系。实际开发中应根据具体需求调整接口设计和构建流程。\n","date":"2025-08-01T16:14:03+08:00","permalink":"https://techinsight.pages.dev/p/%E4%BD%BF%E7%94%A8-rust-%E5%BC%80%E5%8F%91%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5%E5%BA%93.so%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/","title":"使用 Rust 开发动态链接库（.so）完整指南"},{"content":"一、WebAssembly简介 WebAssembly（简称Wasm）是一种二进制指令格式，设计用于在现代Web浏览器中以接近原生速度运行。它作为JavaScript的补充，为Web应用提供高性能计算能力，同时保持了Web平台的跨平台和安全特性。\n核心特点： 高性能：执行效率接近原生代码，比JavaScript快30%-50% 多语言支持：支持C/C++、Rust、Go等语言编译 安全沙箱：运行在严格隔离的环境中 跨平台：不仅限于浏览器，也可用于服务器和边缘计算 二、Wasm开发环境搭建 1. Rust环境配置（以Rust为例） 1 2 3 4 5 6 7 8 # 安装Rust工具链 curl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh # 添加Wasm编译目标 rustup target add wasm32-unknown-unknown # 安装wasm-pack工具 cargo install wasm-pack 2. 项目结构 1 2 3 4 5 wasm-demo/ ├── Cargo.toml ├── src/ │ └── lib.rs └── index.html 三、开发第一个Wasm应用 1. 配置Cargo.toml 1 2 3 4 5 6 7 8 9 10 [package] name = \u0026#34;wasm-demo\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; [lib] crate-type = [\u0026#34;cdylib\u0026#34;] [dependencies] wasm-bindgen = \u0026#34;0.2\u0026#34; 2. 编写Rust代码（src/lib.rs） 1 2 3 4 5 6 7 8 9 10 11 use wasm_bindgen::prelude::*; #[wasm_bindgen] pub fn add(a: i32, b: i32) -\u0026gt; i32 { a + b } #[wasm_bindgen] pub fn greet(name: \u0026amp;str) -\u0026gt; String { format!(\u0026#34;Hello, {}!\u0026#34;, name) } 3. 编译Wasm模块 1 wasm-pack build --target web 4. 浏览器调用（index.html） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script type=\u0026#34;module\u0026#34;\u0026gt; import init, { add, greet } from \u0026#39;./pkg/wasm_demo.js\u0026#39;; async function run() { await init(); console.log(add(5, 3)); // 输出8 console.log(greet(\u0026#34;World\u0026#34;)); // 输出\u0026#34;Hello, World!\u0026#34; } run(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 四、Wasm的扩展应用领域 1. 高性能Web应用 游戏开发：Unity和Unreal Engine支持导出为Wasm 音视频处理：FFmpeg编译为Wasm实现浏览器端视频转码 科学计算：TensorFlow.js使用Wasm加速机器学习推理 2. 跨平台开发 桌面应用：通过Tauri框架构建跨平台桌面应用 移动端：React Native结合Wasm提升性能 区块链：以太坊智能合约的Wasm实现（eWASM） 3. 服务器端应用 边缘计算：Cloudflare Workers支持Wasm实现无服务器函数 插件系统：Envoy代理的Wasm过滤器 高性能服务：数据库引擎（如SQLite）的Wasm版本 五、进阶开发技巧 1. 性能优化 1 2 3 4 5 6 7 8 #[wasm_bindgen] pub fn process_image(data: \u0026amp;[u8]) -\u0026gt; Vec\u0026lt;u8\u0026gt; { // 使用SIMD指令优化图像处理 unsafe { let output = simd_processing(data); output } } 2. 与JavaScript互操作 1 2 3 4 5 6 7 8 9 #[wasm_bindgen(module = \u0026#34;/js/utils.js\u0026#34;)] extern \u0026#34;C\u0026#34; { fn js_log(message: \u0026amp;str); } #[wasm_bindgen] pub fn log_hello() { js_log(\u0026#34;Hello from Wasm!\u0026#34;); } 3. 多线程支持 1 2 3 4 5 6 7 use wasm_bindgen::prelude::*; use rayon::prelude::*; #[wasm_bindgen] pub fn parallel_compute(input: Vec\u0026lt;f64\u0026gt;) -\u0026gt; Vec\u0026lt;f64\u0026gt; { input.par_iter().map(|x| x * 2.0).collect() } 六、Wasm生态系统工具 编译工具链：\nEmscripten（C/C++） wasm-pack（Rust） Go原生Wasm支持 运行时环境：\nWasmtime（独立运行时） Wasmer（跨平台运行时） Node.js Wasm支持 调试工具：\nChrome DevTools Wasm调试 wasm-bindgen-test测试框架 七、未来发展趋势 WASI（WebAssembly System Interface）：标准化系统接口，扩展Wasm在服务端的应用 组件模型：实现Wasm模块间的安全组合 线程和SIMD：增强并行计算能力 GC支持：简化高级语言集成 结语 WebAssembly正在重塑Web和云计算的基础架构。从简单的性能优化到复杂的跨平台应用，Wasm为开发者提供了前所未有的可能性。通过本文的入门指南，您已经掌握了Wasm开发的基础知识，可以开始探索更广阔的应用场景。\n","date":"2025-04-29T15:39:59+08:00","permalink":"https://techinsight.pages.dev/p/webassemblywasm%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97%E4%BB%8E%E5%9F%BA%E7%A1%80%E5%88%B0%E5%AE%9E%E8%B7%B5/","title":"WebAssembly（Wasm）入门指南：从基础到实践"},{"content":"引言 作为一个多年macOS用户，最近切换到Linux后，最不习惯的就是键位绑定。macOS的快捷键设计非常符合我的操作习惯，而Linux的默认键位让我感到非常不适应。为了解决这个问题，我开始了寻找键位绑定解决方案的旅程。经过一番折腾，最终找到了一个相对完美的解决方案——Toshy。在这篇博客中，我将记录下我的解决过程，希望能帮助到有同样需求的朋友。\n寻找解决方案 在Linux下模拟macOS键位绑定，我首先想到的是通过修改系统键位映射来实现。经过一番搜索，我发现了以下几种常见的方案：\nxmodmap：这是一个经典的X11工具，可以通过修改键位映射来实现自定义键位。不过，它的功能相对基础，无法实现复杂的键位绑定逻辑。\nToshy：这是一个专门为Linux设计的键位映射工具，旨在将macOS的键位绑定移植到Linux上。它支持Gnome、KDE等多种桌面环境，并且配置相对简单。最终，我决定尝试Toshy。\n安装与配置Toshy 按照Toshy的官方文档，我首先通过以下命令安装了Toshy：\n1 2 3 git clone https://github.com/RedBearAK/toshy.git cd toshy ./setup_toshy.py install 安装完成后，我启动了Toshy服务，但发现键位绑定并没有生效。于是，我查看了服务日志，发现还需要安装一个名为xremap的Gnome扩展。安装完成后，Toshy终于可以正常工作了。\n3. 解决VS Code中的快捷键冲突 虽然Toshy在大多数情况下工作良好，但在VS Code中，我发现了一些问题。例如，Ctrl+E本来是希望光标跳转到行末，但在终端中却执行了“查找文件”的操作。经过排查，我发现在VS Code的键位绑定中，可以通过调整when条件来避免这种冲突。\n具体来说，我在VS Code的keybindings.json文件中，将Ctrl+E的绑定条件改为editorTextFocused，这样它就不会在终端中生效了。类似的，其他快捷键也可以通过调整when条件来避免冲突。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [ { \u0026#34;key\u0026#34;: \u0026#34;ctrl+e\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;workbench.action.quickOpen\u0026#34;, \u0026#34;when\u0026#34;: \u0026#34;editorTextFocus\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;ctrl+e\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;-workbench.action.quickOpen\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;ctrl+p\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;workbench.action.quickOpen\u0026#34;, \u0026#34;when\u0026#34;: \u0026#34;editorTextFocus\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;ctrl+p\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;-workbench.action.quickOpen\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;ctrl+f\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;-workbench.action.terminal.focusFind\u0026#34;, \u0026#34;when\u0026#34;: \u0026#34;terminalFindFocused \u0026amp;\u0026amp; terminalHasBeenCreated || terminalFindFocused \u0026amp;\u0026amp; terminalProcessSupported || terminalFocusInAny \u0026amp;\u0026amp; terminalHasBeenCreated || terminalFocusInAny \u0026amp;\u0026amp; terminalProcessSupported\u0026#34; } ] 加上以上配置，就基本上能在终端里保持在独立终端中的体验了。\n如果还不行，可能还要在user Settings里加上这行\n1 \u0026#34;terminal.integrated.allowChords\u0026#34;: false, 这个就是避免在终端中触发全局快捷键，比如Ctrl-K作为前缀开始的一些组合操作。这些组合操作我是完全没有用过的，所以关了能解决一些问题。\n4. 解决Vim模式下的剪贴板问题 在VS Code中使用Vim模式时，我发现Ctrl+C和Ctrl+V无法正常工作，需要加上Shift才行，但我现在也没有仔细研究toshy的实现机制，所以也没办法改配置。为了解决这个问题，我希望在Vim模式下使用系统剪贴板进行复制粘贴。\n具体做法是，在Vim扩展配置中，勾选使用系统剪贴板。\n其他app如果想使用系统剪贴板可以自行搜索，一般都很简单。\n5. 总结 通过Toshy，我成功地在Linux下模拟了macOS的键位绑定，解决了大部分操作习惯上的不适。虽然在VS Code和Vim模式下遇到了一些问题，但通过调整键位绑定和Vim配置，最终也找到了解决方案。\n如果你也是一个从macOS切换到Linux的用户，希望这篇博客能帮助你更快地适应新的操作环境。如果你有更好的解决方案或建议，欢迎在评论区分享！\n参考链接 Toshy GitHub仓库 VS Code键位绑定文档 Vim剪贴板配置 ","date":"2025-03-08T10:38:14+08:00","permalink":"https://techinsight.pages.dev/p/linux%E4%B8%8B%E6%A8%A1%E6%8B%9Fmacos%E9%94%AE%E4%BD%8D%E7%BB%91%E5%AE%9A/","title":"Linux下模拟macOS键位绑定"},{"content":"引言 GraphQL 是一种用于 API 的查询语言，它允许客户端按需获取数据，而不是像 REST API 那样返回固定的数据结构。近年来，GraphQL 在微服务架构和移动端优化中得到了广泛应用。本文将带你从零开始了解 GraphQL，探讨其核心概念、适用场景，以及在生产环境中的最佳实践。\n什么是 GraphQL？ GraphQL 的核心思想是为数据访问提供一种灵活、高效的查询语言，让客户端能够精确地指定需要的数据。它的主要优势包括：\n按需查询：客户端可以精确指定需要哪些字段，避免返回不需要的数据。 减少网络请求：通过一次查询获取多个资源，减少网络请求次数。 强类型 schema：提供更好的开发体验和错误检查。 实时数据更新：支持 Subscriptions（订阅），可以实时获取数据更新。 GraphQL 与 REST 的对比 特性 REST GraphQL 数据获取 返回固定的数据结构。 按需返回数据。 网络请求 需要多次请求获取多个资源。 一次请求获取多个资源。 灵活性 灵活性较低，客户端依赖后端定义的接口。 灵活性高，客户端可以自由指定查询。 性能 可能返回过多或过少的数据。 按需查询，减少数据传输量。 适用场景 简单场景，数据需求固定。 复杂场景，数据需求灵活。 GraphQL 的适用场景 客户端需求复杂：客户端需要从多个服务获取数据，且每次需要的数据结构可能不同。 微服务架构：系统由多个微服务组成，每个微服务负责不同的领域（如用户、订单、支付等）。 快速迭代的产品：产品需求频繁变化，客户端需要快速适应新的数据需求。 多客户端支持：系统需要支持多种客户端（如 Web、iOS、Android），每个客户端的数据需求不同。 性能敏感的场景：客户端需要减少网络请求次数或减少数据传输量。 GraphQL 的多语言实现 GraphQL 是一种协议和查询语言，它的实现不依赖于特定的编程语言。以下是一些常见的 GraphQL 实现：\nJavaScript/Node.js：Apollo Server、Express-GraphQL。 Java：GraphQL Java (Spring Boot)。 Python：Graphene (Django、Flask)。 Go：gqlgen。 Ruby：GraphQL Ruby (Rails)。 .NET：GraphQL.NET (ASP.NET Core)。 Rust：Juniper。 GraphQL 作为网关的优势 在微服务架构中，GraphQL 通常作为网关处理南北流量（客户端与服务器之间的流量），其优势包括：\n减少网络传输：GraphQL 允许客户端按需获取字段，避免返回不需要的数据。 简化客户端逻辑：客户端只需与 GraphQL 网关交互，无需关心后端微服务的复杂性。 优化移动端性能：GraphQL 可以减少网络请求次数和数据量，提升加载速度。 统一入口：GraphQL 网关可以作为所有微服务的统一入口，简化客户端与后端的交互。 生产环境中的 GraphQL 方案 在生产环境中，GraphQL 的服务端方案需要满足高性能、高可用性、易扩展性和可维护性等要求。以下是一些常见的生产级 GraphQL 服务端方案：\nApollo Server：最流行的 GraphQL 服务器实现之一，支持 Node.js 和多种其他语言。 GraphQL Java (Spring Boot)：Java 生态中的 GraphQL 实现，支持 Spring Boot 集成。 Hasura：一个开源的 GraphQL 引擎，可以直接将 PostgreSQL 数据库暴露为 GraphQL API。 Apollo Federation：一种分布式 GraphQL 架构，允许将多个 GraphQL 服务合并为一个统一的 API。 GraphQL Mesh：一个工具，可以将多种数据源（如 REST、gRPC、GraphQL）聚合为一个统一的 GraphQL API。 生产环境最佳实践 性能优化：使用 DataLoader 批量处理请求，使用缓存（如 Redis）缓存频繁查询的结果。 监控与日志：使用 Apollo Studio、Prometheus 等工具监控 GraphQL 服务的性能，记录详细的日志。 安全控制：实现权限控制和数据校验，防止 GraphQL 查询过深或过大。 高可用性：部署多个实例，通过负载均衡（如 Nginx、Kubernetes）分散请求，实现熔断和限流机制。 持续集成与部署：使用 CI/CD 工具（如 Jenkins、GitLab CI）自动化测试和部署。 总结 GraphQL 并不是万能的，它的价值在复杂场景下尤为明显，但在简单场景下可能会增加开发负担。是否使用 GraphQL 应根据具体需求和团队情况来决定：\n适合使用 GraphQL 的场景：\n客户端需求复杂且频繁变化。 微服务架构复杂。 团队规模较大，前端和后端可以独立工作。 不建议使用 GraphQL 的场景：\n小型团队或简单场景。 性能要求极高。 团队技术储备不足。 如果你的项目符合 GraphQL 的适用场景，并且团队愿意投入时间学习和优化，那么 GraphQL 可以显著提高开发效率和系统灵活性。否则，使用 REST API 可能是更简单直接的选择。\n参考文献：\nGraphQL 官方文档 Apollo Server 文档 Hasura 文档 Apollo Federation 文档 ","date":"2025-02-28T16:08:11+08:00","permalink":"https://techinsight.pages.dev/p/graphql-%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5%E4%BB%8E%E9%9B%B6%E5%88%B0%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83/","title":"GraphQL 入门与实践：从零到生产环境"},{"content":"最近，我为 Grafana k6 编写了一个支持 YAR 协议的扩展，整个过程非常有趣且充满挑战。在这篇博客中，我将分享如何从零开始实现这个扩展，并解释其中的一些关键细节。特别地，我会详细说明为什么在 Go 中定义的是 NewClient 方法，但在 JavaScript 中调用的是 newClient，以及这背后的机制。\n1. 背景 什么是 Grafana k6？ Grafana k6 是一个开源负载测试工具，专注于性能和可靠性测试。它允许用户使用 JavaScript 编写测试脚本，并通过虚拟用户（VU）模拟高并发场景。\n什么是 YAR 协议？ YAR（Yet Another RPC）是一个轻量级的 RPC 协议，常用于高性能的远程服务调用。为了更好地测试 YAR 服务，我决定为 k6 编写一个扩展，使其能够直接调用 YAR 协议。\n2. 实现过程 2.1 编写 YAR 客户端 首先，我用 Go 编写了一个 YAR 客户端，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package client import ( \u0026#34;git.happyhacker.fun/frost/yargo/protocol\u0026#34; ) type YarClient struct { hostname string port int persistent bool timeout int } func (c *YarClient) Call(request protocol.Request, params ...any) (*protocol.Response, error) { // 实现 YAR 调用逻辑 } 2.2 创建 k6 扩展 接下来，我将这个 YAR 客户端封装为 k6 扩展。k6 允许通过 Go 编写扩展，并将其暴露给 JavaScript 运行时。\n1 go install go.k6.io/xk6/cmd/xk6@latest 这会安装xk6到$GOAPTH中，具体的目录是$GOPATH/bin/xk6，如果它不在你的$PATH中，可以指定绝对路径。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package yar import ( \u0026#34;go.k6.io/k6/js/modules\u0026#34; \u0026#34;git.happyhacker.fun/frost/yargo/protocol\u0026#34; ) type YarClient struct { client *client.YarClient } func init() { modules.Register(\u0026#34;k6/x/yar\u0026#34;, new(YarClient)) } func (y *YarClient) NewClient(hostname string, port int, persistent bool, timeout int) *YarClient { y.client = \u0026amp;client.YarClient{ Hostname: hostname, Port: port, Persistent: persistent, Timeout: timeout, } return y } func (y *YarClient) Call(request protocol.Request, params ...any) (*protocol.Response, error) { return y.client.Call(request, params...) } 2.3 编译 k6 扩展 使用 xk6 工具将扩展编译到 k6 二进制文件中：\n1 xk6 build --with k6-yar=. 2.4 编写 k6 测试脚本 最后，我编写了一个 k6 测试脚本来使用这个扩展：\n1 2 3 4 5 6 7 8 9 import yar from \u0026#39;k6/x/yar\u0026#39;; let client = yar.newClient(\u0026#34;127.0.0.1\u0026#34;, 8009, true, 5); export default function () { let request = yar.newRequest(\u0026#34;default\u0026#34;); let response = client.call(request); console.log(JSON.stringify(response)); } 3. 关键细节：为什么 NewClient 变成了 newClient？ 在实现过程中，我一度困惑：为什么在 Go 中定义的是 NewClient 方法，但在 JavaScript 中调用的是 newClient？这是否是一个错误？\n3.1 Go 的方法命名 在 Go 中，方法名通常采用 PascalCase（首字母大写），以表示其公开性。因此，我定义了 NewClient 方法：\n1 2 3 func (y *YarClient) NewClient(hostname string, port int, persistent bool, timeout int) *YarClient { // 初始化 YAR 客户端 } 3.2 JavaScript 的命名规则 在 JavaScript 中，函数名通常采用 camelCase（首字母小写），以符合语言的习惯用法。k6 的模块系统会自动将 Go 方法名转换为 JavaScript 风格的命名。\n因此，NewClient 在 JavaScript 中变成了 newClient。\n3.3 k6 的模块注册机制 k6 的 modules.Register 方法会将 Go 结构体的方法暴露给 JavaScript 运行时，并自动进行命名转换。例如：\nNewClient → newClient Call → call 这种转换是 k6 设计的一部分，旨在让 JavaScript 代码更加符合语言习惯。\n4. 遇到的问题与解决方案 4.1 replace 指令未生效 在 xk6 build 过程中，我发现 go mod replace 指令未生效，导致构建工具仍然尝试从原始路径下载依赖。通过显式指定 replace 标志解决了这个问题：\n1 xk6 build --with k6-yar=. --replace git.happyhacker.fun/frost/yargo=../path/to/your/yar/client 4.2 VU 配置 在测试过程中，我通过 options 灵活配置了虚拟用户（VU）的数量和行为。例如：\n1 2 3 4 export const options = { vus: 10, duration: \u0026#39;30s\u0026#39; }; 5. 总结 通过为 Grafana k6 编写 YAR 协议扩展，我不仅加深了对 k6 和 Go 的理解，还体会到了跨语言开发的乐趣。特别地，NewClient 到 newClient 的命名转换让我意识到，框架设计中的细节往往是为了让开发者更加专注于业务逻辑，而不是被语言差异所困扰。\n如果你也有兴趣为 k6 编写扩展，不妨从一个小功能开始，逐步探索其强大的能力。希望这篇博客能为你的开发之旅提供一些帮助！\n参考链接 Grafana k6 官方文档 xk6 项目 YAR 协议介绍 Happy coding! 🚀\n","date":"2025-02-25T19:00:09+08:00","permalink":"https://techinsight.pages.dev/p/%E4%B8%BA-grafana-k6-%E7%BC%96%E5%86%99-yar-%E5%8D%8F%E8%AE%AE%E6%89%A9%E5%B1%95%E4%BB%8E-go-%E5%88%B0-javascript-%E7%9A%84%E6%97%A0%E7%BC%9D%E9%9B%86%E6%88%90/","title":"为 Grafana k6 编写 YAR 协议扩展：从 Go 到 JavaScript 的无缝集成"},{"content":"","date":"2025-02-08T10:20:22+08:00","permalink":"https://techinsight.pages.dev/p/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84c%E8%AF%AD%E8%A8%80/","title":"面向对象的C语言"},{"content":"这几年入坑输入法，自己根据一堆开源项目拼凑出了基于小鹤双拼，使用小鹤音形作为辅助码的方案，使用了也快2年了，效率非常高。项目是基于雾凇拼音二次开发的，我自己的fork在小鹤流，最近同步了上游的更新之后发现我的方案无法使用了，因为几个月才同步一次，中间上游的提交非常多，所以比较难以定位是什么原因。本文记录一下排查的过程以及面对这种问题的处理方式。\n为什么要同步？ 从我用输入法多年的经验来看，一个大词库是必要的。也曾经尝试过小鹤音形方案，这是一个可以支持二码上屏的方案，也就是打有些字是不需要按空格的，这样有的需要按空格而有的不需要就会很大程度上打乱打字的节奏，所以尝试了一段时间就放弃了，但学到了这个音形方案之后我在想为什么不能把它作为辅助码呢？如果有一个大词库，那么90%的情况下是不需要使用辅助码的，和直接用双拼是一样的感受，但当真的需要快速定目标字词时可以直接输入形码定字，省去了翻页的烦恼。\n所以我拼凑出来的方案就是这个思路，而雾凇拼音最厉害的就是它的庞大的、优质的、定期维护的词库，所以就需要定期同步，前段时间比较忙，就没有怎么同步，已经5个月了。等最近同步了之后就发现已经无法使用了。\ngit bisect git bisect这个命令还真是没用过，可以简单理解是用二分查找的方式来快速定位有问题的提交。\ngit bisect good sha1用来指定功能正常的提交，git bisect bad sha2来指定功能不正常的提交，举例来说如果中间有128次提交，那么最多7次就能定位到有问题的那次提交了，但是这个方案并不适用于我这个情况。\n看一下我的项目的提交情况。\n这是用github上的Sync Fork功能实现的。那么使用git bisect会有什么问题呢？\n它会在good到bad之间的所有commit之间跳跃，但是一旦跳到了上游作者提交的commit上，就会不包含我的改动，我也就没办法测试自己的功能了。所以为了定位到具体的原因，我就先切到了最近一次合并之前的合并，然后逐个merge在那之后的上游更新（当然也是使用二分法），终于找到了引发错误的提交。\n来看下原因\n可以看到，作者把原来放在rime.lua里的内容给拆了，拆到了lua/*.lua里，拆成了很多个文件，我没有细研究这个做法，但带来的问题就是原来在xxx.scheme.yaml里的类似lua_translator@xxx或lua_filter@xxx的写法都要改成lua_translator@*xxx和lua_filter@*xxx，也许*是个通配符，是为了解决这一层多出来的目录，而我自己的方案是没有同步这个改动的，事实证明确实是这个问题，我把*加上去之后就解决了。\ngit merge 和 git rebase的区别 从这个例子中就可以深刻理解git merge和git rebase的区别了。git merge是把你的提交attache到上游的提交上，然后生成一个新的commit，只有在这个commit内才有你自己的提交，就像这样\n只有我框出来的这部分是包含了我自己的提交的，所以用git bisect跳转到其他任意的commit都无法测试我自己的改动。但这样的好处就是非常清晰（或许吧），至少可以明确看到每次合并的动作。但如果用git rebase，来看下效果\n可以看到，提交记录是一条直线，但这是有非常大的代价的，要一直解决冲突，因为这里有一个最重要的问题：到底是谁要同步谁的更新？\ngit merge做的是，把对方的更新同步到我这里，由于项目基本上都是对方的改动，而我改的东西并不影响上游，所以合并是不会有冲突的，同一个文件我这上次也是他改的，现在合并的新提交也是他改的。\n但git rebase不同，是要先把我的base切换到他的主干，然后把我的改动merge到上游的顶部，原则上不会产生一次commit，但问题在于，上游的文件是在更新，而我的分支里比如是他4月份的提交，我git rebase就是要把他去年4月份提交的文件合并到他今年1月份提交的新文件上，这是什么道理呢？所以就需要一直解决冲突，一直选择git checkout --theirs . git add . git commit -m 'use theirs'类似这种，然后git rebase --continue。\n总结 回到这个问题，git merge 和 git rebase并没有谁比谁更好，起码在这个场景，虽然花了我更多的时间去debug，但我还是认为用merge是好于rebase的。而rebase适合什么场景呢？适合的是多人协作的情况下，每个人都有自己的开发分支，但有一条主线分支，每个人提交代码时都是git rebase origin/main这样，因为每个人的分支都可以认为是比origin/master分支要更新的，所以除非两个人同时修改了同一个文件的情况，否则是不需要处理冲突的。\n","date":"2025-01-26T10:15:30+08:00","permalink":"https://techinsight.pages.dev/p/%E4%BF%AE%E5%A4%8Dgithub%E4%B8%8A%E7%9A%84%E4%B8%8A%E6%B8%B8%E6%94%B9%E5%8A%A8%E5%BC%95%E8%B5%B7%E7%9A%84%E4%B8%8D%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/","title":"修复GitHub上的上游改动引起的不兼容问题"},{"content":"在一些介绍Rust的文章中经常会说Rust具有RAII的机制，所谓的Resource Acquisition Is Initialization，也即\u0026quot;资源获取即初始化\u0026quot;，那到底什么是RAII呢？我们从资源管理最初的问题开始说起。\n没有RAII时C++是如何管理资源的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include \u0026lt;iostream\u0026gt; #include \u0026lt;fstream\u0026gt; #include \u0026lt;string\u0026gt; int main() { // 创建一个文件流对象 std::ifstream inputFile; // 打开文件 inputFile.open(\u0026#34;example.txt\u0026#34;); // 检查文件是否成功打开 if (!inputFile) { std::cerr \u0026lt;\u0026lt; \u0026#34;无法打开文件!\u0026#34; \u0026lt;\u0026lt; std::endl; return 1; // 返回错误代码 } // 处理文件内容 // 关闭文件 inputFile.close(); // 关闭文件流 return 0; // 正常结束程序 } 可以看到，打开资源之后还要手动进行关闭，否则就会产生资源泄漏，这是为什么呢？\n什么是资源？ 资源可以理解为程序在运行时锁使用的任何外部或内部的实体，包括但不限于：\n内存：动态分配的内存块（例如通过new或者malloc分配的内存） 文件句柄：打开的文件，涉及读写操作 网络连接：与外部服务或设备的连接 数据库连接：与数据库的链接，执行查询和事务 锁：在并发编程中，用于保护共享资源的机制 资源通过是有限的，程序需要合理地管理这些资源，以避免资源泄漏或者触发系统限制。\n实现RAII最简单的形式 我们知道很多面向对象的语言都有构造函数和析构函数，这不就是天然的创建和释放资源的方法吗？如果把资源的创建放在构造函数里，在析构函数里释放它，就可以通过这个包装对象来自动管理资源了。\n自定义一个对象来包装资源 1 2 3 4 5 6 7 8 9 10 11 12 class Resource { public: Resource() { inputFile.open(\u0026#34;example.txt\u0026#34;); } ~Resource() { inputFile.close(); } private: std::ifstream inputFile; } 这就是RAII最基本的形式了。\n使用C++内置的智能指针来包装资源 但如果这样是不是就太麻烦了，要给每个对象定义一个包装类，再去写它的构造函数和析构函数，有没有更简单的用法呢？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include \u0026lt;iostream\u0026gt; #include \u0026lt;fstream\u0026gt; #include \u0026lt;memory\u0026gt; int main() { // 使用智能指针管理文件流 std::unique_ptr\u0026lt;std::ifstream\u0026gt; inputFile = std::make_unique\u0026lt;std::ifstream\u0026gt;(\u0026#34;example.txt\u0026#34;); // 检查文件是否成功打开 if (!(*inputFile)) { std::cerr \u0026lt;\u0026lt; \u0026#34;无法打开文件!\u0026#34; \u0026lt;\u0026lt; std::endl; return 1; // 返回错误代码 } // 处理文件内容 // inputFile 会在超出作用域时自动关闭 return 0; // 正常结束程序 } 这样就不用再去手动close了。\n为什么资源需要手动回收? 但为什么？\n为什么资源需要手动回收呢？\n首先要明确的是，这里说的资源通常是和外部系统有交互的，所以对资源的使用很多都是有限制的，比如系统会限制一个文件被多个进程同时打开，如果不及时释放，可能会导致其他进程无法使用这个资源。\n到这里又要分两块说：无GC的语言和有GC的语言。\n对于无GC的语言，也就只能手动回收了，那有GC的语言比如PHP、Java为什么也需要呢？这是因为垃圾回收也不是时刻在进行的，手动回收是为了及时释放，避免对有限资源的长期占用。\n普通的资源回收(自己指定在何时close) 上面讲的都是要手动释放资源，即通过一个指令告知程序何时要释放资源，但大多数时候都是在方法结束时释放，而且有时打开资源的过程可能会有异常，这时最下面的资源回收的语句还没来得及执行就已经退出了，资源就泄漏了。\n改进后的资源回收(Golang的 defer Close) 为了解决上面提到的异常退出时资源无法成功回收的问题，Java做了一个try-with-resource，可以自动关闭Closable类型的资源。而Golang的方案要更灵活，只需要在打开资源之后立即写一条defer就可以了，当资源使用完毕后就会自动触发defer中的语句，而且和打开过程中是否出现异常无关。相比Java，这个通用型强了很多，而且defer的语句也不限于资源，其他的也都可以，比如退出前记录一条日志之类。\n但即便这样，还是要手动管理，这顶多算个半自动挡。\nRAII的资源回收 回到最初的问题，吹Rust的人总说它有RAII，然后C++这边就说“不就是RAII吗？搞得好像谁没有似的”，到这里就要体现出区别了。\n从上面的例子来看，C++还需要显式使用智能指针来利用RAII自动回收资源，但Rust的RAII机制是根植在语法中的。Rust的所有权系统是实现RAII的基础，每个对象都有一个所有者，并且只能被一个所有者管理，当超出所有者的作用域时，Rust就会自动调用该对象的析构函数，释放资源。而析构函数是通过实现Droptrait来定义的，当一个对象超出其作用域时，Rust会自动调用它的drop方法，从而释放关联的资源。对于文件、网络连接等资源，Rust会在对象析构时自动关闭它们。\n1 2 3 4 5 6 7 8 9 struct MyFile { file: File, } impl Drop for MyFile { fn drop(\u0026amp;mut self) { // 在此处释放资源，例如关闭文件 } } 最简单的例子 看一个最简单的例子，打开文件，读取文件，剩下的事交给Rust就好了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 use std::fs::File; use std::io::{self, Read}; fn main() -\u0026gt; io::Result\u0026lt;()\u0026gt; { let mut file = File::open(\u0026#34;file.txt\u0026#34;)?; // 打开文件 let mut contents = String::new(); file.read_to_string(\u0026amp;mut contents)?; // 读取文件内容 println!(\u0026#34;{}\u0026#34;, contents); // 打印文件内容 // 当 `file` 超出作用域时，文件会自动关闭 Ok(()) } // `file` 变量在这里超出作用域，资源会被自动释放 稍稍复杂一点的例子 上面这个和Golang的defer相比看不出明显的优势，但如果再加一个额外的使用这个file的方法呢？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 use std::fs::File; use std::io::{self, Read}; fn process_file(file: File) { // 在这里处理文件 } fn main() -\u0026gt; io::Result\u0026lt;()\u0026gt; { let mut file = File::open(\u0026#34;file.txt\u0026#34;)?; // 打开文件 let mut contents = String::new(); file.read_to_string(\u0026amp;mut contents)?; // 读取文件内容 println!(\u0026#34;{}\u0026#34;, contents); // 打印文件内容 process_file(file); // 当 `file` 超出作用域时，文件会自动关闭 Ok(()) } // `file` 变量在这里超出作用域，资源会被自动释放 把file句柄传递到别的方法里去了，还怎么在main里defer呢？这就是Golang的半自动化处理留下的问题，而Rust由于存在所有权系统，当把file作为参数传递给process_file方法之后，它的所有权就发生了转移，就只能在process_file方法里去释放它了。\n总结 本文通过一个典型的例子解释了什么是RAII，以及在不同语言中实现RAII的区别，总的来说Rust的RAII机制更加原生，当然这也是有代价的，比如上面的例子，file的所有权转移给process_file之后，main方法的下半部分就无法使用file变量了，这又涉及Clone、Copy等一堆概念了。所以给我的感觉是为了避免所有权系统带来的麻烦，用Rust可能会写出更加长的方法——既然所有权会被方法调用转移，那么就不调用其他方法了。\n","date":"2025-01-23T09:21:25+08:00","permalink":"https://techinsight.pages.dev/p/%E4%BB%8Ec-%E5%92%8Crust%E8%81%8A%E8%81%8Araii/","title":"从C++和Rust聊聊RAII"},{"content":"Jupyter Note Book是个好东西，可以很方便地写一些脚本来执行，很多时候都需要画图，但是一个很头疼的问题是画图用的matplotlib用的默认字体中不包含中文，知道可以用\n1 plt.rcParams[\u0026#39;font.family\u0026#39;] = \u0026#39;STHeiti\u0026#39; # 替换为你选择的字体 这个命令来让它显示中文了，可又不知道系统里有哪些是中文字体，而且最头疼的是你安装了一个中文字体，却不知道要怎么引用它，因为字体的文件名和注册到系统中的名字是不一定一样的，好在还有一个Python脚本可以做到这个。\n1 2 3 4 5 6 from matplotlib import pyplot as plt import matplotlib a=sorted([f.name for f in matplotlib.font_manager.fontManager.ttflist]) for i in a: print(i) 这样就可以把matplotlib能用的所有字体都打印出来，选择带heiti或者songti的就可以用了，我选择的是STHeiti，确实解决了这个问题。\n简化一下，可以下面这个脚本来只打印可能是中文的字体\n1 2 3 4 5 6 7 8 9 10 11 12 from matplotlib import pyplot as plt import matplotlib # 获取所有字体 fonts = sorted([f.name for f in matplotlib.font_manager.fontManager.ttflist]) # 筛选并打印包含特定字符串的字体 keywords = [\u0026#34;Noto\u0026#34;, \u0026#34;Songti\u0026#34;, \u0026#34;Heiti\u0026#34;] filtered_fonts = [font for font in fonts if any(keyword in font for keyword in keywords)] for font in filtered_fonts: print(font) 我系统上可用的就有Noto Sans Mono CJK SC，这个非常好了，Linux上也能用，Debian上只需要sudo apt install fonts-noto-cjk即可。\n","date":"2025-01-17T14:44:07+08:00","permalink":"https://techinsight.pages.dev/p/jupyter-matplotlib-%E7%9A%84%E4%B8%AD%E6%96%87%E5%AD%97%E4%BD%93%E9%97%AE%E9%A2%98/","title":"Jupyter Matplotlib 的中文字体问题"},{"content":"近期不知道是怎么回事，\niTerm鼠标点一下就会出现一个外框，再点一下就消失了，非常讨厌，猜测了多种可能的原因：ohmyzsh、power10k，换到最基础的bash配置还是存在，基本上就确定是iTerm的问题了，但找不到是哪里配置的这个，索性就用appcleaner卸载，把所有配置文件也都删了，然后用homebrew重新安装，结果一打开就发现了问题所在。\n不点击时这样\n点击一下就变成这样\n当然这不是我平时使用的状态，只是说明一下这个难受的状态。\n在复现这个问题的过程中又发现了一个新问题，因为开始我是用appcleaner卸载的，打开后第一时间iTerm就提示了那个让我难受的功能，还提示要不要关掉，然后我去看了下，但当我要复现这个过程，使用homebrew把它卸载了之后却没有这个弹窗了，充分说明用homebrew是卸载不干净的。\n用appcleaner卸载，又重新安装之后终于复现了\n点击那个链接就跳到这个配置页\n仔细看这个功能是为了让搜索高亮字符串的时候限制在选定的区域，看起来是个非常有用的功能，但这个展示方式真是太annoying了。\n","date":"2025-01-15T10:49:20+08:00","permalink":"https://techinsight.pages.dev/p/%E5%85%B3%E6%8E%89iterm%E4%B8%80%E4%B8%AA%E9%9D%9E%E5%B8%B8%E6%81%BC%E4%BA%BA%E7%9A%84%E5%8A%9F%E8%83%BD/","title":"关掉iTerm一个非常恼人的功能"},{"content":"在撰写技术文档或个人博客时，常常需要强调某些重要的内容或提示。为了提高内容的可读性和吸引力，我们可以使用 Callout 功能。本文将详细记录如何在 Hugo 中实现 Callout 功能，并在日后的写作中灵活运用。\n什么是 Callout？ Callout 是一种用于突出显示重要信息的设计元素，通常以框的形式呈现，并可以用不同的颜色和样式来区分内容的类型，例如信息、警告和危险等。通过使用 Callout，读者可以更容易地注意到关键信息。\n实现步骤 1. 创建短代码 在 Hugo 中，我们可以通过自定义短代码来实现 Callout 功能。以下是具体步骤：\n在项目的 layouts/shortcodes 目录中创建一个文件，命名为 callout.html。 1 2 3 4 5 6 7 8 9 10 11 12 {{ $type := .Get 0 }} \u0026lt;!-- 获取类型参数 --\u0026gt; {{ $title := .Get 1 }} \u0026lt;!-- 获取标题参数 --\u0026gt; {{ $content := .Inner }} \u0026lt;!-- 获取内容 --\u0026gt; \u0026lt;div class=\u0026#34;callout callout-{{ $type | lower }}\u0026#34;\u0026gt; \u0026lt;h5\u0026gt;{{ $title | title }}\u0026lt;/h5\u0026gt; \u0026lt;!-- 显示标题 --\u0026gt; {{ if gt (len $content) 1 }} \u0026lt;blockquote\u0026gt; {{ $content }} \u0026lt;/blockquote\u0026gt; {{ end }} \u0026lt;/div\u0026gt; 2. 添加 SCSS 样式 为了使 Callout 的显示效果更佳，我们需要为其添加样式。以下是 SCSS 样式示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 $callout-border-color: #c3e6cb; // 默认边框颜色（可根据需要调整） $callout-info-bg: #d1ecf1; $callout-info-text-color: #0c5460; $callout-warning-bg: #fff3cd; $callout-warning-text-color: #856404; $callout-danger-bg: #f8d7da; $callout-danger-text-color: #721c24; .callout { max-width: 100%; // 确保不会超出父容器的宽度 border-radius: 5px; padding: 15px; // 外边距 margin: 15px 0; // 内边距 border: 1px solid $callout-border-color; // 边框颜色 h5 { margin-top: 0; // 标题的上边距 font-size: 1.5rem; // 标题字体大小 font-weight: bold; // 加粗标题 } blockquote { max-width: 100%; // 确保不会超出父容器的宽度 margin: 10px 0; // 设置 blockquote 的上下边距 padding: 10px 15px; // 设置 blockquote 的内边距 border-left: 5px solid darken($callout-border-color, 10%); background-color: rgba(0, 0, 0, 0.03); // 可选：设置背景色来增加可读性 overflow-wrap: break-word; // 处理文本超出部分 } \u0026amp;.callout-info { background-color: $callout-info-bg; color: $callout-info-text-color; border-color: lighten($callout-info-bg, 10%); } \u0026amp;.callout-warning { background-color: $callout-warning-bg; color: $callout-warning-text-color; border-color: lighten($callout-warning-bg, 10%); } \u0026amp;.callout-danger { background-color: $callout-danger-bg; color: $callout-danger-text-color; border-color: lighten($callout-danger-bg, 10%); } } 将以上样式保存在 static/css/style.scss 或其他相应的 SCSS 文件中。\n3. 使用 Callout 短代码 在 Markdown 文件中，您可以使用以下格式来调用 Callout 短代码：\n1 2 3 4 5 6 7 这是一个标题 这是一个信息提示的内容。 这是一个标题 这是一个警告提示的内容。 这是一个标题 这是一个危险提示的内容。 这是一个标题 通过这种方式，可以轻松地在文章中插入 Callout。如果不想把内容插入到callout中，可一不写内容，只保留标题。\n结论 通过以上步骤，您可以在 Hugo 中实现和使用 Callout 功能。这不仅能增强文章的可读性，还能帮助读者更好地理解关键信息。无论是在撰写技术文档、教程还是博客文章，Callout 都是一个非常实用的工具。\n","date":"2025-01-07T16:50:45+08:00","permalink":"https://techinsight.pages.dev/p/%E5%9C%A8-hugo-%E4%B8%AD%E5%AE%9E%E7%8E%B0-callout-%E5%8A%9F%E8%83%BD%E7%9A%84%E6%8C%87%E5%8D%97/","title":"在 Hugo 中实现 Callout 功能的指南"},{"content":"在 Rust 开发中，性能基准测试是非常重要的一环，能够帮助开发者评估代码的性能并进行优化。Criterion.rs 是一个强大的 Rust 基准测试框架，它提供了简单易用的接口和丰富的功能。本文将介绍如何使用 Criterion 进行基准测试，以及一些注意事项。\n1. 什么是 Criterion.rs？ Criterion.rs 是一个用于 Rust 的基准测试库，旨在取代 Rust 标准库中的基准测试功能。它提供更精确的测量手段和更好的报告功能，能够帮助开发者深入了解代码的性能表现。\n2. 安装 Criterion.rs 要在项目中使用 Criterion.rs，首先需要在 Cargo.toml 文件中添加依赖。你可以使用以下命令来安装：\n1 2 [dev-dependencies] criterion = \u0026#34;0.5.1\u0026#34; 注意：这个依赖是添加在dev-dependencies中的，所以不能直接用cargo add criterion来安装。那么它有两种方式实现\n编辑Cargo.toml 这是废话\n通过cargo-edit 1 2 cargo install cargo-edit cargo add --dev criterion 安装cargo-edit就是为了能让cargo add支持一个--dev的选项。\n3. 创建基准测试 创建一个基准测试非常简单。首先，在项目的 benches 目录中创建一个新的基准测试文件，例如 benches/bench_hash.rs。以下是一个简单的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // benches/bench_hash.rs use criterion::{criterion_group, criterion_main, Criterion}; use deduplicate::file_info; fn bench_hash() { let path = \u0026#34;/path/to/a/file\u0026#34;; file_info::sha256_hash(path); } fn benchmark_hash(c: \u0026amp;mut Criterion) { c.bench_function(\u0026#34;benchmark_sha256\u0026#34;, |b| b.iter(|| bench_hash())); } criterion_group!(benches, benchmark_hash); criterion_main!(benches); 在这个例子中，我们定义了一个 bench_hash() 方法，用来调用你的lib.rs中已经定义过的file_info::sha256_hash()方法，这里只是为了给它一个输入参数，并创建了一个基准测试来测量其性能。c.bench_function 函数接收一个名称和一个闭包作为参数，闭包中是我们希望基准测试执行的代码。\n4. 运行基准测试 要运行基准测试，你可以使用以下命令：\n1 cargo bench 这时你会发现基准测试并没有执行，这是因为还需要在Cargo.toml中添加下列的内容\n1 2 3 [[bench] name = \u0026#34;bench_hash\u0026#34; harness = false 这里解释一下这几行的含义\nname = \u0026quot;bench_hash\u0026quot; 告诉criterion去benches目录中找bench_hash.rs或bench_hash/main.rs harness = false cargo自带的bench是使用harness，这也是个基准测试框架，但我们现在要用的是一个第三方的框架，所以就要把内置的框架给禁用掉 以此类推，还可以定义更多的[[bench]]。\n5. 输出结果 运行基准测试后，Criterion.rs 将会生成详细的输出，显示每个基准测试的平均时间、标准差、样本数量等信息。这些结果可以帮助你了解代码在不同条件下的表现。\n6. 自定义 迭代次数 对于比较慢的方法，默认会运行100个迭代，如果你的方法非常慢，就会消耗大量时间，比如像上面的给文件计算SHA256散列值的，对于大文件来说就会很慢，这时可能会希望减少执行次数来减少等待时间。通过自定义criterion_group!(benches, benchmark_hash);这行来解决\n1 2 3 4 5 criterion_group!( name = benches; config = Criterion::default().sample_size(10).warm_up_time(Duration::from_secs(20)); targets = benchmark_hash ); 其中sample_size就是迭代次数，不能低于10，warm_up_time是预热用的时间，有兴趣可以再研究其他的选项。\n注意：criterion_group!是一个宏，里面的三行结尾就是;，而不是,，我没有写错，targets可以有多个，可以自己尝试一下。\n日志 一般最简单的日志可以通过log接口和env_logger来实现，而env_logger在使用之前需要执行一次env_logger::init()，可以放在这里\n1 2 3 4 fn benchmark_hash(c: \u0026amp;mut Criterion) { env_logger::init(); c.bench_function(\u0026#34;benchmark_sha256\u0026#34;, |b| b.iter(|| bench_hash())); } 当然如果要定义多个类似benchmark_hash的函数，这也会有一些冲突，就需要once_cell或lazy_static等其他方案了。\n输出结果 安装criterion时有一个选项是使用html_report，默认会使用gnuplot来画图，但rust里有一个比较好的实现是plotters，所以如果没有安装gnuplot就会默认使用plotters。\n1 Gnuplot not found, using plotters backend 1 2 [dev-dependencies] criterion = { version = \u0026#34;0.5.1\u0026#34;, features = [\u0026#34;html_reports\u0026#34;] } 这样执行完之后会生成target/criterion目录，其中有/report/index.html文件，用浏览器打开，会看到类似这样的结果\n非常美观了，从直线的情况来看，说明并没有随着运行次数的增加性能变差。\n调优 运行性能测试肯定是为了调优，当你调整了代码，再次执行时，会得到一个类似这样的报告\n1 2 3 4 5 benchmark_sha256 time: [40.128 µs 40.160 µs 40.191 µs] change: [-0.5928% -0.3165% -0.0596%] (p = 0.04 \u0026lt; 0.05) Change within noise threshold. Found 1 outliers among 10 measurements (10.00%) 1 (10.00%) high mild 还是用截图会更直观一些，因为它带颜色\n就能看到每次的改动优化了多少，当然我这里没有改代码，所以基本上测试结果也没变化。\n而运行第二次测试之后刚才的report也会更新，新增了一部分变成这样\n可以说是非常贴心了。\n7. 注意事项 多次运行：Criterion.rs 会自动进行多次运行以提高结果的准确性。默认情况下，它会运行 100 次并根据结果进行统计分析。 测量影响：基准测试可能会受到多种因素的影响，如 CPU 温度、背景进程等。在进行基准测试时，建议尽量减少外部干扰，最好在空闲的机器上运行。 避免优化：在基准测试中，Rust 编译器可能会对未使用的结果进行优化。确保你的基准测试代码中使用了结果，可以通过将结果赋值给 _ 变量来避免这种情况。例如： 清理数据：Criterion.rs 会在每次运行后生成输出数据。可以通过删除 target/criterion 目录来清理旧的数据和基准测试结果。 使用基准聚合：你可以将多个基准测试组合在一起以获得更全面的性能评估。使用 criterion_group! 和 criterion_main! 宏来组织你的基准测试。 8. 结论 Criterion.rs 是一个强大且灵活的基准测试工具，能够帮助 Rust 开发者获取详细的性能数据。通过遵循本文的步骤和注意事项，你可以有效地使用 Criterion 进行性能基准测试，从而找到代码中的瓶颈并进行优化。希望这篇文章能够帮助你更好地理解和使用 Criterion.rs，提升你的 Rust 编程能力！\n","date":"2024-11-15T09:56:01+08:00","permalink":"https://techinsight.pages.dev/p/%E4%BD%BF%E7%94%A8-rust-criterion-%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/","title":"使用 Rust Criterion 进行性能基准测试"},{"content":"Git Revert 的各种情况及解决方法 在使用 Git 进行版本控制时，偶尔会遇到需要撤销某个提交的情况。git revert 是一个非常实用的命令，它允许你创建一个新的提交来撤销之前的更改。本文将介绍 git revert 的用法，特别是在处理合并提交时的特殊情况。\n1. 使用 git revert 撤销普通提交 如果你需要撤销一个普通的提交，可以直接使用 git revert 命令。该命令会生成一个新的提交，反转指定提交的更改。例如：\n1 git revert \u0026lt;commit-hash\u0026gt; 执行此命令后，Git 会提示你编辑提交信息，默认情况下会提供有关被撤销提交的信息。保存并退出后，将会创建一个新的提交，撤销指定的更改。\n2. 撤销合并提交 合并提交是指将两个分支合并后的提交，这种提交有多个父提交。为了撤销某个合并提交，必须使用 -m 选项来指定父提交。-m 选项后面跟上父提交的编号（1 表示第一个父提交，2 表示第二个父提交）。\n撤销合并提交的步骤如下： 找到合并提交的哈希值：\n使用 git log 查找需要撤销的合并提交的哈希值。\n1 git log 执行撤销操作：\n使用 git revert 命令，添加 -m 选项。\n1 git revert -m 1 \u0026lt;merge-commit-hash\u0026gt; 例如：\n1 git revert -m 1 abc1234 解决冲突（如果有）：\n如果出现冲突，Git 会提示你解决它们。解决后，添加更改并提交。\n1 2 git add . git commit 提交更改：\nGit 会创建一个新的提交来记录撤销操作。\n3. 使用 git reset 完全回退 如果你想要完全回退到合并之前的状态，而不想指定父提交，可以通过 git reset 命令实现。这种方法会改变提交历史。\n1 git reset --hard HEAD~1 使用这条命令将会将当前分支重置到上一个提交，丢失合并之后的所有更改。请在使用 --hard 选项之前，确保保存好你的未提交工作。\n使用 git checkout 创建新分支 如果你不想丢失当前的历史，可以选择创建一个新分支，从合并之前的状态开始：\n1 git checkout -b new-branch HEAD~1 这将创建一个新的分支 new-branch，并将其指向合并之前的状态。\n注意事项 使用 git reset --hard 命令会丢失未提交的更改，请谨慎使用。 如果在共享的分支上进行了这些操作，其他开发者可能会受到影响，因此最好在进行这些操作之前通知他们。 如果已推送合并的提交到远程仓库，使用 git reset 后需要强制推送（git push --force），这可能会影响其他开发者的工作。 结论 git revert 是一个功能强大的工具，可以帮助你安全地撤销错误的提交，尤其是合并提交。在使用时要特别注意合并提交的父提交选择，以及在公共分支上执行历史重写操作的风险。理解这些命令能够帮助你更有效地管理代码版本，确保你的项目保持在正确的轨道上。\n希望本文能帮助你更好地理解和使用 Git 的撤销功能！如果你有任何问题或想分享你的经验，欢迎留言讨论！\n","date":"2024-09-11T15:22:30+08:00","permalink":"https://techinsight.pages.dev/p/git-revert%E7%9A%84%E5%87%A0%E7%A7%8D%E6%83%85%E5%86%B5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/","title":"git revert的几种情况及解决方法"},{"content":"后记：把pyenv的代码下载下来才发现，这些版本居然都是硬编码，连所有的下载链接也都是写死的，所以如果Python更新了新版本，我还得先更新pyenv才能下载新版本的Python，这真是太离谱啦！\nPython开发者需要解决的问题可真多啊，周边的配套设施也是真多，但不管怎么多，还是免不了受到高强的阻碍。\n事情是这样的，开发服务器用的Rocky Linux8，默认的Python环境是3.6.8，太旧的了，想着安装个新版本，但用dnf安装的肯定还是很老，想着用社区的方案装一个最新的吧，又想到可能会遇到多版本的问题，找了下发现了pyenv，了解了下这个东西很像sdkman（用来管理Java相关的工具链的），可以安装不同的Python版本。按照官方文档开搞吧。\n1 curl https://pyenv.run | bash 这样就可以执行pyenv，然后把下面这段代码放在$HOME/.bashrc里，就可以使用了。\n1 2 3 4 export PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34; [[ -d $PYENV_ROOT/bin ]] \u0026amp;\u0026amp; export PATH=\u0026#34;$PYENV_ROOT/bin:$PATH\u0026#34; eval \u0026#34;$(pyenv init -)\u0026#34; eval \u0026#34;$(pyenv virtualenv-init -)\u0026#34; 让后如果你想安装一个Python3.12，就可以pyenv install 3.12，它会自动找符合这个要求的最新的版本，也就是3.12.5去下载。如果事情到这里就结束了，我想我是不会写这篇文章的。\n执行之后看到了这个提示\n1 2 --2024-08-21 07:44:27-- https://www.python.org/ftp/python/3.12.5/Python-3.12.5.tar.xz Resolving www.python.org (www.python.org)... 然后就是你知道的，它永远无法执行成功了。pyenv不懂中国人啊，连k8s都知道可以设置中国区，把相关的镜像仓库都指向了阿里云。废话不多说，先看看手动怎么解决。\n注意：这里不是要解决pip下载PyPI包的问题，而是要下载Python解释器。国内比较稳定的也就是阿里的了。\n1 https://registry.npmmirror.com/binary.html?path=python/3.12.5/ 还要注意，最终的下载地址并不是这样的。而是这样的\n1 https://registry.npmmirror.com/-/binary/python/3.12.5/Python-3.12.5.tar.xz 分析之前的输出，发现它把这个包下载到了$HOME/.pyenv/cache目录下，那么有没有可能我们自己把它下载到这里，它就不会再去下载了呢？尝试了一下果然是可以的。那么接下来就要把这个过程自动化了。写一个简单的python脚本就可以了。别奇怪这里为什么用Python脚本，每个系统都装了Python，这里没有使用任何标准库之外的东西，所以没什么关系。\n这是使用wget的版本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import os import sys import subprocess from pathlib import Path def download_file(url, filename): try: subprocess.run([\u0026#34;wget\u0026#34;, \u0026#34;-O\u0026#34;, str(filename), url], check=True) except subprocess.CalledProcessError as e: print(f\u0026#34;下载失败: {e}\u0026#34;) sys.exit(1) def main(): # 获取用户输入的Python版本 version = input(\u0026#34;请输入要安装的Python版本 (例如 3.12.5): \u0026#34;) # 构建下载URL base_url = \u0026#34;https://registry.npmmirror.com/-/binary/python\u0026#34; url = f\u0026#34;{base_url}/{version}/Python-{version}.tar.xz\u0026#34; # 设置下载目录 cache_dir = Path.home() / \u0026#34;.pyenv\u0026#34; / \u0026#34;cache\u0026#34; cache_dir.mkdir(parents=True, exist_ok=True) filename = cache_dir / f\u0026#34;Python-{version}.tar.xz\u0026#34; print(f\u0026#34;正在下载 Python {version}...\u0026#34;) download_file(url, filename) print(f\u0026#34;下载完成。文件保存在: {filename}\u0026#34;) print(f\u0026#34;开始使用 pyenv 安装 Python {version}...\u0026#34;) try: subprocess.run([\u0026#34;pyenv\u0026#34;, \u0026#34;install\u0026#34;, version], check=True) print(f\u0026#34;Python {version} 安装成功！\u0026#34;) except subprocess.CalledProcessError as e: print(f\u0026#34;安装失败: {e}\u0026#34;) sys.exit(1) if __name__ == \u0026#34;__main__\u0026#34;: main() 如果你的系统没有安装wget，还可以使用curl的版本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import os import sys import subprocess from pathlib import Path def download_file(url, filename): try: subprocess.run([\u0026#34;curl\u0026#34;, \u0026#34;-L\u0026#34;, \u0026#34;-o\u0026#34;, str(filename), url], check=True) except subprocess.CalledProcessError as e: print(f\u0026#34;下载失败: {e}\u0026#34;) sys.exit(1) def main(): # 获取用户输入的Python版本 version = input(\u0026#34;请输入要安装的Python版本 (例如 3.12.5): \u0026#34;) # 构建下载URL base_url = \u0026#34;https://registry.npmmirror.com/-/binary/python\u0026#34; url = f\u0026#34;{base_url}/{version}/Python-{version}.tar.xz\u0026#34; # 设置下载目录 cache_dir = Path.home() / \u0026#34;.pyenv\u0026#34; / \u0026#34;cache\u0026#34; cache_dir.mkdir(parents=True, exist_ok=True) filename = cache_dir / f\u0026#34;Python-{version}.tar.xz\u0026#34; print(f\u0026#34;正在下载 Python {version}...\u0026#34;) download_file(url, filename) print(f\u0026#34;下载完成。文件保存在: {filename}\u0026#34;) print(f\u0026#34;开始使用 pyenv 安装 Python {version}...\u0026#34;) try: subprocess.run([\u0026#34;pyenv\u0026#34;, \u0026#34;install\u0026#34;, version], check=True) print(f\u0026#34;Python {version} 安装成功！\u0026#34;) except subprocess.CalledProcessError as e: print(f\u0026#34;安装失败: {e}\u0026#34;) sys.exit(1) if __name__ == \u0026#34;__main__\u0026#34;: main() 其实最应该做的是让pyenv也支持自定义可用的镜像，毕竟它的识别最新小版本号的功能是可用的，现在这样需要输入很精确的版本号才能用。\n","date":"2024-08-21T18:59:46+08:00","permalink":"https://techinsight.pages.dev/p/pyenv-%E4%B8%AD%E5%9B%BD%E5%BC%80%E5%8F%91%E8%80%85%E5%BF%85%E8%AF%BB/","title":"Pyenv: 中国开发者必读"},{"content":"我觉得直接从完整的源码来解释这个过程比干巴巴的文字描述可能更有说服力。两个的区别就和Runnable跟Callable的关系一毛一样，一个没有返回值，一个有返回值。\n这例子就是完全从Guide to the Fork/Join Framework in Java借来的，在注释里加上了自己的一些尝试和理解。\nRecursiveAction没有返回值 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 package fun.happyhacker; import java.util.ArrayList; import java.util.List; import java.util.concurrent.ForkJoinTask; import java.util.concurrent.RecursiveAction; import java.util.logging.Level; import java.util.logging.Logger; public class CustomRecursiveAction extends RecursiveAction { private String workload = \u0026#34;\u0026#34;; private static final int THRESHOLD = 4; private static final Logger logger = Logger.getAnonymousLogger(); public CustomRecursiveAction(String workload) { this.workload = workload; } public static void main(String[] args) { CustomRecursiveAction task = new CustomRecursiveAction(\u0026#34;Hello World\u0026#34;); // task.compute(); // 除了手动触发compute()方法，还可以使用ForkJoinPool.commonPool()方法来执行 // ForkJoinPool.commonPool().execute(task); // task.join(); // 可能最简单的方式就是直接invoke了 // task.invoke(); // 还有不太常用的方法，手动指定哪个fork哪个join // List\u0026lt;CustomRecursiveAction\u0026gt; subActions = task.createSubtasks(); // subActions.get(0).fork(); // subActions.get(1).join(); // 最好的方式还是用ForkJoinTask.invokeAll来提交所有的task，而不是手动执行fork和join } @Override protected void compute() { if (workload.length() \u0026gt; THRESHOLD) { ForkJoinTask.invokeAll(createSubtasks()); } else { processing(workload); } } /** * 这里使用创建两个子任务，然后放在一个列表里，等待invokeAll方法执行 * * @return */ private List\u0026lt;CustomRecursiveAction\u0026gt; createSubtasks() { List\u0026lt;CustomRecursiveAction\u0026gt; subtasks = new ArrayList\u0026lt;\u0026gt;(); String partOne = workload.substring(0, workload.length() / 2); String partTwo = workload.substring(workload.length() / 2, workload.length()); subtasks.add(new CustomRecursiveAction(partOne)); subtasks.add(new CustomRecursiveAction(partTwo)); return subtasks; } private void processing(String work) { String result = work.toUpperCase(); logger.log(Level.INFO, \u0026#34;This result - ({0}) was processed by {1} ( {2})\u0026#34;, new Object[] { result, Thread.currentThread().getName(), Thread.currentThread().getId() }); } } RecusiveTask 有返回值 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 package fun.happyhacker; import java.util.ArrayList; import java.util.Arrays; import java.util.Collection; import java.util.List; import java.util.concurrent.ForkJoinTask; import java.util.concurrent.RecursiveTask; import java.util.logging.Level; import java.util.logging.Logger; public class CustomRecursiveTask extends RecursiveTask\u0026lt;Integer\u0026gt; { private final int[] arr; private static final int THRESHOLD = 20; private static final Logger logger = Logger.getAnonymousLogger(); public static void main(String[] args) { int[] arr = new int[1000000]; for (int i = 0; i \u0026lt; arr.length; i++) { arr[i] = (int) (Math.random() * 100); } CustomRecursiveTask customRecursiveTask = new CustomRecursiveTask(arr); Integer result = customRecursiveTask.compute(); logger.log(Level.INFO, \u0026#34;sum of ints between 10 and 27 and multiply by 10 is: {0}\u0026#34;, result); } public CustomRecursiveTask(int[] arr) { this.arr = arr; } @Override protected Integer compute() { if (arr.length \u0026gt; THRESHOLD) { return ForkJoinTask.invokeAll(createSubtasks()) .stream() .mapToInt(ForkJoinTask::join) .sum(); } else { return processing(arr); } } private Collection\u0026lt;CustomRecursiveTask\u0026gt; createSubtasks() { List\u0026lt;CustomRecursiveTask\u0026gt; dividedTasks = new ArrayList\u0026lt;\u0026gt;(); dividedTasks.add(new CustomRecursiveTask(Arrays.copyOfRange(arr, 0, arr.length / 2))); dividedTasks.add(new CustomRecursiveTask(Arrays.copyOfRange(arr, arr.length / 2, arr.length))); return dividedTasks; } private Integer processing(int[] arr) { return Arrays.stream(arr) .filter(a -\u0026gt; a \u0026gt; 10 \u0026amp;\u0026amp; a \u0026lt; 27) .map(a -\u0026gt; a * 10) .sum(); } } ","date":"2024-08-16T16:37:37+08:00","permalink":"https://techinsight.pages.dev/p/forkjoinpool-recursiveaction/recursivetask%E5%AE%9E%E4%BE%8B%E8%A7%A3%E6%9E%90/","title":"ForkJoinPool RecursiveAction/RecursiveTask实例解析"},{"content":"这是为搞定Flink AsyncIO做的准备工作，要搞清楚CompletableFuture。基本都是Guide To CompletableFuture这篇文章中的例子，在此基础上扩展了一些，最主要的就是在Async结尾的方法指定自定义线程池时，用默认的例子直接改成Async发现使用的仍然是ForkJoinPool.commonPool，但看前面人家的解释，说的是当并行度大于1时，所以就给这个例子加了一些并行度，也就是让这个Future内的任务本身又有一些并行，这时就符合文章的描述了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 package fun.happyhacker; import java.util.List; import java.util.concurrent.CompletableFuture; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.Future; import java.util.concurrent.ThreadFactory; import java.util.stream.Collectors; import java.util.stream.IntStream; import java.util.stream.Stream; public class CompletableFutureDemo { public static void main(String[] args) { System.out.println(\u0026#34;Hello world!\u0026#34;); } /** * .complete()直接接收一个字面量，使用场景应该比较有限 * * @return * @throws InterruptedException */ public Future\u0026lt;String\u0026gt; simpleComplete() throws InterruptedException { CompletableFuture\u0026lt;String\u0026gt; completableFuture = new CompletableFuture\u0026lt;\u0026gt;(); Executors.newCachedThreadPool().submit(() -\u0026gt; { Thread.sleep(500); completableFuture.complete(\u0026#34;Hello\u0026#34;); return null; }); return completableFuture; } /** * .supplyAsync() 接收一个 Supplier 函数，应用场景就可以比较广泛了 * * @return * @throws InterruptedException */ public Future\u0026lt;String\u0026gt; supplyAsync() throws InterruptedException { return CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Hello\u0026#34;); } /** * 把两个Future组装成一个Future，这才是使用Future的普通场景，但实际情况可能会更复杂 * * @return */ public Future\u0026lt;String\u0026gt; thenApply() { CompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Hello\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; future = completableFuture.thenApply(s -\u0026gt; s + \u0026#34; World\u0026#34;); return future; } /** * 把两个Future组装成一个Future，但最后不需要返回结果，而是直接在回调中使用结果了 * * @return */ public Future\u0026lt;Void\u0026gt; thenAccept() { CompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Hello\u0026#34;); CompletableFuture\u0026lt;Void\u0026gt; future = completableFuture .thenAccept(s -\u0026gt; System.out.println(\u0026#34;Computation returned: \u0026#34; + s)); return future; } /** * 比thenAccept更进一步，不光最后不需要返回结果，甚至在thenRun这一步也不需要知道结果，只是知道结果计算完成了就可以了，具体结果是什么，不关心 * * @return */ public Future\u0026lt;Void\u0026gt; thenRun() { CompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Hello\u0026#34;); CompletableFuture\u0026lt;Void\u0026gt; future = completableFuture.thenRun(() -\u0026gt; System.out.println(\u0026#34;Computation finished\u0026#34;)); return future; } /** * thenCompose中传入的又是一个新的Future，也就是把第一个future的结果传递给下一个future，这样就可以串起来多个future了 * * @return */ public Future\u0026lt;String\u0026gt; thenCompose() { CompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Hello\u0026#34;) .thenCompose(s -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; s + \u0026#34; Future\u0026#34;)) .thenCompose(s -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; s + \u0026#34; World\u0026#34;)); return completableFuture; } private CompletableFuture\u0026lt;String\u0026gt; composeHelper(String s) { return CompletableFuture.supplyAsync(() -\u0026gt; s + \u0026#34; World\u0026#34;); } public Future\u0026lt;String\u0026gt; thenCompose2() { CompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Hello\u0026#34;) .thenCompose(this::composeHelper); return completableFuture; } /** * 也是联合两个Future，但是它的回调函数是把前后两个Future的结果同时作为参数 * * @return */ public Future\u0026lt;String\u0026gt; thenCombine() { CompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Hello\u0026#34;) .thenCombine(CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34; Future\u0026#34;), (s1, s2) -\u0026gt; s1 + s2) .thenCombine(CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34; World\u0026#34;), (s1, s2) -\u0026gt; s1 + s2); return completableFuture; } /** * 可以直接连接多个Future，但最大的问题是无法返回结果 * * @return */ public Future\u0026lt;Void\u0026gt; allOf() { CompletableFuture\u0026lt;String\u0026gt; future1 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Hello\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; future2 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Beautiful\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; future3 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;World\u0026#34;); return CompletableFuture.allOf(future1, future2, future3); } /** * 这等于是用CompletableFuture::join方法把所有的Future都join起来，然后再把结果拼接起来 * join方法没有参数，它的作用就是等待所有的Future执行完成 * * @return */ public String join() { CompletableFuture\u0026lt;String\u0026gt; future1 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Hello\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; future2 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Beautiful\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; future3 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;World\u0026#34;); String combined = Stream.of(future1, future2, future3) .map(CompletableFuture::join) .collect(Collectors.joining(\u0026#34; \u0026#34;)); return combined; } /** * 每个Future都有一个handle方法，它的作用是把Future的结果或者异常转换成另外一个Future * handle的第一个参数是前面Future的结果，第二个参数是前面Future的异常 * * @param name * @return */ public Future\u0026lt;String\u0026gt; handleError(String name) { CompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; { if (name == null) { throw new RuntimeException(\u0026#34;Computation Error\u0026#34;); } return \u0026#34;Hello \u0026#34; + name; }).handle((s, t) -\u0026gt; s != null ? s : \u0026#34;Hello Stranger\u0026#34;); return future; } public Future\u0026lt;String\u0026gt; handleError2(String name) { CompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; { if (name == null) { throw new RuntimeException(\u0026#34;Computation Error\u0026#34;); } return \u0026#34;Hello \u0026#34; + name; }).handle((s, t) -\u0026gt; t != null ? \u0026#34;抛异常了 \u0026#34; + t.getMessage() + \u0026#34; Hello Stranger\u0026#34; : s); return future; } public Future\u0026lt;String\u0026gt; handleError3(String name) { CompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; { if (name == null) { throw new RuntimeException(\u0026#34;Computation Error\u0026#34;); } return \u0026#34;Hello \u0026#34; + name; }); future.completeExceptionally(new RuntimeException(\u0026#34;Computation Error Outer\u0026#34;)); return future; } /** * 带Async后缀的then方法默认在ForkJoinPool.commonPool()中执行，而不是与前面的Future在同一个线程中执行 * 这可以更好地利用CPU资源，避免由于前面的线程阻塞而导致的资源浪费 * * @return */ public Future\u0026lt;String\u0026gt; composeAsync() { CompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;First Task: \u0026#34; + Thread.currentThread().getName()); return \u0026#34;Hello\u0026#34;; }) .thenComposeAsync(s -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;Second Task: \u0026#34; + Thread.currentThread().getName()); return s + \u0026#34; Future\u0026#34;; })) .thenComposeAsync(s -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;Third Task: \u0026#34; + Thread.currentThread().getName()); return s + \u0026#34; World\u0026#34;; })); return completableFuture; } /** * 相应的，不带Async后缀的就是在同一个线程内执行的 * * @return */ public Future\u0026lt;String\u0026gt; composeAsync2() { CompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;First Task: \u0026#34; + Thread.currentThread().getName()); return \u0026#34;Hello\u0026#34;; }) .thenCompose(s -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;Second Task: \u0026#34; + Thread.currentThread().getName()); return s + \u0026#34; Future\u0026#34;; })) .thenCompose(s -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;Third Task: \u0026#34; + Thread.currentThread().getName()); return s + \u0026#34; World\u0026#34;; })); return completableFuture; } /** * 带Async后缀的then方法会默认在ForkJoinPool.commonPool()中执行，而不是与前面的Future在同一个线程中执行 * 也可以指定自己的线程池来执行，但一定要注意，如果任务太过简单，也就是thenComposeAsync中的逻辑太简单，它并不会按预期的在自定义的线程池中执行，而要让并行度达到某个水平才可以 * * @return */ public Future\u0026lt;String\u0026gt; composeAsync3() { ExecutorService service1 = Executors.newFixedThreadPool(3, threadFactory(\u0026#34;MyFirstPool\u0026#34;, true)); ExecutorService service2 = Executors.newFixedThreadPool(3, threadFactory(\u0026#34;MySecondPool\u0026#34;, true)); ExecutorService service3 = Executors.newFixedThreadPool(3, threadFactory(\u0026#34;MyThirdPool\u0026#34;, true)); CompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;First Task: \u0026#34; + Thread.currentThread().getName()); try { Thread.sleep(500); } catch (InterruptedException ex) { } return \u0026#34;Hello\u0026#34;; }, service1) .thenComposeAsync(s -\u0026gt; { List\u0026lt;CompletableFuture\u0026lt;String\u0026gt;\u0026gt; futures = IntStream.range(0, 10) .mapToObj(i -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; { simulateWork(500); System.out.println(\u0026#34;Second Task \u0026#34; + i + \u0026#34;: \u0026#34; + Thread.currentThread().getName()); return s + \u0026#34; Future\u0026#34; + i; }, service2)) .collect(Collectors.toList()); return CompletableFuture.allOf(futures.toArray(CompletableFuture[]::new)) .thenApply(v -\u0026gt; futures.stream() .map(CompletableFuture::join) .collect(Collectors.joining(\u0026#34;, \u0026#34;))); }, service2) .thenComposeAsync(s -\u0026gt; { List\u0026lt;CompletableFuture\u0026lt;String\u0026gt;\u0026gt; futures = IntStream.range(0, 10) .mapToObj(i -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; { simulateWork(500); System.out.println(\u0026#34;Thrird Task \u0026#34; + i + \u0026#34;: \u0026#34; + Thread.currentThread().getName()); return s + \u0026#34; Future\u0026#34; + i; }, service3)) .collect(Collectors.toList()); return CompletableFuture.allOf(futures.toArray(CompletableFuture[]::new)) .thenApply(v -\u0026gt; futures.stream() .map(CompletableFuture::join) .collect(Collectors.joining(\u0026#34;, \u0026#34;))); }, service3); // service1.shutdown(); // service2.shutdown(); // service3.shutdown(); return completableFuture; } private static void simulateWork(int milliseconds) { try { Thread.sleep(milliseconds); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } public static ThreadFactory threadFactory(String name, boolean daemon) { return runnable -\u0026gt; { Thread result = new Thread(runnable, name); result.setDaemon(daemon); return result; }; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 package fun.happyhacker; import java.util.concurrent.ExecutionException; import java.util.concurrent.Future; import static org.junit.jupiter.api.Assertions.assertEquals; import org.junit.jupiter.api.Test; public class CompletableFutureDemoTest { @Test void testSimpleComplete() throws InterruptedException, ExecutionException { Future\u0026lt;String\u0026gt; future = new CompletableFutureDemo().simpleComplete(); assertEquals(\u0026#34;Hello\u0026#34;, future.get()); } @Test void testSupplyAsync() throws InterruptedException, ExecutionException { Future\u0026lt;String\u0026gt; future = new CompletableFutureDemo().supplyAsync(); assertEquals(\u0026#34;Hello\u0026#34;, future.get()); } @Test void testThenApply() throws InterruptedException, ExecutionException { Future\u0026lt;String\u0026gt; future = new CompletableFutureDemo().thenApply(); assertEquals(\u0026#34;Hello World\u0026#34;, future.get()); } @Test void testThenAccept() throws InterruptedException, ExecutionException { Future\u0026lt;Void\u0026gt; future = new CompletableFutureDemo().thenAccept(); assertEquals(null, future.get()); } @Test void testThenRun() throws InterruptedException, ExecutionException { Future\u0026lt;Void\u0026gt; future = new CompletableFutureDemo().thenRun(); assertEquals(null, future.get()); } @Test void testThenCompose() throws InterruptedException, ExecutionException { Future\u0026lt;String\u0026gt; future = new CompletableFutureDemo().thenCompose(); assertEquals(\u0026#34;Hello Future World\u0026#34;, future.get()); } @Test void testThenCompose2() throws InterruptedException, ExecutionException { Future\u0026lt;String\u0026gt; future = new CompletableFutureDemo().thenCompose2(); assertEquals(\u0026#34;Hello World\u0026#34;, future.get()); } @Test void testThenCombine() throws InterruptedException, ExecutionException { Future\u0026lt;String\u0026gt; future = new CompletableFutureDemo().thenCombine(); assertEquals(\u0026#34;Hello Future World\u0026#34;, future.get()); } @Test void testAllOf() throws InterruptedException, ExecutionException { Future\u0026lt;Void\u0026gt; future = new CompletableFutureDemo().allOf(); future.get(); } @Test void testJoin() throws InterruptedException, ExecutionException { String str = new CompletableFutureDemo().join(); assertEquals(\u0026#34;Hello Beautiful World\u0026#34;, str); } @Test void testHandleError() throws InterruptedException, ExecutionException { Future\u0026lt;String\u0026gt; future = new CompletableFutureDemo().handleError(\u0026#34;World\u0026#34;); assertEquals(\u0026#34;Hello World\u0026#34;, future.get()); Future\u0026lt;String\u0026gt; future2 = new CompletableFutureDemo().handleError(null); assertEquals(\u0026#34;Hello Stranger\u0026#34;, future2.get()); Future\u0026lt;String\u0026gt; future3 = new CompletableFutureDemo().handleError2(null); assertEquals(\u0026#34;抛异常了 java.lang.RuntimeException: Computation Error Hello Stranger\u0026#34;, future3.get()); // Future\u0026lt;String\u0026gt; future4 = new CompletableFutureDemo().handleError3(null); // try { // future4.get(); // } catch (InterruptedException | ExecutionException e) { // assertEquals(\u0026#34;java.lang.RuntimeException: Computation Error\u0026#34;, // e.getMessage()); // } } @Test void testThenComposeAsync() throws InterruptedException, ExecutionException { Future\u0026lt;String\u0026gt; future = new CompletableFutureDemo().composeAsync(); assertEquals(\u0026#34;Hello Future World\u0026#34;, future.get()); } @Test void testThenComposeAsync2() throws InterruptedException, ExecutionException { Future\u0026lt;String\u0026gt; future = new CompletableFutureDemo().composeAsync2(); assertEquals(\u0026#34;Hello Future World\u0026#34;, future.get()); } @Test void testThenComposeAsync3() throws InterruptedException, ExecutionException { Future\u0026lt;String\u0026gt; future = new CompletableFutureDemo().composeAsync3(); future.get(); } } ","date":"2024-08-16T16:22:17+08:00","permalink":"https://techinsight.pages.dev/p/completablefuture%E5%AE%9E%E4%BE%8B%E8%AE%B2%E8%A7%A3/","title":"CompletableFuture实例讲解"},{"content":"当我们说消息队列消费时，指的是什么？\n几年前的我其实一直没太理解到底所谓的流处理和我们普通的消息队列的处理有什么区别。那时还没有可以给你任何你想要的知识的大模型，从一些文章里得到的最直观最简单的结论就是：流是连续的。\n那我们的消息队列的处理也是无限的啊，除非业务调整或者公司关门了，肯定不会说消息队列突然没了，对吧？所以这很显然不是问题的答案。而当时涉及到新项目上马和技术选型，在众多的消息处理和流处理框架中，选择了Flink。\n刚开始的业务逻辑还没那么复杂，所以虽然遇到了一些问题，但也都陆续解决了。随着业务越来越重，消费能力不足的问题逐渐暴露出来，消费能力变得很脆弱，一点小小的抖动马上就会引起Kafka topic的堆积报警，而真正的问题远不止于此。\n使用流处理框架来做消息处理的问题 流处理的过程通常不涉及外部服务的依赖，是作业内部的CPU运算，比如把数据分组、变换、求和等等，一个具体的场景可能是收集到传感器采集的数据，然后算每10秒的平均值，再计算平均数的增长率。这样的过程其实就完全依赖于作业运行的集群本身的算力，如果消息量大了，就看哪一步的计算是瓶颈，增加算子就可以了，基本上不会出现某个TaskManager上的某个节点卡住，导致作业整体背压的情况。\n但消息处理通常要处理的是更复杂的业务，具体来说就是可能需要调接口、写数据库、和各种各样的系统做交互，比如上游传来的消息是一个比较精简的版本（可能受限与最上游消息队列的长度限制），然后在处理过程中需要补充更多的详情，再进行一些判断如此这般，而依赖的这些接口性能又不完全相同，稳定性保证也不完全相同，这时候作业的消费能力或者稳定性就完全取决于这些外部服务的稳定性了。\n那你会说，这么说所有的系统不都是这样的吗？只要和外部系统交互，这个问题就一定存在。是，也不完全是。\nFlink的杀手锏同时也是软肋 Flink的杀手锏就是检查点机制，它会周期性地向数据流中插入Barrier，当每个算子接收到所有输入流中的Barrier时，会执行快照操作，保存当前状态，当作业中的所有算子全部都完成保存状态的操作，或者说检查点完成时，才继续下一轮操作。这样做的实现了分布式快照，确保了整个系统状态的一致性，还能支持故障恢复，当系统发生故障时，可以从最近的一致性检查点恢复。看起来是没什么问题，但实际上问题很大。\n最常见的问题就是某个算子中调用的某个服务偶发变慢了，这时候这个算子的处理能力就会变慢，也许这个算子只是消费的一个Topic中40个分区中的1个，但这个算子由于处理速度变慢就会更晚接收到Barrier，也就更晚地保存状态，那么其他的正常的算子呢？它们早就接收到了Barrier，保存了状态——然后等着这个最慢的算子也完成。虽然其他的算子没有问题，但由于要做一个全局的一致性检查点，其他的39个分区也会开始堆积。\n那你说堆积就堆积吧，等它自己恢复了不就好了吗？\n堆积发生时，上游仍然在生产数据，所以堆积的消息会快速增加。好了，终于等到前面那个慢的算子完成了状态提交并且在超时之前完成了检查点（算你运气好），这时候Kafka里和Flink的Source算子里都已经堆积了大量的数据，你幻想它会以最大速度开始消费？No No No，首先，它的消费能力依赖于整个链路上最慢的一环，这时候整个系统的压力可能已经达到了平时了数倍，你依赖的那些服务有那么多冗余吗？它们能承受这种突然增加数倍的请求吗？不能，那它就变慢给你看。\n结果就是，消息越堆越多，越多越慢。\n而Flink标榜的自带的内部队列、天然反压机制在这时并没有发挥作用。\n逐渐地我开始意识到了流处理和消息队列的真正区别。\nFlink的原生流处理和SparkStreaming的microbatch的区别 我们依赖的接口多数是支持批量调用的，也就是拿多个id去请求一个接口，这样比用多个id分别去发起请求调用要更高效，但Flink并不原生支持这种行为，需要自己做内部计数器实现，这就还要自己维护状态了。但它所谓的原生流处理支持，其实说的是有时间窗口和计数窗口，这也对应了我前面的说它更适合用于统计某个窗口内的数据并计算窗口之间的数据变化，其实就是聚合操作，但这种状态的维护其实还是很重量级的，尤其对于需要调用外部接口的业务而言，因为你本身是并不需要去计算这个所谓“窗口”内的数据的关系的，只是需要把他们放在一起处理而已。\n而SparkStreaming是通过小批量来模拟流的操作，这种小批量可以通过控制每隔多长时间触发一次计算或者每多少条数据触发一次计算来实现，所以某种程度上说其实更适合我们的场景。\n消息处理的瓶颈在哪里？ 消费消息队列的瓶颈在于依赖的业务接口或方法，而不在于消费消息队列本身。\n所以当我们说要扩容时，其实扩的就是依赖的业务接口。那么我们就做好给这个瓶颈扩容就好了。但现实肯定没有这么简单，有以下这些限制。\n消息队列本身的设计限制 Kafka这类基于数据分区的队列，消费者的数量不能大于分区数，不然不光不能增加消费能力，反倒会因为频繁的rebalance而让消费能力下降，而你又不能无限制地提高分区数，何况Kafka临时扩分区数量还是有风险的。\n语言的限制 像PHP这类单线程的语言，只能一个进程消费一个Kafka分区，而一个进程内的消费能力非常有限，无论如何也只能用一个核心，所以根本不可能承载比较大的数据量。\n业务场景的限制 说到底还是业务代码的处理能力太差了，尤其现在一些GPU处理图片甚至视频的场景，消息本身不多，但挡不住处理得慢啊，即便代码是Java，即便是多线程，但在这时都不顶用，慢就是慢。而你不可能因为消费能力差就去买更多的卡吧。。。。\n说了半天怎么解决呢？ 这是我设想的一个架构图。\n这个架构主要是2部分\n中间的消息代理层，负责接入各种消息队列。比如针对Kafka、Pulsar这种支持offset提交的消息队列，就通过手动维护offset来保证数据的完整性，而其他不只支持offset的，则通过维护本地文件缓存或其他方式来实现，反正就是千方百计地保证数据完整性。 协议。协议规定的是和后端业务直接的交互，比如约定只有业务正确处理了这批/条消息才返回OK，其他情况都认为是失败，这样消息代理只有收到OK时才提交offset/删除本地缓存，否则将重放这批消息。 这样的好处在于将不是瓶颈的消息拉取部分单独解耦出来，让业务开发的同事专注于业务逻辑开发，而不用关心队列消费这些事情。同时，得益于K8s这类基础设施，作为HTTP服务提供的Pod可以用Deployment的方式快速水平扩容，而由于中间层的存在，就不用受限于诸如分区数、进程/线程数这些，实现了足够的弹性。\n中间这块的逻辑计划用Golang实现，因为主要是IO的处理，数据的拉取和调用HTTP接口，可以重复利用Golang容易并发、异步IO、云原生的优势，快速启动、快速扩容，非常适合做这类中间件。\n","date":"2024-08-15T14:40:12+08:00","permalink":"https://techinsight.pages.dev/p/%E7%BB%86%E8%AF%B4%E6%B5%81%E5%A4%84%E7%90%86%E5%92%8C%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"细说流处理和消息队列的区别"},{"content":"打脸时刻到来，我还是太年轻了。\n对于macOS和Windows的对比，我只能说，得先问能不能用，再问好不好用。\n前几天买了一台机械革命15X，R7 8845HS，32G内存。而且还算是运气比较好，屏幕是京东方的，SSD是长存的PC411。系统是Windows11 23H2。\n按说这个配置已经不低了，应该原地起飞才对，但实际上不是这样。\n休眠无法唤醒。这是遇到的第一个问题，也是网上反映最多的问题之一，能确定不只是机械革命，各个品牌都有，比如ROG、联想各系列。好在北京有售后，去给升级了芯片组驱动，问题解决了。定位为驱动的问题。 还是休眠的问题，合盖装在包里莫名会发热，我理解这还是Windows的问题，网上也有很多讨论什么S0级休眠，回头我再研究下怎么解决。 还是休眠的问题，到手时SSD通电次数20次，但只有通电时间只有几分钟，姑且认为是全新的吧，但用了两周之后我又看了下，通电次数8800多次，通电时间24小时，这个通电时间我是认可的，但次数？这是在不停地自动唤醒？所以才有了上面2的问题？ 默认状态下广告满天飞啊。。。系统级的广告，搜索栏、资讯窗口啥的，不优化的话真是没法用，干扰信息太多了，微软真是不当人啊。。。 都2024年的Windows11了，广告都已经加了这么多了，居然还没有解决分辨率适配的问题。我这个是15.3寸的2560x1600，默认给的缩放比例是200%，那就是变成了10年前的1280x800的比例了，窗口组件超级大，显示效率（密度）极低。当然我可以调整缩放比例，但这时候字就变得特别小，看起来是同样的调整缩放比例，文本比窗口组件更早地变小了。然后还可以调文本缩放比例。最终把屏幕缩放比例调到150%，文本拉到120%，整体看起来还凑活了，但万恶的小而美居然直接忽视文本缩放配置，打开之后文本变成了3毫米左右，而且Windows还存在字体发虚的问题，我的眼睛都要看瞎了，而且小而美居然还不提供单独的文本缩放功能（和macOS上的钉钉有异曲同工之妙）。。。这到底是在干什么啊。不得不说这个是最影响体验的了 键盘布局，没了CMD键真是不习惯啊，用了autohotkey还是没有找到一个特别舒服的键盘布局，不过这只是习惯的问题，但我不想习惯它，我需要让它习惯我，准备用autohotkey改成和macOS一样的常用布局。还有一个小问题，是这个笔记本带数字区，虽然我明白它已经把数字区做得很窄了（一定是考虑了使用频率，这非常好），但实际使用时由于手放置的位置（基于F/J）是整体偏左的，导致看屏幕的时候就需要斜着脑袋，所以这也有点别扭。我理解了为什么Mac系列的笔记本不做数字区了。 输入法是真烂啊，我以前还想着macOS上的输入法体验没有那么好，结果Windows上的更烂，由于我在macOS上一直用的是squirrel，所以直接在Windows上装了个weasel，把配置文件直接甩过去，直接带来相同的体验。可问题还是来了，VSCode+vim扩展，用weasel打字时候选框像地震一样一直震，震得眼睛都花了，搜了一圈都说是vim扩展的问题，但我也不能不用啊。。。然后想着是不是换成搜狗就好了，好嘛，搜狗确实没这个问题，但它居然不会跟随光标，试用了一个晚上居然好几个app里不跟着光标走，我在浏览器的地址栏里打字，输入法的候选框在左下角停着。。。这时我已经麻了 edge包括Chrome浏览器也是废柴，也不知道为啥打开好多网站都提示网络无法连接，貌似是证书的问题，但用Firefox就没这个问题。但Firefox有自己的问题，看b站时播放结束就啪一下浏览器黑屏，然后过一会儿自动恢复了，所有打开的b站的标签页都变成了空白。。。这么抽象的吗？不过edge可以称作是最强PDF阅读编辑器了，技能树点偏了吧 好在office系列表现抢眼，比macOS上的体验好很多，但我不理解为什么打开会这么慢？难道是因为开启了solver加载项？ 还有一个powertoys，真不知道这玩意儿的定位，我怎么感觉这些功能应该是系统内置的呢？为啥会是一个需要单独安装的软件呢？\n环境变量真是折腾人，winget包括它的第三方前端uniget都是了色，半成品，安装了个julia，要把带版本号的路径放在Path里，那以后升级了不就不能用了吗？都已经有Homebrew这么优秀的先行者了，怎么还是搞不明白要怎么做呢？ ————————————\n记得早年知乎有个定律：先问是不是，再问为什么。\nmacOS并不稳定，尤其是高强度使用的用户。\n我以下的总结是基于身边多位同事和我自己的使用经验。\n几天不重启时有几率无法从睡眠中唤醒，键盘有反应，但屏幕不亮 变得非常卡，延迟十几秒几十秒那种，重启后恢复 所以大家都会隔几天重启一次，也不纠结了。\n我遇到最严重的bug是升级macOS 13.1（还等到发了修复版本之后才升级的）接外接显示器频繁（10分钟一次）WindowServer崩溃，效果就是突然卡住1秒然后退出到登录界面，输入密码登录之后所有工作都没有保存，这个bug非常严重，可能和外接4K显示器有关，最后找到了一些偏方，删除了显示器色彩相关的一些配置文件之后有好转（频率降低），后续连续更新了几次系统之后就没再发生了\n另外，电池鼓包这种问题已经是所有人公认的通病了，到了时间必发生。\nWindows好多年没用过了，不好评价，以我有限的认知来看，换了固态硬盘的Windows还是非常流畅的。\nmacOS让我难受的一点是有时候要关闭窗口，红绿灯按钮太小了（我知道可以CMD+Q，也知道F11是回到桌面），而Windows上想关闭窗口只需要无脑把鼠标向右向上滑动，一定会到达关闭按钮。\n","date":"2024-08-14T18:13:23+08:00","permalink":"https://techinsight.pages.dev/p/windows11-%E6%80%8E%E4%B9%88%E5%8F%AF%E4%BB%A5%E8%BF%99%E4%B9%88%E7%83%82/","title":"Windows11 怎么可以这么烂？？？"},{"content":"在Go语言中，类型断言(type assertion)和类型选择(type switch)是用于检查和操作空接口类型interface{}或any（自Go 1.18开始引入的别名）的两个重要特性。\n你的代码片段中使用了类型选择(type switch)，下面我们深入解释一下它的工作原理。\n类型选择的工作原理 类型选择用于检测并处理接口值的动态类型。类型选择的语法如下：\n1 2 3 4 5 6 7 8 9 switch v := x.(type) { case T1: // v 是 T1 类型并且持有 x 的值 case T2: // v 是 T2 类型并且持有 x 的值 // ... default: // x 不符合任何已列出的类型 } 代码解析 你的代码如下：\n1 2 3 4 5 6 7 8 var structure any switch structure := structure.(type) { case map[string]any: fmt.Printf(\u0026#34;%v\u0026#34;, structure) case []any: fmt.Printf(\u0026#34;%v\u0026#34;, structure) } 这里的类型选择的具体步骤如下：\n类型断言：\nstructure.(type)用于检测structure的动态类型。在类型选择的上下文中，structure.(type)是一个特殊的语法，用于获取接口的具体类型。\n类型选择的语法：\n类型选择用于根据接口的动态类型来执行不同的代码。具体来说，structure := structure.(type)会对structure进行类型断言，并将其结果赋值给新的变量structure（在每个case块中，structure的类型和值依据具体的类型断言而变化）。\n类型匹配：\n如果structure的动态类型是map[string]any，则执行第一个case块，且此时structure的类型就是map[string]any。 如果structure的动态类型是[]any，则执行第二个case块，且此时structure的类型就是[]any。 为什么既是类型，又是值？ 在类型选择的每个case块中，structure被重新赋值为具体类型，并且持有与其动态类型匹配的值。因此，在每个case块中：\n变量structure不仅仅是类型，还代表了实际的值。 例如，在case map[string]any:这个块中，structure的类型是map[string]any，且其值是原始structure变量中存储的map[string]any类型的值。 总结 类型选择的语法和行为可能看起来有些复杂，但实际上它非常直观地将类型断言和分支控制结合在一起：\nstructure.(type)用于检查接口的动态类型。 在每个case块中，structure不仅持有确定的类型，还持有接口的实际值。 这使得你可以针对不同的动态类型执行不同的处理逻辑，而无需显式地进行多次类型断言。 这样写法简洁且易读，避免了繁冗的类型断言和错误处理逻辑。\n","date":"2024-07-25T14:32:18+08:00","permalink":"https://techinsight.pages.dev/p/go%E8%AF%AD%E8%A8%80%E7%9A%84%E7%B1%BB%E5%9E%8B%E9%80%89%E6%8B%A9/","title":"Go语言的类型选择"},{"content":"Why? The first programming language I\u0026rsquo;ve learnt is PHP and I love it today though I don\u0026rsquo;t write it so frequently.\nThe most charming part of PHP is the design of array. It combines the concept of dict list in Python, someone would say it\u0026rsquo;s not so clear but a man who has writen PHP for a long time would say that\u0026rsquo;s a fucking good design.\nRecently I\u0026rsquo;ve fallen in love with Golang because of its simplicity and readability. But there would be some awful times when it comes to retriet a value from a map. This project is designed for this.\nUsage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; array \u0026#34;github.com/lovelock/garray\u0026#34; ) func main() { data := `{ \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Anytown\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12345\u0026#34; }, \u0026#34;phoneNumbers\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5555\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;work\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5556\u0026#34; } ], \u0026#34;projects\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Project Alpha\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 1\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-01\u0026#34;, \u0026#34;completed\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Task 2\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-15\u0026#34;, \u0026#34;completed\u0026#34;: false } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Project Beta\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;in progress\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 3\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-11-01\u0026#34;, \u0026#34;completed\u0026#34;: false }, { \u0026#34;name\u0026#34;: \u0026#34;Task 4\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-12-01\u0026#34;, \u0026#34;completed\u0026#34;: false } ] } ], \u0026#34;preferences\u0026#34;: { \u0026#34;contactMethod\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;newsletterSubscribed\u0026#34;: true, \u0026#34;languages\u0026#34;: [\u0026#34;English\u0026#34;, \u0026#34;Spanish\u0026#34;, \u0026#34;German\u0026#34;] } }` var jsonMap map[string]any json.Unmarshal([]byte(data), \u0026amp;jsonMap) completed, err := array.Get(jsonMap, \u0026#34;projects\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;tasks\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;completed\u0026#34;) if err != nil { fmt.Println(\u0026#34;statusOfTheSecondTaskOfTheFirstProject: \u0026#34;, completed) } } What problems does it solve Imagine you\u0026rsquo;ve got a complex structure like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Anytown\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12345\u0026#34; }, \u0026#34;phoneNumbers\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5555\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;work\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5556\u0026#34; } ], \u0026#34;projects\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Project Alpha\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 1\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-01\u0026#34;, \u0026#34;completed\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Task 2\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-15\u0026#34;, \u0026#34;completed\u0026#34;: false } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Project Beta\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;in progress\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 3\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-11-01\u0026#34;, \u0026#34;completed\u0026#34;: false }, { \u0026#34;name\u0026#34;: \u0026#34;Task 4\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-12-01\u0026#34;, \u0026#34;completed\u0026#34;: false } ] } ], \u0026#34;preferences\u0026#34;: { \u0026#34;contactMethod\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;newsletterSubscribed\u0026#34;: true, \u0026#34;languages\u0026#34;: [\u0026#34;English\u0026#34;, \u0026#34;Spanish\u0026#34;, \u0026#34;German\u0026#34;] } } How can you check if the first projects second task is completed?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; ) // 定义JSON结构体 type Address struct { Street string `json:\u0026#34;street\u0026#34;` City string `json:\u0026#34;city\u0026#34;` State string `json:\u0026#34;state\u0026#34;` PostalCode string `json:\u0026#34;postalCode\u0026#34;` } type PhoneNumber struct { Type string `json:\u0026#34;type\u0026#34;` Number string `json:\u0026#34;number\u0026#34;` } type Task struct { Name string `json:\u0026#34;name\u0026#34;` DueDate string `json:\u0026#34;dueDate\u0026#34;` Completed bool `json:\u0026#34;completed\u0026#34;` } type Project struct { Name string `json:\u0026#34;name\u0026#34;` Status string `json:\u0026#34;status\u0026#34;` Tasks []Task `json:\u0026#34;tasks\u0026#34;` } type Preferences struct { ContactMethod string `json:\u0026#34;contactMethod\u0026#34;` NewsletterSubscribed bool `json:\u0026#34;newsletterSubscribed\u0026#34;` Languages []string `json:\u0026#34;languages\u0026#34;` } type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` IsActive bool `json:\u0026#34;isActive\u0026#34;` Address Address `json:\u0026#34;address\u0026#34;` PhoneNumbers []PhoneNumber `json:\u0026#34;phoneNumbers\u0026#34;` Projects []Project `json:\u0026#34;projects\u0026#34;` Preferences Preferences `json:\u0026#34;preferences\u0026#34;` } func main() { // JSON数据 data := `{ \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Anytown\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12345\u0026#34; }, \u0026#34;phoneNumbers\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5555\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;work\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5556\u0026#34; } ], \u0026#34;projects\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Project Alpha\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 1\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-01\u0026#34;, \u0026#34;completed\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Task 2\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-15\u0026#34;, \u0026#34;completed\u0026#34;: false } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Project Beta\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;in progress\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 3\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-11-01\u0026#34;, \u0026#34;completed\u0026#34;: false }, { \u0026#34;name\u0026#34;: \u0026#34;Task 4\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-12-01\u0026#34;, \u0026#34;completed\u0026#34;: false } ] } ], \u0026#34;preferences\u0026#34;: { \u0026#34;contactMethod\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;newsletterSubscribed\u0026#34;: true, \u0026#34;languages\u0026#34;: [\u0026#34;English\u0026#34;, \u0026#34;Spanish\u0026#34;, \u0026#34;German\u0026#34;] } }` var person Person // 解析JSON数据 err := json.Unmarshal([]byte(data), \u0026amp;person) if err != nil { log.Fatalf(\u0026#34;Error parsing JSON: %v\u0026#34;, err) } // 获取第一个项目的第二个任务是否完成 if len(person.Projects) \u0026gt; 0 \u0026amp;\u0026amp; len(person.Projects[0].Tasks) \u0026gt; 1 { isCompleted := person.Projects[0].Tasks[1].Completed fmt.Printf(\u0026#34;The second task of the first project is completed: %v\\n\u0026#34;, isCompleted) } else { fmt.Println(\u0026#34;The required task does not exist.\u0026#34;) } } Too complicated? What if you do not want define the structs in advance?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; ) func main() { // JSON数据 data := `{ ... }` // 定义一个用来存储JSON数据的map var result map[string]interface{} // 解析JSON数据到map err := json.Unmarshal([]byte(data), \u0026amp;result) if err != nil { log.Fatalf(\u0026#34;Error parsing JSON: %v\u0026#34;, err) } // 获取第一个项目的第二个任务是否完成 projects, ok := result[\u0026#34;projects\u0026#34;].([]interface{}) if !ok || len(projects) == 0 { log.Fatalf(\u0026#34;No projects found or format is incorrect\u0026#34;) } firstProject, ok := projects[0].(map[string]interface{}) if !ok { log.Fatalf(\u0026#34;First project format is incorrect\u0026#34;) } tasks, ok := firstProject[\u0026#34;tasks\u0026#34;].([]interface{}) if !ok || len(tasks) \u0026lt; 2 { log.Fatalf(\u0026#34;No tasks found or insufficient tasks in the first project\u0026#34;) } secondTask, ok := tasks[1].(map[string]interface{}) if !ok { log.Fatalf(\u0026#34;Second task format is incorrect\u0026#34;) } completed, ok := secondTask[\u0026#34;completed\u0026#34;].(bool) if !ok { log.Fatalf(\u0026#34;Completed field is missing or not a boolean\u0026#34;) } fmt.Printf(\u0026#34;The second task of the first project is completed: %v\\n\u0026#34;, completed) } Holy shit! Too many template code. Now do you miss the PHP way?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 \u0026lt;?php $jsonStr = \u0026lt;\u0026lt;\u0026lt;EOF { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Anytown\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12345\u0026#34; }, \u0026#34;phoneNumbers\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5555\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;work\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5556\u0026#34; } ], \u0026#34;projects\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Project Alpha\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 1\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-01\u0026#34;, \u0026#34;completed\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Task 2\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-15\u0026#34;, \u0026#34;completed\u0026#34;: false } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Project Beta\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;in progress\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 3\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-11-01\u0026#34;, \u0026#34;completed\u0026#34;: false }, { \u0026#34;name\u0026#34;: \u0026#34;Task 4\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-12-01\u0026#34;, \u0026#34;completed\u0026#34;: false } ] } ], \u0026#34;preferences\u0026#34;: { \u0026#34;contactMethod\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;newsletterSubscribed\u0026#34;: true, \u0026#34;languages\u0026#34;: [\u0026#34;English\u0026#34;, \u0026#34;Spanish\u0026#34;, \u0026#34;German\u0026#34;] } } EOF; $json = json_decode($jsonStr, true); $statusOfTheSecondTaskOfTheFirstProject = $json[\u0026#39;projects\u0026#39;][1][\u0026#39;completed\u0026#39;] ?? false; var_dump($statusOfTheSecondTaskOfTheFirstProject); var_dump(isset($json[\u0026#39;projects\u0026#39;][2][\u0026#39;completed\u0026#39;])); In PHP you don\u0026rsquo;t have to check the validatbility of every level and you can use isset to check only the target key.\nIn golang operators are not allowed to be overrided, so we can use variant variables.\nExamples See the *_test.go files.\n","date":"2024-07-25T14:12:47+08:00","permalink":"https://techinsight.pages.dev/p/%E5%83%8Fphp%E9%A3%8E%E6%A0%BC%E4%B8%80%E6%A0%B7%E7%9A%84go-array/","title":"像PHP风格一样的Go Array"},{"content":"前面我写了一篇关于实现一个像PHP风格一样的Go语言map/array/slice解析的文章，在写这篇文章的过程中我还发现了一些有意思的事情，这里记录两件。\n判断一个变量的类型 比如一个变量structure的类型可能是map[string]any或者是[]any，正常我们的想法肯定是\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package mai import \u0026#34;fmt\u0026#34; func main() { var structure any switch structure.(type) { case map[string]any: s := structure.(map[string]any) fmt.Printf(\u0026#34;%v\u0026#34;, s) case []any: s := structure.([]any) fmt.Printf(\u0026#34;%v\u0026#34;, s) } } 我当时写完这段代码的第一反应就是为什么还要在里面再转换一次呢？外面都已经判断过是这样的了，肯定里面要按判断的这个类型去用啊，不过也没有多想，这个时候看到了VSCode有提示\n这个意思就是可以改写成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import \u0026#34;fmt\u0026#34; func main() { var structure any switch structure := structure.(type) { case map[string]any: fmt.Printf(\u0026#34;%v\u0026#34;, structure) case []any: fmt.Printf(\u0026#34;%v\u0026#34;, structure) } } 这个写法简直完美了，简洁又优雅，虽然感觉有点奇怪。具体的解释可以看Go语言的类型选择。\n补充：刚才验证了下，如果想直接获取一个变量的类型，正常的做法是reflect.TypeOf(structure)，而不是structure.(type)，后者就是只能在switch/case语句中使用的，所以它就是这么一个结构，无论如何，这确实是一个非常好的设计。\n使用VSCode粘贴json内容时自动生成structs 在写Garray的过程中，我发现把那段用于测试的json粘贴到Go的代码中时，它会自动转换成struct，而且是会将整个json中用到的所有struct都能定义出来，刚开始我还以为是我复制错了，直到反复确认才确定了确实是VSCode的功能，至于是哪个插件做到的我就不知道了。比如粘贴这样一段json到*.go的代码中时，粘贴进去的将是这样的一组类型定义。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Anytown\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12345\u0026#34; }, \u0026#34;phoneNumbers\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5555\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;work\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5556\u0026#34; } ], \u0026#34;projects\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Project Alpha\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 1\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-01\u0026#34;, \u0026#34;completed\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Task 2\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-15\u0026#34;, \u0026#34;completed\u0026#34;: false } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Project Beta\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;in progress\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 3\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-11-01\u0026#34;, \u0026#34;completed\u0026#34;: false }, { \u0026#34;name\u0026#34;: \u0026#34;Task 4\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-12-01\u0026#34;, \u0026#34;completed\u0026#34;: false } ] } ], \u0026#34;preferences\u0026#34;: { \u0026#34;contactMethod\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;newsletterSubscribed\u0026#34;: true, \u0026#34;languages\u0026#34;: [\u0026#34;English\u0026#34;, \u0026#34;Spanish\u0026#34;, \u0026#34;German\u0026#34;] } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 type AutoGenerated struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` IsActive bool `json:\u0026#34;isActive\u0026#34;` Address Address `json:\u0026#34;address\u0026#34;` PhoneNumbers []PhoneNumbers `json:\u0026#34;phoneNumbers\u0026#34;` Projects []Projects `json:\u0026#34;projects\u0026#34;` Preferences Preferences `json:\u0026#34;preferences\u0026#34;` } type Address struct { Street string `json:\u0026#34;street\u0026#34;` City string `json:\u0026#34;city\u0026#34;` State string `json:\u0026#34;state\u0026#34;` PostalCode string `json:\u0026#34;postalCode\u0026#34;` } type PhoneNumbers struct { Type string `json:\u0026#34;type\u0026#34;` Number string `json:\u0026#34;number\u0026#34;` } type Tasks struct { Name string `json:\u0026#34;name\u0026#34;` DueDate string `json:\u0026#34;dueDate\u0026#34;` Completed bool `json:\u0026#34;completed\u0026#34;` } type Projects struct { Name string `json:\u0026#34;name\u0026#34;` Status string `json:\u0026#34;status\u0026#34;` Tasks []Tasks `json:\u0026#34;tasks\u0026#34;` } type Preferences struct { ContactMethod string `json:\u0026#34;contactMethod\u0026#34;` NewsletterSubscribed bool `json:\u0026#34;newsletterSubscribed\u0026#34;` Languages []string `json:\u0026#34;languages\u0026#34;` } 这简直太妙了。\n","date":"2024-07-25T14:12:15+08:00","permalink":"https://techinsight.pages.dev/p/%E4%BD%BF%E7%94%A8vscode%E5%BC%80%E5%8F%91go%E7%9A%84%E5%A5%87%E5%A6%99%E4%BD%93%E9%AA%8C-part1/","title":"使用VScode开发Go的奇妙体验-part1"},{"content":"Apache Commons 是一个由Apache软件基金会维护的Java库的集合，旨在提供一系列可重用的开源Java组件。这些库通常解决了广泛的编程问题，使得Java开发者可以更加高效地编码，避免重复造轮子。Apache Commons 系列包括许多不同的组件，每个组件都专注于解冒特定的问题域。下面是一些比较常用的Apache Commons组件的简介：\nCommons Lang：\ncommons-lang 提供了一些核心的Java库的补充，这些库对于日常的Java开发非常有用。它包括工具类来处理字符串操作、数值计算、并发、反射以及更多。 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-lang3\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.14.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Commons IO：\ncommons-io 提供了一系列实用工具，以简化文件、流和文件系统的操作。这些功能包括文件复制、文件过滤、文件监视以及对输入/输出流的操作。 Commons Collections：\ncommons-collections 提供了扩展和优化的集合类，这些集合类在Java标准库中未被包含。它包括新类型的集合，如双向映射、多值映射、有序集合等。 Commons Codec：\ncommons-codec 包括对常见的编解码算法的实现，如Base64、Hex、Phonetic 和 MD5。这些工具类帮助开发者在Java应用中轻松地实现数据编解码。 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-codec\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-codec\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.17.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Commons Net：\ncommons-net 包含了许多用于开发网络应用的类，支持许多网络协议，包括FTP、SMTP、Telnet和NNTP。它为这些协议的客户端实现提供了一个框架。 Commons Math：\ncommons-math 是一个涵盖数学和统计组件的库，提供了工具类和方法，用于数学运算、统计分析和数值计算。 Commons Configuration：\ncommons-configuration 提供了各种格式的配置文件（如XML、JSON、Properties文件）的读写支持，以及配置信息的管理。 Commons DBUtils：\ncommons-dbutils 是一个小型的，用来简化JDBC的库，它封装了JDBC的操作，减少常见的数据库编程任务的代码量。 这些组件通常是通过 Maven 或 Gradle 等构建工具引入到项目中的。Apache Commons 库的主要优点是它的稳定性、广泛的测试和活跃的社区支持。使用这些库可以显著提高开发效率并增强应用程序的稳定性和功能。\n","date":"2024-07-19T14:43:20+08:00","permalink":"https://techinsight.pages.dev/p/apache-commons-%E7%B3%BB%E5%88%97%E5%BA%93%E7%AE%80%E4%BB%8B/","title":"Apache Commons 系列库简介"},{"content":" 这是GPT给的答案，看起来很不错，但实际上shardingsphere问题也很多，而且API变更非常频繁。\n在Spring Boot中实现多数据库配置、数据库分片（Sharding）、以及主从自动切换，可以依赖多种技术和框架组合。一个较为流行且被广泛认为是最优方案之一的组合是使用Spring Boot结合MyBatis或JPA，以及集成ShardingSphere作为数据库中间件。Apache ShardingSphere提供了透明化的数据分片、读写分离、分布式事务等能力。\n步骤概览 添加依赖: 在Spring Boot项目的pom.xml中引入ShardingSphere的依赖。\n配置数据源: 在application.yml或application.properties文件中配置ShardingSphere以实现多数据库集成、分片以及读写分离等。\n实现业务逻辑: 使用Spring Data JPA或MyBatis等进行数据访问。\n1. 添加依赖 以使用Spring Boot 2.x版本和ShardingSphere 5.x版本为例，首先在pom.xml中添加必要的依赖：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;!-- Spring Boot的启动器依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- ShardingSphere的数据分片和读写分离依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.shardingsphere\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;shardingsphere-jdbc-core-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.x.x\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- MyBatis或Spring Data JPA依赖（基于选择） --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-mybatis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 2. 配置数据源 接下来，在application.yml中配置ShardingSphere以实现数据源的分片和读写分离：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 spring: shardingsphere: datasource: names: ds0,ds1 # 数据源名称 ds0: # 第一个数据源配置 type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.cj.jdbc.Driver jdbc-url: jdbc:mysql://localhost:3306/ds0?serverTimezone=UTC\u0026amp;useSSL=false username: root password: password ds1: # 第二个数据源配置 type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.cj.jdbc.Driver jdbc-url: jdbc:mysql://localhost:3306/ds1?serverTimezone=UTC\u0026amp;useSSL=false username: root password: password rules: sharding: # 分片策略配置 tables: your_table: actual-data-nodes: ds$-\u0026gt;{0..1}.your_table table-strategy: inline: sharding-column: id algorithm-expression: your_table$-\u0026gt;{id % 2} binding-tables: your_table readwrite-splitting: # 读写分离配置 dataSources: ds0replica: primary-data-source-name: ds0 replica-data-source-names: ds0replica0,ds0replica1 load-balancer-name: round_robin loadBalancers: round_robin: type: ROUND_ROBIN 3. 实现业务逻辑 接下来，你可以像平常一样使用MyBatis或JPA来实现你的业务逻辑。ShardingSphere会根据你的配置自动进行数据分片和读写分离。\n注意事项 版本兼容性: 确保你所使用的Spring Boot、ShardingSphere以及任何ORM框架（如MyBatis或JPA）的版本是相互兼容的。 性能测试: 在生产环境部署前，进行全面的性能测试是非常必要的，确保配置的分片和读写分离策略符合你的性能需求。 事务管理: 如果你的应用需要处理跨多个数据库实例的事务，需要仔细考虑ShardingSphere提供的分布式事务解决方案。 通过上述步骤，你可以在Spring Boot应用中实现高效的多数据库配置、数据分片以及主从自动切换，以满足复杂的业务需求。\n","date":"2024-07-19T10:56:38+08:00","permalink":"https://techinsight.pages.dev/p/springboot%E5%BA%94%E7%94%A8%E7%9A%84sharding%E6%96%B9%E6%A1%88/","title":"SpringBoot应用的Sharding方案"},{"content":"之前体验过一把IDEA的远程开发，只能说能看出来IntelliJ想把功能做到很完善，也确实做到了，但由于占用资源太多，即使远程使用的是配置相对非常高的服务器，占用10G+的内存也不是一个好的选择吧，尝试了几次之后就被惊人的延迟和频繁丢失的输入给劝退了。\n听说VSCode的远程开发效果非常不错，所以想尝试一下。只需要给服务器开一个专用的SSH端口，把用密码登录的选项禁用掉，就相对安全了。只要有了SSH通道，整个配置就已经完成了，对，就是这么简单。\n以RockyLinux8的容器为例。\n在容器中安装并启动SSH服务 1 2 dnf install -y ssh-server /usr/sbin/ssh-server 这时不出意外会报错，因为配置文件中指定的key文件不存在，执行以下三条命令之后就好了。\n1 2 3 ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key 创建一个开发专用的账号，赋予一个登录的shell，指定Home目录，并赋予相应的权限。 1 2 3 4 5 useradd dev usermod -s /bin/bash dev usermod -aG wheel dev usermod -d /data1/code dev chown -R dev. /data1/code 好了，现在/data1/code已经是dev账户的家目录了。\n现在设置登录相关的信息。随便设置一个密码，其实可以尽量复杂一些。\n1 passwd dev #接下来要输入密码 这时候尝试从远程登录，可能会这个错误\n1 System is booting up. Unprivileged users are not permitted to log in yet. Please come back later. For technical details, see pam_nologin(8). 你知道，系统并没有正在启动，这明显是一个误报。针对不同的系统可能不同，对于Rocky Linux 8而言，需要删除/run/nologin文件，再尝试登录就可以了。\n删除的这个文件中的内容恰恰就是上面的报错信息，意外吧。\n修改sshd配置文件 密码登录既不方便也不安全，需要先用ssh-copy-id来实现证书登录，再从服务端关闭密码登录功能。\n1 ssh-copy-id dev@remote-host -p{newPort} 这就可以了。然后在服务端的/etc/ssh/sshd_config中找到\n1 PasswordAuthentication yes 把yes改成no，重启ssh-server，就会发现已经无法用密码登录了，也就安全了。\n搞定了。\n总结 最近使用VSCode比较多，我发现其实绝大多数情况下使用VSCode就已经够了，完全不需要JetBrains家的产品了，甚至JB家引以为傲的Java开发好像也没有那么大的优势了，jdt-lsp的很多提示已经比IDEA更友好了，更容易在代码上线运行之前就发现问题。举一个例子\n1 2 3 4 5 @Builder public class Data { private Integer age = 0; } 上面的代码非常具有迷惑性，因为实际运行时你会发现，如果没有执行.age()方法，这个值将是null，而不是0，这在VSCode中就会提示如果不使用@Builder.Default，则代码中指定的这个=0的值将直接被抛弃。这非常有用。\n","date":"2024-07-17T14:53:16+08:00","permalink":"https://techinsight.pages.dev/p/vscode%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E5%88%9D%E4%BD%93%E9%AA%8C/","title":"VSCode远程开发初体验"},{"content":"有时候在网上找一些Python项目来运行，需要用到的Python环境版本和本地已经安装好的不同，又不想真的重新安装一个，这时候就需要本文提到的方法了。\n在macOS上创建一个临时的Python 3.8虚拟环境，即使系统上已经安装了更高版本的Python（如3.12），你可以使用pyenv配合virtualenv来完成这个任务。这里是如何做到的：\n安装 pyenv：\n使用Homebrew来安装pyenv，可以让你在macOS上轻松地管理多个Python版本。\n打开终端并运行：\n1 2 brew update brew install pyenv 安装 Python 3.8：\n使用pyenv安装Python 3.8版本。\n1 pyenv install 3.8.12 # 选择一个3.8.x的版本 设置 pyenv 版本：\n设置你的终端会话使用刚才通过pyenv安装的Python版本。\n1 pyenv shell 3.8.12 创建虚拟环境：\n使用Python自带的模块venv来创建虚拟环境。\n1 python -m venv myenv 或者，如果你喜欢virtualenv：\n首先安装virtualenv（如果还没有安装的话）：\n1 pip install virtualenv 然后创建一个新的虚拟环境：\n1 virtualenv myenv 激活虚拟环境：\n1 source myenv/bin/activate 现在，你应该在名为myenv的虚拟环境中，并且使用的是Python 3.8版本。当你完成工作后，可以通过命令deactivate退出虚拟环境。\n如果你想确保虚拟环境被创建为临时的，只需在完成工作后删除myenv虚拟环境所在的文件夹即可。\n","date":"2024-07-12T21:26:36+08:00","image":"https://techinsight.pages.dev/images/covers/python-growing.webp","permalink":"https://techinsight.pages.dev/p/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%92%8C%E4%BD%A0%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E7%89%88%E6%9C%AC%E4%B8%8D%E5%90%8C%E7%9A%84python%E8%A7%A3%E9%87%8A%E5%99%A8/","title":"创建一个和你系统安装版本不同的Python解释器"},{"content":"PyInstaller 和 Nuitka 在将 Python 应用程序打包成独立的可执行文件时，可能会导致启动时间变慢。以下是一些可能导致启动时间变慢的原因以及相应的解决方法：\n1. 文件大小和加载时间 原因：打包后的可执行文件通常较大，包含所有依赖项和 Python 解释器。较大的文件可能需要更长的时间来加载和初始化。\n解决方法：\n使用 --onefile 选项将所有内容打包成一个单独的可执行文件，但这可能会增加加载时间。 尝试使用 --onedir 选项将所有内容打包成一个目录，这样可以减少单个文件的加载时间。 2. 代码优化 原因：打包后的可执行文件可能包含一些不必要的代码或模块，这些代码在启动时会被加载和初始化。\n解决方法：\n优化代码，移除不必要的导入和初始化操作。 使用 if __name__ == \u0026quot;__main__\u0026quot;: 确保只在主程序中执行必要的代码。 3. 冻结时间（Freeze Time） 原因：Nuitka 在编译过程中会将 Python 代码转换为 C++ 代码，并进行优化。这个过程可能会导致启动时间变慢。\n解决方法：\n使用 Nuitka 的 --standalone 选项生成独立的可执行文件，但可能会增加启动时间。 尝试使用 Nuitka 的 --onefile 选项将所有内容打包成一个单独的可执行文件，但这可能会进一步增加启动时间。 4. 动态链接库（DLL）加载 原因：打包后的可执行文件可能需要加载多个动态链接库（DLL），这可能会增加启动时间。\n解决方法：\n确保所有依赖项的 DLL 文件都正确包含在打包文件中。 使用 patchelf 工具（Linux）或 rcedit 工具（Windows）调整可执行文件的依赖项。 5. 调试信息 原因：打包过程中可能包含调试信息，这会增加文件大小并影响启动时间。\n解决方法：\n使用 --strip 选项（Linux）或 /DEBUG:NONE 选项（Windows）移除调试信息。 6. 缓存和优化 原因：首次运行打包后的可执行文件时，可能需要进行一些初始化和优化操作。\n解决方法：\n确保在打包过程中启用所有优化选项，例如 Nuitka 的 --lto 选项。 多次运行可执行文件，以利用缓存和优化。 总结 PyInstaller 和 Nuitka 打包后的可执行文件启动时间变慢可能是由于文件大小、代码优化、冻结时间、动态链接库加载、调试信息以及缓存和优化等因素导致的。通过优化打包选项、代码和依赖项，可以减少启动时间并提高性能。\n","date":"2024-07-09T14:49:27+08:00","image":"https://techinsight.pages.dev/images/covers/python-growing.webp","permalink":"https://techinsight.pages.dev/p/%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6%E5%90%AF%E5%8A%A8%E6%85%A2%E7%9A%84%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/","title":"可执行文件启动慢的原因分析"},{"content":"有几种方法可以将 Python 解释器和您的应用程序一起打包，以创建一个完全独立的可执行文件。以下是几种常见的方法：\n1. PyInstaller PyInstaller 是一个流行的工具，可以将 Python 应用程序及其所有依赖项打包成一个独立的可执行文件，适用于 Windows、Mac 和 Linux。\n安装 PyInstaller 1 pip install pyinstaller 使用 PyInstaller 1 pyinstaller --onefile your_script.py --onefile 选项将所有内容打包成一个单独的可执行文件。\n如果没有特别的要求，目前来看PyInstaller还是第一位的选择。\n如果要将一些二进制文件比如ffmpeg也打包进来，可以使用 --add-binary来进行添加，具体参数是这样 --add-binary='/path/to/ffmpeg:.，其中 /path/to/ffmpeg是二进制文件的实际路径，而后面的 .表示要打包到当前路径下，如果就多个就跟多个 --add-binary。\n完整的命令是这样的\n1 pyinstaller --onefile --add-binary=\u0026#39;/opt/homebrew/Cellar/ffmpeg/7.0.1/bin/ffmpeg:.\u0026#39; --add-binary=\u0026#39;/opt/homebrew/Cellar/ffmpeg/7.0.1/bin/ffprobe:.\u0026#39; hello.py 执行完之后会在当前路径下生成一个 hello.spec，就是把上面指定的这些东西保存到配置文件了，后面如果你再需要添加就直接往配置文件里添加即可。然后直接执行 pyinstaller hello.spec，就可以使用里面的所有配置了。\n2. Nuitka Nuitka 是一个 Python 到 C++ 的编译器，可以将 Python 代码编译成高度优化的可执行文件。\n安装 Nuitka 1 pip install nuitka 使用 Nuitka 1 nuitka --standalone --onefile your_script.py --standalone 选项生成一个独立的可执行文件，--onefile 选项将其打包成一个文件。\n3. PyOxidizer PyOxidizer 是一个工具，可以将 Python 应用程序及其所有依赖项打包成一个独立的可执行文件，支持 Windows、Mac 和 Linux。\n安装 PyOxidizer 1 cargo install pyoxidizer 使用 PyOxidizer 1 pyoxidizer build --release 这将生成一个独立的可执行文件。\n实际上pyoxidizer需要一个还挺复杂的配置文件，详情还得查看对应的官方文档。不过我发现pyinstaller和nuitka已经可以满足需求了，就没有继续研究这个了。\n4. shiv 与 Docker 虽然 shiv 本身不包含 Python 解释器，但您可以结合 Docker 来创建一个包含所有依赖项的容器镜像。\n创建 Dockerfile 1 2 3 4 5 6 7 8 9 FROM python:3.8-slim COPY . /app WORKDIR /app RUN pip install shiv RUN shiv -o my_app.pyz -c my_app . CMD [\u0026#34;python\u0026#34;, \u0026#34;my_app.pyz\u0026#34;] 构建 Docker 镜像 1 docker build -t my_app_image . 运行 Docker 容器 1 docker run -it my_app_image 这种方法将 Python 解释器和应用程序及其依赖项打包在一个 Docker 容器中。\n总结 以上方法都可以将 Python 解释器和应用程序一起打包，创建一个完全独立的可执行文件。选择哪种方法取决于您的具体需求和目标平台。\n但是测试发现PyInstaller和Nuitka打包成的可执行文件，在启动时都有一个明显的延迟，这里是解释\n","date":"2024-07-09T11:31:01+08:00","image":"https://techinsight.pages.dev/images/covers/python-growing.webp","permalink":"https://techinsight.pages.dev/p/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E5%8F%AF%E7%A7%BB%E6%A4%8D%E7%9A%84python%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83/","title":"创建一个完整的可移植的Python运行环境"},{"content":"如果shiv不能把解释器打包进去，不就丧失了可移植性吗？如果只能在当前机器上移动，这还有什么实际的应用场景呢？\n确实，shiv 本身并不包含 Python 解释器，这可能会影响其可移植性。然而，shiv 的设计目标和应用场景有其特定的考虑和优势：\nshiv 的设计目标和优势 轻量级和快速部署：shiv 生成的 .pyz 文件非常轻量级，因为它不包含 Python 解释器。这使得文件传输和部署速度更快。 依赖系统 Python：shiv 依赖于目标系统上已安装的 Python 解释器。这意味着您可以利用系统上已有的 Python 环境，而不需要额外打包解释器。 简化依赖管理：shiv 可以与 pip 和 requirements.txt 文件结合使用，简化依赖项的管理和打包过程。 实际应用场景 尽管 shiv 不包含 Python 解释器，但它仍然在以下场景中非常有用：\n内部工具和脚本：在企业内部或团队内部，假设所有目标机器都安装了兼容的 Python 环境，shiv 可以用于快速部署和共享内部工具和脚本。 持续集成和部署（CI/CD）：在 CI/CD 流程中，假设构建和部署环境都安装了兼容的 Python 环境，shiv 可以用于自动化构建和部署过程。 开发和测试环境：在开发和测试阶段，假设开发和测试机器都安装了兼容的 Python 环境，shiv 可以用于快速分发和测试代码。 临时任务和脚本：对于一些临时任务和脚本，如果目标机器已经安装了兼容的 Python 环境，shiv 可以用于快速执行这些任务。 结合其他工具增强可移植性 虽然 shiv 本身不包含 Python 解释器，但您可以结合其他工具来增强其可移植性：\nDocker：结合 Docker 容器化技术，您可以将 shiv 生成的 .pyz 文件与所需的 Python 解释器一起打包成一个 Docker 镜像。这样，您可以在任何支持 Docker 的环境中运行应用程序。\n1 2 3 4 5 6 7 8 9 FROM python:3.8-slim COPY . /app WORKDIR /app RUN pip install shiv RUN shiv -o my_app.pyz -c my_app . CMD [\u0026#34;python\u0026#34;, \u0026#34;my_app.pyz\u0026#34;] PyInstaller 或 Nuitka：如果您需要完全独立的可执行文件，可以使用 PyInstaller 或 Nuitka 将 Python 解释器和应用程序一起打包。\n总结 shiv 的设计和应用场景主要集中在轻量级和快速部署上，依赖于目标系统上已安装的 Python 环境。虽然这可能会影响其可移植性，但结合 Docker 或其他打包工具，您可以增强其可移植性，使其适用于更广泛的场景。\n","date":"2024-07-09T11:18:50+08:00","image":"https://techinsight.pages.dev/images/covers/python-growing.webp","permalink":"https://techinsight.pages.dev/p/shiv%E8%A7%A3%E5%86%B3%E4%BA%86%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/","title":"shiv解决了什么问题"},{"content":"使用 pip install dist/my_simple_package-0.1.0-py3-none-any.whl 命令安装包后，该包会被安装到你的Python环境中。具体的安装位置取决于你使用的Python环境类型和配置。\n安装位置 系统全局环境：\n如果使用系统Python并且没有激活任何虚拟环境，包通常安装在Python的 site-packages 目录中。例如：\n1 /usr/local/lib/python3.9/site-packages/my_simple_package 你可以通过以下命令查看 site-packages 目录的位置：\n1 2 import site print(site.getsitepackages()) 虚拟环境：\n如果使用的是虚拟环境，包会安装在虚拟环境的 site-packages 目录中。例如：\n1 /path/to/venv/lib/python3.9/site-packages/my_simple_package 你可以通过以下命令查看虚拟环境的 site-packages 目录位置：\n1 2 import site print(site.getsitepackages()) 用户目录：\n如果使用 --user 选项进行安装，包会被安装到用户目录下的 site-packages 目录。例如：\n1 ~/.local/lib/python3.9/site-packages/my_simple_package 引用安装的包 一旦包被安装到Python环境中，你可以像引用任何其他标准库或已安装的第三方包一样引用它。假设包名是 my_simple_package，可以在Python脚本或交互式环境中使用以下代码引用该包：\n1 2 3 4 import my_simple_package.simple_module as sm print(sm.hello()) # Output: Hello, World! 检查包是否成功安装 你可以通过以下几种方法检查包是否成功安装：\n使用 pip list 查看已安装的包：\n1 pip list | grep my_simple_package 使用 pip show 获取包的详细信息：\n1 pip show my_simple_package 在Python环境中尝试导入包：\n1 2 import my_simple_package print(my_simple_package) 示例 假设你已经创建并安装了一个名为 my_simple_package 的包，以下是一个完整的示例流程：\n创建并安装包：\n1 2 python setup.py sdist bdist_wheel pip install dist/my_simple_package-0.1.0-py3-none-any.whl 引用包：\n1 2 3 4 import my_simple_package.simple_module as sm print(sm.hello()) # Output: Hello, World! 通过这些步骤，你可以成功安装和引用一个本地创建并通过 pip 安装的Python包。\n","date":"2024-07-08T15:07:16+08:00","image":"https://techinsight.pages.dev/images/covers/python-growing.webp","permalink":"https://techinsight.pages.dev/p/pip%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85%E5%9C%A8%E5%93%AA%E9%87%8C/","title":"pip安装的包在哪里？"},{"content":"要创建一个最简单的 Python 包来演示包的作用，你只需创建一个包含必要文件和目录结构的项目。这个示例包不会有实际的功能，但它会展示如何构建和打包一个 Python 项目。\n目录结构 1 2 3 4 5 6 7 my_simple_package/ ├── my_simple_package/ │ ├── __init__.py │ └── simple_module.py ├── README.md ├── setup.py └── pyproject.toml 详细步骤 创建包目录\n首先创建一个名为 my_simple_package 的目录，这是你的项目根目录。\n创建包的子目录\n在项目根目录下，再创建一个同名的子目录 my_simple_package，这是实际的包目录。\n创建 __init__.py 文件\n在 my_simple_package 目录下创建一个空的 __init__.py 文件，这个文件告诉 Python 这个目录是一个包。\n1 # my_simple_package/__init__.py 创建一个简单的模块\n在 my_simple_package 目录下创建一个名为 simple_module.py 的文件，并在其中添加一些简单的代码。\n1 2 3 4 # my_simple_package/simple_module.py def hello(): return \u0026#34;Hello, World!\u0026#34; 创建 README 文件\n在项目根目录下创建一个 README.md 文件，为包提供一些基本的描述。\n1 2 3 # My Simple Package This is a simple package to demonstrate how to create a Python package. 创建 setup.py 文件\n在项目根目录下创建一个 setup.py 文件，用于设置包的元数据和安装信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # setup.py from setuptools import setup, find_packages setup( name=\u0026#34;my_simple_package\u0026#34;, version=\u0026#34;0.1.0\u0026#34;, packages=find_packages(), install_requires=[], # List your package dependencies here author=\u0026#34;Your Name\u0026#34;, author_email=\u0026#34;your.email@example.com\u0026#34;, description=\u0026#34;A simple package to demonstrate packaging in Python\u0026#34;, long_description=open(\u0026#39;README.md\u0026#39;).read(), long_description_content_type=\u0026#34;text/markdown\u0026#34;, url=\u0026#34;http://example.com/my_simple_package\u0026#34;, classifiers=[ \u0026#34;Programming Language :: Python :: 3\u0026#34;, \u0026#34;License :: OSI Approved :: MIT License\u0026#34;, \u0026#34;Operating System :: OS Independent\u0026#34;, ], python_requires=\u0026#39;\u0026gt;=3.6\u0026#39;, ) 创建 pyproject.toml 文件\n在项目根目录下创建一个 pyproject.toml 文件，指定构建系统的要求。\n1 2 3 4 # pyproject.toml [build-system] requires = [\u0026#34;setuptools\u0026#34;, \u0026#34;wheel\u0026#34;] build-backend = \u0026#34;setuptools.build_meta\u0026#34; 构建和安装 构建包\n在项目根目录下运行以下命令，生成源代码分发包和二进制分发包：\n1 python setup.py sdist bdist_wheel 这将会在 dist 目录下生成 .tar.gz 和 .whl 文件。\n安装包\n你可以使用 pip 安装生成的包文件：\n1 pip install dist/my_simple_package-0.1.0-py3-none-any.whl 那么安装了之后的包去了哪里呢？参考pip安装的包去了哪里\n测试包 你可以在 Python 环境中测试你的包是否正常工作：\n1 2 3 4 import my_simple_package.simple_module as sm print(sm.hello()) # Output: Hello, World! 总结 这个简单的 Python 包演示了包的基本结构和构建过程。通过这个示例，你可以了解如何创建、打包和分发一个 Python 包。尽管这个包没有实际的功能，但它展示了所有必要的步骤和文件。\n","date":"2024-07-08T15:04:13+08:00","image":"https://techinsight.pages.dev/images/covers/python-growing.webp","permalink":"https://techinsight.pages.dev/p/%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84python%E5%8C%85/","title":"构建一个最简单的Python包"},{"content":"虽然 Python 是一种解释型脚本语言，不需要像 C 或 C++ 那样进行编译，但在开发和发布 Python 项目时，仍然有一些构建步骤是必要的。这些步骤可以包括依赖管理、打包、测试、文档生成等。因此，Python 项目通常需要一些构建工具来简化和自动化这些任务。\n常见的 Python 构建工具 setuptools：\n功能：提供基础的打包和分发功能，支持依赖管理、入口点和扩展等。 使用：通过 setup.py 和 pyproject.toml 文件进行配置。 wheel：\n功能：将 Python 项目打包成 .whl 格式的二进制分发包，方便用户安装。 使用：通常与 setuptools 一起使用，通过 python setup.py bdist_wheel 命令生成。 pip：\n功能：主要用于包管理和安装，但也支持从源代码进行构建和安装。 使用：通过 requirements.txt 或 pyproject.toml 文件定义依赖。 build：\n功能：一个现代的构建工具，用于生成源代码分发包（sdist）和二进制分发包（wheel）。 使用：通过 python -m build 命令执行。 tox：\n功能：用于自动化测试和环境管理，支持在多个 Python 版本和虚拟环境中运行测试。 使用：通过 tox.ini 文件进行配置。 flit：\n功能：一个简化的打包和发布工具，主要用于纯 Python 包。 使用：通过 pyproject.toml 文件进行配置。 poetry：\n功能：一个现代的依赖管理和打包工具，集成了构建、依赖管理、版本管理和发布等功能。 使用：通过 pyproject.toml 文件进行配置，并且有自己的 poetry.lock 文件。 pbr：\n功能：专注于简化 setuptools 配置，自动化版本控制和包信息管理。 使用：通过 setup.cfg 文件进行配置。 为什么 Python 需要构建？ 尽管 Python 是解释型语言，但在实际开发和发布过程中，仍然有一些任务需要构建步骤：\n依赖管理：\n确保项目在不同环境中有相同的依赖版本，避免兼容性问题。 打包和分发：\n将项目打包成标准格式（如 wheel 或 sdist），方便用户安装和使用。 性能优化：\n通过构建，将一些关键模块编译为 C 扩展，以提高性能。 代码检查和测试：\n自动化运行代码检查工具（如 flake8、pylint）和测试框架（如 pytest），确保代码质量。 文档生成：\n使用工具（如 Sphinx）生成项目文档，确保文档的一致性和可维护性。 构建生成的结果 源代码分发包（sdist）：\n包含项目的源代码和元数据，通常是一个 .tar.gz 文件。 通过 python setup.py sdist 或 python -m build 生成。 二进制分发包（wheel）：\n包含编译后的二进制文件，通常是一个 .whl 文件。 通过 python setup.py bdist_wheel 或 python -m build 生成。 安装包：\n最终用户可以通过 pip install 命令安装打包好的项目，而不需要关心底层的构建细节。 总结 尽管 Python 是解释型语言，但项目的开发和发布过程仍然需要构建步骤来管理依赖、打包、测试和优化。通过使用适当的构建工具，可以显著简化和自动化这些任务，提高项目的可维护性和可移植性。\n","date":"2024-07-08T14:25:52+08:00","image":"https://techinsight.pages.dev/images/covers/python-builders.png","permalink":"https://techinsight.pages.dev/p/python%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/","title":"Python构建工具"},{"content":"pyproject.toml 文件是 Python 项目的新标准配置文件，它遵循 PEP 517 和 PEP 518 的规范。这个文件的主要目的是定义项目构建系统的要求，从而使得项目的构建过程更加标准化和独立于具体的构建工具。\npyproject.toml 的主要功能 定义构建系统要求：pyproject.toml 文件中可以指定项目所需的构建工具及其版本。例如：\n1 2 3 [build-system] requires = [\u0026#34;setuptools\u0026#34;, \u0026#34;wheel\u0026#34;] build-backend = \u0026#34;setuptools.build_meta\u0026#34; 这表示项目需要 setuptools 和 wheel 这两个工具来进行构建，并且使用 setuptools.build_meta 作为构建后端。\n项目元数据：除了构建系统要求，pyproject.toml 还可以包含项目的其他元数据，例如项目名称、版本、作者等信息。这些信息通常在 setup.py 中通过 setup() 函数来定义。\n1 2 3 4 5 6 7 [project] name = \u0026#34;example\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;An example package\u0026#34; authors = [ { name=\u0026#34;John Doe\u0026#34;, email=\u0026#34;john.doe@example.com\u0026#34; } ] 其他配置：pyproject.toml 还可以包含其他工具的配置，例如 pytest、black 等，使得项目的配置更加集中和统一。\n与 setup.py 的比较 标准化：pyproject.toml 提供了一种标准化的方式来定义构建系统要求，使得项目的构建过程更加一致和可预测。 独立性：pyproject.toml 使得项目的构建过程独立于具体的构建工具，用户不需要了解或修改 setup.py 文件，只需要安装指定的构建工具即可。 扩展性：pyproject.toml 可以包含更多的配置信息，不仅限于构建系统要求，还可以包含项目的其他元数据和工具配置。 之前的构建工具定义方式 在 pyproject.toml 出现之前，项目的构建工具通常在 setup.py 文件中定义。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from setuptools import setup setup( name=\u0026#39;example\u0026#39;, version=\u0026#39;0.1.0\u0026#39;, packages=[\u0026#39;example\u0026#39;], install_requires=[ \u0026#39;some_dependency\u0026#39;, ], entry_points={ \u0026#39;console_scripts\u0026#39;: [ \u0026#39;example-cli=example.main:main\u0026#39;, ], }, ) 这种方式的缺点是：\n依赖隐式：用户需要阅读 setup.py 文件才能了解项目所需的构建工具和依赖。 不标准化：不同的项目可能使用不同的方式来定义构建工具，导致构建过程不一致。 总结 pyproject.toml：提供了一种标准化的方式来定义构建系统要求，使得项目的构建过程更加一致和可预测。 功能增强：pyproject.toml 不仅限于构建系统要求，还可以包含项目的其他元数据和工具配置。 之前的定义方式：通常在 setup.py 文件中定义，依赖隐式且不标准化。 通过使用 pyproject.toml，Python 项目的构建过程变得更加标准化和独立于具体的构建工具，提高了项目的可维护性和可移植性。\n","date":"2024-07-08T14:23:21+08:00","image":"https://techinsight.pages.dev/images/covers/python-pyproject.png","permalink":"https://techinsight.pages.dev/p/pyproject%E7%AE%80%E6%98%8E%E4%BB%8B%E7%BB%8D/","title":"Pyproject简明介绍"},{"content":"引用一下Flutter中的章节，说明需要用到shiv的地方。\n我们知道，用Go语言写出来的程序是平台有关的二进制文件，随便复制到相同的平台下都可以运行，但Python很多时候不是，因为用PyPI（pip）安装的包并不会打到Python脚本里去，直接拿一个引用了很多其他包的脚本放在其他机器上是无法执行的，所以需要一个把整个运行环境都复制出来可随处移动的工具，而Shiv就是干这个用的。\nPython 项目管理方式介绍 这是理解shiv的工作原理的重点，希望能仔细理解。\nsetup.py 这应该是最传统的方式了，把基础信息和包的信息都写在一个py脚本里，基本的样式如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 # setup.py from setuptools import setup, find_packages setup( name=\u0026#39;example\u0026#39;, version=\u0026#39;0.1\u0026#39;, packages=find_packages(), entry_points={ \u0026#39;console_scripts\u0026#39;: [ \u0026#39;example-cli=example.main:main\u0026#39;, ], }, ) 简单介绍一下这些字段的含义。\nname就是项目名了，注意是项目名，而不是最终生成的可执行文件的名字，version是版本号，不再过多解释。\npackages=find_packages() 值得解释一下。正常来说这里是要手写一些包的，但这里使用了一个方法来动态查找这些包。\n那么问题来了，什么是包呢？\n在其他语言里，通常一级目录就是一个包了，但Python里并不是，还需要一个通常为空的文件 __init__.py，可以查一下为什么要有这个文件。当存在这个文件的时候才认为这是一个包，比如说我们有一个包 example，就需要是这样的结构\n1 2 3 4 5 -- example_project -- setup.py -- example -- __init__.py -- main.py 正常来说，这时候在 packages处应该填写的是 ['example']，但由于这个规则的存在，可以使用 find_packages方法查找整个目录下所有存在 __init__.py的目录，把它们的名字放在 packages里。当然这个方法还有别的参数，这里就不再赘述。\n下面就是程序的可执行文件了。顾名思义，console_scripts就是在终端能直接使用的脚本名，作为脚本语言，Python有两种写法\n1 print(\u0026#39;hello world\u0026#39;) 这时候直接 python main.py是可以的，但在标准化的项目里，通常是像下面这样的写法\n1 2 3 4 5 def main(): print(\u0026#39;hello world\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: main() 这里就定义了一个 main方法，其实就类似于C系列语言中的main函数了。\n问题讨论到这里就已经很清晰了，我们有了包名 example，里面有一个 hello模块，其实就是 hello.py，现在又有了方法名 main，其实已经可以引用它了，没错就是 example.hello:main\n搞清楚了这些问题，就可以看看Python包的安装和使用了。\n1 2 3 pip install setuptools python setup.py pip install . 如果当前使用的Python环境是用virtualenv安装的，这时候就会在 venv/bin/下生成相应的可执行文件，对应上面的例子，就是 example-cli。执行 venv/bin/example-cli就能运行上面的代码了。\n到这里终于把前置的问题都说清楚了，现在可以讨论shiv的问题了。\n其他方式 setup.cfg 后面又发展除了 setup.cfg，用ini的语法来写一些静态的内容，没有解决什么核心的问题。\n1 2 3 4 5 6 7 8 9 10 11 # setup.cfg [metadata] name = example version = 0.1 [options] packages = find: [options.entry_points] console_scripts = example-cli = example.main:main pyproject.toml pyproject.toml是一个新的标准，用于定义构建系统的要求。它使用TOML格式，并且旨在取代setup.py和setup.cfg。pyproject.toml文件可以包含构建工具的配置，如setuptools、wheel、flit等。\n1 2 3 4 5 6 7 8 9 10 # pyproject.toml [build-system] requires = [\u0026#34;setuptools\u0026#34;, \u0026#34;wheel\u0026#34;] [project] name = \u0026#34;example\u0026#34; version = \u0026#34;0.1\u0026#34; [project.scripts] example-cli = \u0026#34;example:main\u0026#34; 这个看起来就有点Cargo.toml的味道了。\nshiv解决了什么问题？ 简单来讲，就是把当前程序和它的所有依赖打成一个包，让它成为一个相对独立的发行版本。有点类似Java的fatjar。\n注意这里说的是相对独立，因为Python的很多包其实是一个Wrapper，也就是对其他可执行文件/库的Python封装，而不是实现，所以如果你引用的包是一个Wrapper，那么用shiv也不能解决这个问题。\n简单的例子 新建一个目录叫 shiv_demo，写一个脚本叫 hello.py，内容如下\n1 2 3 4 5 6 7 8 import simplejson as json def main(): r = json.dumps([\u0026#39;foo\u0026#39;, {\u0026#39;bar\u0026#39;: (\u0026#39;baz\u0026#39;, None, 1.0, 2)}]) print(r) if __name__ == \u0026#39;__main__\u0026#39;: main() 可以看到依赖 simplejson，安装它\n1 pip install simplejson shiv 配置一个 setup.py\n1 2 3 4 5 6 7 8 9 10 11 from setuptools import setup setup( name=\u0026#34;hello-world\u0026#34;, version=\u0026#34;0.0.1\u0026#34;, description=\u0026#34;Greet the world.\u0026#34;, py_modules=[\u0026#34;hello\u0026#34;], entry_points={ \u0026#34;console_scripts\u0026#34;: [\u0026#34;hello=hello:main\u0026#34;], }, ) 这时候就可以给它打包了\n1 shiv -c hello -o hello.pyz . 这时候打包生成了一个 hello.pyz文件，就可以把这个文件拿到别处去执行了，然后你就会发现报错。。。\n到这里验证的时候我才发现，其实shiv解决的问题非常有限，参考这里shiv解决了什么问题，但起码它可以让你的程序在安装了相同的Python执行器的机器之间移植。下面测试一下，在另外一个目录里再执行\n1 2 virtualenv venv --python=3.12 source venv/bin/activate 然后执行刚刚打包生成的 hello.pyz，结果还是报错\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ❯ ./hello Traceback (most recent call last): File \u0026#34;/private/tmp/shiv3/./hello/_bootstrap/__init__.py\u0026#34;, line 76, in import_string File \u0026#34;/Users/qingchun3/.shiv/hello_e90513a0cebd72cf33c783fbb6faeb3783e9e41fd501901ce8b27f69a16aef65/site-packages/hello.py\u0026#34;, line 1, in \u0026lt;module\u0026gt; import simplejson as json ModuleNotFoundError: No module named \u0026#39;simplejson\u0026#39; During handling of the above exception, another exception occurred: Traceback (most recent call last): File \u0026#34;\u0026lt;frozen runpy\u0026gt;\u0026#34;, line 198, in _run_module_as_main File \u0026#34;\u0026lt;frozen runpy\u0026gt;\u0026#34;, line 88, in _run_code File \u0026#34;/private/tmp/shiv3/./hello/__main__.py\u0026#34;, line 3, in \u0026lt;module\u0026gt; File \u0026#34;/private/tmp/shiv3/./hello/_bootstrap/__init__.py\u0026#34;, line 253, in bootstrap File \u0026#34;/private/tmp/shiv3/./hello/_bootstrap/__init__.py\u0026#34;, line 81, in import_string File \u0026#34;/private/tmp/shiv3/./hello/_bootstrap/__init__.py\u0026#34;, line 59, in import_string File \u0026#34;/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py\u0026#34;, line 90, in import_module return _bootstrap._gcd_import(name[level:], package, level) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1387, in _gcd_import File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1360, in _find_and_load File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1331, in _find_and_load_unlocked File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 935, in _load_unlocked File \u0026#34;\u0026lt;frozen importlib._bootstrap_external\u0026gt;\u0026#34;, line 995, in exec_module File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 488, in _call_with_frames_removed File \u0026#34;/Users/qingchun3/.shiv/hello_e90513a0cebd72cf33c783fbb6faeb3783e9e41fd501901ce8b27f69a16aef65/site-packages/hello.py\u0026#34;, line 1, in \u0026lt;module\u0026gt; import simplejson as json ModuleNotFoundError: No module named \u0026#39;simplejson\u0026#39; 唉？不是说好的会把依赖也打包进来吗？为什么没有？原来是因为没有添加\n1 -r requirements.txt 选项，那么这个文件从哪里来呢？如果你已经通过pip安装了所有依赖，通过执行 pip freeze \u0026gt; requirements.txt就可以了。\n完整命令就是\n1 shiv -c hello -o hello.pyz -r requirements.txt . 再测试就通过了。\n总结 其实到最后我是很失望的，因为一开始并没有完全搞清楚shiv到底解决了什么问题，我想做的是完全的可移植性，在一台机器上将环境配置好之后拿到同平台的其他机器上直接可以执行的那种，但明显shiv并不符合我的要求，这时候我突然就发现了Java和Golang的厉害之处了，Java可以一个jar包走天下，只要有JRE就能运行，而Golang更厉害了，甚至什么都不需要，直接就是一个包含所有依赖的二进制文件。\n但是，问题还是要解决，再问问GPT，继续踏上征程吧。\n","date":"2024-07-08T08:49:34+08:00","image":"https://techinsight.pages.dev/images/covers/python-shiv.png","permalink":"https://techinsight.pages.dev/p/python-shiv%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/","title":"Python shiv简明教程"},{"content":"关于同步和异步，我理解是同步是串行执行，可以用多线程的方式来利用多核加快处理速度，而异步则是在遇到耗时操作时直接yield，待耗时操作完成时再提醒主线程执行回调的方式，所以异步能提高的是“吞吐量”而不是并发数。\n本文就通过OkHttp的两种执行模式来详细说明。\n环境准备 JDK 17 JMH 最著名的benchmark框架是JMH（Java Microbenchmark Harness）。JMH是专门用于代码微性能基准测试的工具，由Oracle的性能团队开发，它是专门为测试Java应用程序中的方法性能而设计的，适合做细粒度的性能测试。 一个肯定不是性能瓶颈的HTTP服务（Rust编写） K6 压力测试工具 实验过程 实现一个性能超级强的HTTP服务 创建项目 1 2 cargo new fast-server cd fast-server 编辑Cargo.toml 1 2 3 4 5 6 7 8 9 [package] name = \u0026#34;fast-server\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html [dependencies] axum = \u0026#34;0.7.5\u0026#34; 编写Rust代码 使用最少的依赖，返回一个Hello, World!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 use axum::{routing::get, Router}; #[tokio::main] async fn main() { // build our application with a route let app = Router::new() // `GET /` goes to `root` .route(\u0026#34;/\u0026#34;, get(index)); let listener = tokio::net::TcpListener::bind(\u0026#34;0.0.0.0:8888\u0026#34;).await.unwrap(); axum::serve(listener, app).await.unwrap(); } // basic handler that responds with a static string async fn index() -\u0026gt; \u0026amp;\u0026#39;static str { \u0026#34;Hello, World!\u0026#34; } 启动服务 1 cargo run --release 测试服务 1 2 3 4 5 6 7 $ http 127.0.0.1:8888 HTTP/1.1 200 OK content-length: 13 content-type: text/plain; charset=utf-8 date: Mon, 22 Apr 2024 14:46:47 GMT Hello, World! 至此，一个高性能服务就完成了。\n什么，这就高性能了？那么我们来测试一下。\n压力测试 安装k6 1 brew install k6 配置测试脚本 1 k6 new fast-server.js 这个命令会生成一份脚手架代码，具体内容如下（删除了大部分注释）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import http from \u0026#39;k6/http\u0026#39;; import { sleep } from \u0026#39;k6\u0026#39;; export const options = { // A number specifying the number of VUs to run concurrently. vus: 10, // A string specifying the total duration of the test run. duration: \u0026#39;30s\u0026#39;, }; export default function() { http.get(\u0026#39;https://test.k6.io\u0026#39;); sleep(1); } 根据我们前面配置的端口，稍微修改一下这个脚本\n1 2 3 4 5 6 7 8 9 10 11 12 13 import http from \u0026#39;k6/http\u0026#39;; import { sleep } from \u0026#39;k6\u0026#39;; export const options = { // A number specifying the number of VUs to run concurrently. vus: 10, // A string specifying the total duration of the test run. duration: \u0026#39;30s\u0026#39;, }; export default function() { http.get(\u0026#39;http://127.0.0.1:8888\u0026#39;); } 执行压力测试 1 k6 run fast-server.js 可以看到，在AMD R5 3600（6C12T）的macOS系统上，跑出了5万多的QPS，而且最大响应时间是14ms，可能用C可以写出更快的服务，但Rust的方式更加简单、直接。\n上面只是为了证明后端服务不是我们整个实验的瓶颈，接下来开始编写OkHttp相关的代码。\nOkHttp测试代码 相关代码见lovelock/okhttp-performance-comparison\n启动服务，并使用VisualVM观察资源占用情况。\n对比——后端服务快 可以看到，多线程模式比异步模式使用的占用的内存多了3倍，但多线程模式的QPS要高于异步模式80%。\n但是，这其实并没有模拟真实的线上场景，因为线上的场景有以下几个特点\n后端服务没有那么快 要放在更长的时间跨度来整体看 但这也可以反过来说明如果后端服务非常快，而且对内存没那么敏感，就可以使用更简单的多线程模式。\n接下来我们让后端服务变慢再来看看效果。\n对比——后端服务慢 把上面Rust服务的index方法改造一个这样（注意use std::time::Duration;）\n1 2 3 4 5 // basic handler that responds with a static string async fn index() -\u0026gt; \u0026amp;\u0026#39;static str { std::thread::sleep(Duration::from_millis(50)); \u0026#34;Hello, World!\u0026#34; } 重新运行服务并压测\n","date":"2024-04-22T22:33:43+08:00","permalink":"https://techinsight.pages.dev/p/okhttp%E5%90%8C%E6%AD%A5%E5%92%8C%E5%BC%82%E6%AD%A5%E6%96%B9%E5%BC%8F%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83/","title":"OkHttp同步和异步方式性能比较"},{"content":"仅半年在B站也发了不少视频，想着在博客上也能引用一下，但国内的视频网站嘛，海外的产品支持可能没有那么好，加上也想看看Hugo的shortcode到底是怎么工作的，所以就有了这篇文章。\n找到要引用的视频地址 这并不是直接点开视频的播放地址，而是要使用嵌入地址，俗称落地页。在B站是这样的\n具体的内容是这样\n1 \u0026lt;iframe src=\u0026#34;//player.bilibili.com/player.html?aid=1302993488\u0026amp;bvid=BV1DM4m1Q71a\u0026amp;cid=1499653065\u0026amp;p=1\u0026#34; scrolling=\u0026#34;no\u0026#34; border=\u0026#34;0\u0026#34; frameborder=\u0026#34;no\u0026#34; framespacing=\u0026#34;0\u0026#34; allowfullscreen=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;/iframe\u0026gt; 就是一段HTML片段。\n优化引用地址 无意间搜到这篇文章Embed Bilibili Video To HTML，博主指出B站的这个片段不是响应式的，对移动端的支持不好，提供了一种新的方式\n1 2 3 \u0026lt;div style=\u0026#34;position: relative; padding: 30% 45%;\u0026#34;\u0026gt; \u0026lt;iframe style=\u0026#34;position: absolute; width: 100%; height: 100%; left: 0; top: 0;\u0026#34; src=\u0026#34;//player.bilibili.com/player.html?aid=1302993488\u0026amp;bvid=BV1DM4m1Q71a\u0026amp;cid=1499653065\u0026amp;p=1\u0026#34; frameborder=\u0026#34;no\u0026#34; scrolling=\u0026#34;no\u0026#34; allowfullscreen=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; 简单对比一下就能发现其实是在外层嵌套了一些支持Responsive的样式，妙！\nHugo的shortcode 没有仔细研读，看了一个例子基本就理解了，几个基本的规则\n基于新版本的目录结构，应该放在layouts/shortcodes/name-of-short-code.html中，其中name-of-short-code就是在文章中插入的名字，比如这里新建的文件名叫bilibili，那么就应该放在layouts/shortcodes/bilibili.html，而且具体的shortcode应该这么写{{\u0026lt; bilibili \u0026gt;}} shortcode中除了写名字之外还有一些其他东西，这里就涉及到模板引擎了，写在shortcode名字后面的东西，可以通过{{ .Get 0 }}这种语法来获取，比如这个例子就是获取第一个参数 根据上面的例子，具体的shortcode内容是这样的\n1 2 3 \u0026lt;div style=\u0026#34;position: relative; padding: 30% 45%;\u0026#34;\u0026gt; \u0026lt;iframe style=\u0026#34;position: absolute; width: 100%; height: 100%; left: 0; top: 0;\u0026#34; src=\u0026#34;{{ .Get 0 }}\u0026#34; frameborder=\u0026#34;no\u0026#34; scrolling=\u0026#34;no\u0026#34; allowfullscreen=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; 在需要引用它的地方这么写\n1 {{\u0026lt; bilibili \u0026#34;//player.bilibili.com/player.html?aid=1302993488\u0026amp;bvid=BV1DM4m1Q71a\u0026amp;cid=1499653065\u0026amp;p=1\u0026#34; \u0026gt;}} 搞定收工！如果要引用别的视频网站，可以根据需求写相应的shortcode。\n","date":"2024-04-16T21:50:49+08:00","image":"https://techinsight.pages.dev/images/covers/embed-video-in-hugo.png","permalink":"https://techinsight.pages.dev/p/%E5%9C%A8hugo%E4%B8%AD%E5%B5%8C%E5%85%A5%E8%A7%86%E9%A2%91/","title":"在Hugo中嵌入视频"},{"content":"背景 有一个很优秀的C语言实现的AC自动机代码，但它不是线程安全的，为什么这么说呢，我们来看下代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 typedef struct ac_trie { struct act_node *root; /**\u0026lt; The root node of the trie */ size_t patterns_count; /**\u0026lt; Total patterns in the trie */ short trie_open; /**\u0026lt; This flag indicates that if trie is finalized * or not. After finalizing the trie you can not * add pattern to trie anymore. */ struct mpool *mp; /**\u0026lt; Memory pool */ /* ******************* Thread specific part ******************** */ /* It is possible to search a long input chunk by chunk. In order to * connect these chunks and make a continuous view of the input, we need * the following variables. */ struct act_node *last_node; /**\u0026lt; Last node we stopped at */ size_t base_position; /**\u0026lt; Represents the position of the current chunk, * related to whole input text */ AC_TEXT_t *text; /**\u0026lt; A helper variable to hold the input chunk */ size_t position; /**\u0026lt; A helper variable to hold the relative current * position in the given text */ MF_REPLACEMENT_DATA_t repdata; /**\u0026lt; Replacement data structure */ ACT_WORKING_MODE_t wm; /**\u0026lt; Working mode */ } AC_TRIE_t; 这是整个自动机的定义，其实作者也意识到了那几个变量是线程相关的，但他并没有选择实现线程安全，因为这个项目是为一个命令行程序服务的，而这个命令行程序显然是没有机会处理多线程场景的。\n那为什么有这几个线程相关的变量就无法实现线程安全呢？再看下面的代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 /** * @brief Search in the input text using the given trie. * * @param thiz pointer to the trie * @param text input text to be searched * @param keep indicated that if the input text the successive chunk of the * previous given text or not * @param callback when a match occurs this function will be called. The * call-back function in turn after doing its job, will return an integer * value, 0 means continue search, and non-0 value means stop search and return * to the caller. * @param user this parameter will be send to the call-back function * * @return * -1: failed; trie is not finalized * 0: success; input text was searched to the end * 1: success; input text was searched partially. (callback broke the loop) *****************************************************************************/ int ac_trie_search (AC_TRIE_t *thiz, AC_TEXT_t *text, int keep, AC_MATCH_CALBACK_f callback, void *user) { size_t position; ACT_NODE_t *current; ACT_NODE_t *next; AC_MATCH_t match; if (thiz-\u0026gt;trie_open) return -1; /* Trie must be finalized first. */ if (thiz-\u0026gt;wm == AC_WORKING_MODE_FINDNEXT) position = thiz-\u0026gt;position; else position = 0; current = thiz-\u0026gt;last_node; if (!keep) ac_trie_reset (thiz); /* This is the main search loop. * It must be kept as lightweight as possible. */ while (position \u0026lt; text-\u0026gt;length) { if (!(next = node_find_next_bs (current, text-\u0026gt;astring[position]))) { if(current-\u0026gt;failure_node /* We are not in the root node */) current = current-\u0026gt;failure_node; else position++; } else { current = next; position++; } if (current-\u0026gt;final \u0026amp;\u0026amp; next) /* We check \u0026#39;next\u0026#39; to find out if we have come here after a alphabet * transition or due to a fail transition. in second case we should not * report match, because it has already been reported */ { /* Found a match! */ match.position = position + thiz-\u0026gt;base_position; match.size = current-\u0026gt;matched_size; match.patterns = current-\u0026gt;matched; /* Do call-back */ if (callback(\u0026amp;match, user)) { if (thiz-\u0026gt;wm == AC_WORKING_MODE_FINDNEXT) { thiz-\u0026gt;position = position; thiz-\u0026gt;last_node = current; } return 1; } } } /* Save status variables */ thiz-\u0026gt;last_node = current; thiz-\u0026gt;base_position += position; return 0; } 可以看到thiz就是正在使用的AC自动机的实例，但在查找过程中它改变了它的相关属性，这在多线程环境中肯定是会有冲突的，那么怎么解决呢？\n方案 设想这样一个简单的场景，我们在处理链表相关的算法题时，最常做的事情是什么呢？对，是建立一个dummyHead，仔细想想为什么要这么做。\n是的，根本目的是不希望我们遍历完成之后改变原来的链表，其实就是保持链表的不变性。\n这里也是一样的道理，既然不能改变Trie里的这些属性，那么就把它提出来，最终让它们成为线程内部的变量，这样就做到了线程之间的隔离，每个线程只处理自己的查找，把Trie变成不可变的。\n实现一个不可变的Trie 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /* * The A.C. Trie data structure */ typedef struct ac_trie { struct act_node *root; /**\u0026lt; The root node of the trie */ size_t patterns_count; /**\u0026lt; Total patterns in the trie */ short trie_open; /**\u0026lt; This flag indicates that if trie is finalized * or not. After finalizing the trie you can not * add pattern to trie anymore. */ struct mpool *mp; /**\u0026lt; Memory pool */ MF_REPLACEMENT_DATA_t repdata; /**\u0026lt; Replacement data structure */ ACT_WORKING_MODE_t wm; /**\u0026lt; Working mode */ } AC_TRIE_t; MF_REPLACEMENT_DATA_t repdata; 这个很可疑，但它是用来实现替换功能的，我暂时没有这个需求，所以先不动它。\n但整个搜索过程还是需要那些变量的，我们把它放在一个新定义的结构体中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 typedef struct ac_search { /* ******************* Thread specific part ******************** */ /* It is possible to search a long input chunk by chunk. In order to * connect these chunks and make a continuous view of the input, we need * the following variables. */ struct act_node *last_node; /**\u0026lt; Last node we stopped at */ size_t base_position; /**\u0026lt; Represents the position of the current chunk, * related to whole input text */ AC_TEXT_t *text; /**\u0026lt; A helper variable to hold the input chunk */ size_t position; /**\u0026lt; A helper variable to hold the relative current * position in the given text */ } AC_SEARCH_t; 相应地，也需要改查找方法的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 int ac_trie_search(AC_TRIE_t *thiz, AC_SEARCH_t *search, int keep, AC_MATCH_CALBACK_f callback, void *user) { size_t position; ACT_NODE_t *current; ACT_NODE_t *next; AC_MATCH_t match; if (thiz-\u0026gt;trie_open) return -1; /* Trie must be finalized first. */ if (thiz-\u0026gt;wm == AC_WORKING_MODE_FINDNEXT) position = search-\u0026gt;position; else position = 0; current = search-\u0026gt;last_node; if (!keep) ac_trie_reset(thiz); /* This is the main search loop. * It must be kept as lightweight as possible. */ while (position \u0026lt; search-\u0026gt;text-\u0026gt;length) { if (!(next = node_find_next_bs(current, search-\u0026gt;text-\u0026gt;astring[position]))) { if (current-\u0026gt;failure_node /* We are not in the root node */) current = current-\u0026gt;failure_node; else position++; } else { current = next; position++; } if (current-\u0026gt;final \u0026amp;\u0026amp; next) /* We check \u0026#39;next\u0026#39; to find out if we have come here after a alphabet * transition or due to a fail transition. in second case we should not * report match, because it has already been reported */ { /* Found a match! */ match.position = position + search-\u0026gt;base_position; match.size = current-\u0026gt;matched_size; match.patterns = current-\u0026gt;matched; /* Do call-back */ if (callback(\u0026amp;match, user)) { if (thiz-\u0026gt;wm == AC_WORKING_MODE_FINDNEXT) { search-\u0026gt;position = position; search-\u0026gt;last_node = current; } return 1; } } } /* Save status variables */ search-\u0026gt;last_node = current; search-\u0026gt;base_position += position; return 0; } 把其中原先是thiz的地方改成search。\n接下来就需要考虑怎么初始化AC_SEARCH_t。\n1 2 3 4 5 6 7 8 9 10 AC_SEARCH_t *ac_search_create(void) { AC_SEARCH_t *search = (AC_SEARCH_t *)malloc(sizeof(AC_SEARCH_t)); search-\u0026gt;text = NULL; search-\u0026gt;position = 0; search-\u0026gt;last_node = NULL; search-\u0026gt;base_position = 0; return search; } 这是一个完全0值的初始化过程，事实上我们得先考虑把text初始化，因为要知道查的文本是什么。\n1 2 3 4 5 AC_SEARCH_t *search = ac_search_create(); AC_TEXT_t chunk; chunk.astring = \u0026#34;experience the ease and simplicity of multifast\u0026#34;; chunk.length = strlen(chunk.astring); search-\u0026gt;text = \u0026amp;chunk; 这时执行会发现报段错误，具体错误就不展示了，原因是在node_find_next_bs方法中，有这样一段代码\n1 max = nod-\u0026gt;outgoing_size - 1; 而nod是在ac_trie_search方法中的current，如果传入的search-\u0026gt;last_node是空值，这里就直接一个空指针异常了。\n检查Trie的初始化代码发现，每次开始查询之前都需要重置一下状态（其实就是我们这里处理的这些变量）\n1 2 3 4 5 6 7 8 9 10 11 /** * @brief reset the trie and make it ready for doing new search * * @param thiz pointer to the trie *****************************************************************************/ static void ac_trie_reset(AC_TRIE_t *thiz) { thiz-\u0026gt;last_node = thiz-\u0026gt;root; thiz-\u0026gt;base_position = 0; mf_repdata_reset(\u0026amp;thiz-\u0026gt;repdata); } 重点就在这里了，需要先把search-\u0026gt;last_node = trie-\u0026gt;root，这样才能开始。\n最终效果 1 2 3 4 5 6 AC_SEARCH_t *search = ac_search_create(); AC_TEXT_t chunk; chunk.astring = chunks[i]; chunk.length = strlen(chunk.astring); search-\u0026gt;text = \u0026amp;chunk; search-\u0026gt;last_node = trie-\u0026gt;root; 这就是一个完整的AC_SEARCH_t的初始化过程了。\n多线程测试 这里只展示关键部分代码\n首先定义一个结构体来向线程传递参数。\n1 2 3 4 5 typedef struct { AC_TRIE_t *automata; AC_SEARCH_t *search; } ThreadParams; 创建多个线程并等待执行结束\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 pthread_t threads[3]; for (i = 0; i \u0026lt; 3; i++) { ThreadParams *threadParams = (ThreadParams *)malloc(sizeof(ThreadParams)); threadParams-\u0026gt;automata = trie; AC_SEARCH_t *search = ac_search_create(); AC_TEXT_t chunk; chunk.astring = chunks[i]; chunk.length = strlen(chunk.astring); search-\u0026gt;text = \u0026amp;chunk; search-\u0026gt;last_node = trie-\u0026gt;root; threadParams-\u0026gt;search = search; pthread_create(\u0026amp;threads[i], NULL, chlid_handler, threadParams); } for (i = 0; i \u0026lt; 3; i++) { pthread_join(threads[i], NULL); } 定义线程内的回调方法，把传进来的参数解出来，调用查找方法\n1 2 3 4 5 6 7 8 9 void *chlid_handler(void *arg) { ThreadParams *params = (ThreadParams *)arg; AC_MATCH_t match; printf(\u0026#34;Searching: \\\u0026#34;%s\\\u0026#34; in thread: %ld\\n\u0026#34;, params-\u0026gt;search-\u0026gt;text-\u0026gt;astring, (unsigned long int)pthread_self()); ac_trie_search(params-\u0026gt;automata, params-\u0026gt;search, 0, match_handler, 0); return NULL; } 大功告成。\n","date":"2024-04-10T23:47:17+08:00","image":"https://techinsight.pages.dev/images/covers/a-threadsafe-ac-automaton-with-c.png","permalink":"https://techinsight.pages.dev/p/%E4%BD%BF%E7%94%A8c%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84ac%E8%87%AA%E5%8A%A8%E6%9C%BA/","title":"使用C语言实现一个线程安全的AC自动机"},{"content":"记录一下快速配置virtualenv。\n1. 安装virtualenv macOS 1 brew install virtualenv Linux(以Debian为例) 1 sudo apt install virtualenv 2. 指定版本 1 virtualenv venv --python=3.11 这时会自动在 venv下复制一份全新的3.11的python环境\n注意：这种写法只能使用系统已经安装的Python版本，否则需要指定Python的安装路径\n3. 让指定的版本生效 1 source venv/bin/activate 这时命令行提示符前会有一个 (venv)的前缀，如下，这时在这里执行 pip install就不会影响到别的环境。\n1 (venv) frost@debian:~/workspace/ 4. 不再使用这个Python环境 1 deactivate ","date":"2024-04-10T00:28:44+08:00","image":"https://techinsight.pages.dev/images/covers/python-virtualenv.png","permalink":"https://techinsight.pages.dev/p/virtualenv%E5%BF%AB%E9%80%9F%E8%AE%BE%E7%BD%AE/","title":"Virtualenv快速设置"},{"content":"PHP 调用 http 服务此前一直都是封装的 curl，这事儿我也干过不少次了，不过今天碰到了一个新问题。\n接口是 clickhouse 服务，查询的响应体比较大，放在内存里要超过 1 G，再继续修改 max_memory_limit 意义不大，所以就希望把结果直接放入文件，然后逐行读取处理，以减少整个过程的内存消耗，那么怎么把结果写入文件呢？\n相关的选项是这个\n1 curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1); 这个选项设置为 1/true 时，就会把结果放在 $response = curl_exec($ch) 里的这个 $response 里，这也是现在这个问题的来源，$response 太大了。但如果把它设置为 0/false，它会写到标准输出，又没办法用代码处理。\n参考了这个问题，给出了两个思路。\nBilibili 视频av号或BV号错误！请检查视频av号或BV号是否正确\n当前视频av或BV号：//player.bilibili.com/player.html?aid=1302993488\u0026amp;bvid=BV1DM4m1Q71a\u0026amp;cid=1499653065\u0026amp;p=1，视频分P：1\n直接写入文件 1 2 3 $fh = fopen($file, \u0026#39;w\u0026#39;); curl_setopt($ch, CURLOPT_RETURNTRANSFER, 0); curl_setopt($ch, CURLOPT_FILE, $fh); 一定要注意的是，如果 $file 对应的目录不存在，则无法自动创建目录，整个过程会失败。\n写回调方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # setup a global file pointer $GlobalFileHandle = null; function saveRemoteFile($url, $filename) { global $GlobalFileHandle; set_time_limit(0); # Open the file for writing... $GlobalFileHandle = fopen($filename, \u0026#39;w+\u0026#39;); $ch = curl_init(); curl_setopt($ch, CURLOPT_URL, $url); curl_setopt($ch, CURLOPT_FILE, $GlobalFileHandle); curl_setopt($ch, CURLOPT_RETURNTRANSFER, false); # Assign a callback function to the CURL Write-Function curl_setopt($ch, CURLOPT_WRITEFUNCTION, \u0026#39;curlWriteFile\u0026#39;); # Exceute the download - note we DO NOT put the result into a variable! curl_exec($ch); # Close CURL curl_close($ch); # Close the file pointer fclose($GlobalFileHandle); } function curlWriteFile($cp, $data) { global $GlobalFileHandle; $len = fwrite($GlobalFileHandle, $data); return $len; } 这个看起来有点多此一举了，因为不指定这个回调函数也会写入这个方法，只是无法对写入的方式做一些精细控制，这个回调函数多数时候是不需要的，因为我更倾向于先整体写入文件，上层的方法再根据业务需求去处理，而不是在这里决定。\n","date":"2024-04-09T17:46:03+08:00","image":"https://techinsight.pages.dev/images/covers/curl-with-large-response-oom-fix.png","permalink":"https://techinsight.pages.dev/p/php%E8%A7%A3%E5%86%B3curl%E5%93%8D%E5%BA%94%E8%BF%87%E5%A4%A7%E6%97%B6%E7%88%86%E5%86%85%E5%AD%98%E7%9A%84%E9%97%AE%E9%A2%98/","title":"PHP解决curl响应过大时爆内存的问题"},{"content":"背景 一直以来，尤其是自从使用Apple Silicon的芯片的MacBook Pro以来，在macOS上使用Linux虚拟机就变得很困难了，加上我家里用的是Ryzentosh，两台电脑都很难使用虚拟机，就搞得很头疼。尤其最近需要调试那个C语言的程序，依赖一些libevent、inotify这类东西，不想把宿主机搞得太乱，所以选择一个虚拟机管理器的事情又得重新考虑了。\nIntel芯片的macOS还是有很多选择的，Parallels Desktop就不说了，唯一的缺点就是贵，只说免费的方案。\nVirtualBox肯定是首选，但在Apple Silicon芯片发布后5年后的今天（2024年4月），支持苹果M系列芯片的版本仍然没有发布，甚至早前公布的虽然不可用的预览版也早已下架了，搞不好Oracle内部早已经放弃了。\n废话不多说，直接介绍今天的主角吧，Lima——Linux Machine，名字简单直接，其实我第一次看到类似的名字是Colima，是在apppleboy64大佬的博客上看到的，是一个本地管理K3s的工具，后来顺着了解到了Lima，可以理解为为了在macOS上运行Containerd而需要先启动一个Linux虚拟机，那么这个Lima就是为了运行这个虚拟机的。但我们现在要的就是这个虚拟机，至于后面是否要用K3s/K8s，不是现在考虑的事情。\n其实所有这些东西背后都是QEMU，无非是使用哪个前端了。我知道的还有一个libvirt，但配置相对很复杂，而我只需要一个能简单run起来的虚拟机而已，所以使用如此简单的Lima几乎是一个完美的选择。\n快速入门 安装 前面说了，我们是要在macOS平台上使用，那么肯定优先选择用homebrew安装了。\n1 brew install lima 可以看到，会自动安装QEMU依赖。\n配置虚拟机 1 limactl start 这时会弹出几个选项，不要着急选，先看下自己的需求\n1 2 3 4 5 6 7 8 $ limactl start ? Creating an instance \u0026#34;default\u0026#34; [Use arrows to move, type to filter] \u0026gt; Proceed with the current configuration Open an editor to review or modify the current configuration Choose another template (docker, podman, archlinux, fedora, ...) Exit ... INFO[0029] READY. Run `lima` to open the shell. 默认用的是Ubuntu 23.10，我不喜欢用Ubuntu，所以会先选择 Choose another template (docker, podman, archlinux, fedora, ...)来重新配置，比如我选择了Debian 12，配置如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # This template requires Lima v0.7.0 or later images: # Try to use release-yyyyMMdd image if available. Note that release-yyyyMMdd will be removed after several months. - location: \u0026#34;https://cloud.debian.org/images/cloud/bookworm/20240211-1654/debian-12-genericcloud-amd64-20240211-1654.qcow2\u0026#34; arch: \u0026#34;x86_64\u0026#34; digest: \u0026#34;sha512:6856277491c234fa1bc6f250cbd9f0d44f77524479536ecbc0ac536bc07e76322ebb4d42e09605056d6d3879c8eb87db40690a2b5dfe57cb19b0c673fc4c58ca\u0026#34; - location: \u0026#34;https://cloud.debian.org/images/cloud/bookworm/20240211-1654/debian-12-genericcloud-arm64-20240211-1654.qcow2\u0026#34; arch: \u0026#34;aarch64\u0026#34; digest: \u0026#34;sha512:c8f3746aa979cdc95c13cd4b8cc032151f1e5685525a85c2b3b2e30defa02dacb1058b68f955ac16f3f2dbd473d13dfef15d2a22f348bcc4abb427e0713fa9a4\u0026#34; # Fallback to the latest release image. # Hint: run `limactl prune` to invalidate the cache - location: \u0026#34;https://cloud.debian.org/images/cloud/bookworm/latest/debian-12-genericcloud-amd64.qcow2\u0026#34; arch: \u0026#34;x86_64\u0026#34; - location: \u0026#34;https://cloud.debian.org/images/cloud/bookworm/latest/debian-12-genericcloud-arm64.qcow2\u0026#34; arch: \u0026#34;aarch64\u0026#34; mounts: - location: \u0026#34;~\u0026#34; - location: \u0026#34;/tmp/lima\u0026#34; writable: true 绝大多数情况下，前面images相关的配置我们完全不用关心，只需要知道它能虚拟出来一个完整的虚拟机就行了，最重要的是mounts选项，不得不说，yaml配置真是很难读\n1 2 3 4 mounts: - location: \u0026#34;~\u0026#34; - location: \u0026#34;/tmp/lima\u0026#34; writable: true 这个配置的意思是它会把~也就是宿主机的家目录挂载到虚拟机，让我们可以在虚拟机内访问到宿主机家目录中的问题，但默认是只读的，不要以为这里有writable: true就不用管了，其实这个writable是给/tmp/lima用的，所以需要简单修改一下，改成\n1 2 3 4 5 mounts: - location: \u0026#34;~\u0026#34; writable: true - location: \u0026#34;/tmp/lima\u0026#34; writable: true 就可以保存了，一直下一步就可以了。这里如果不用魔法可能会非常慢，具体这里就不说了，自己想办法解决。\n访问虚拟机 这就是为什么我说Lima几乎完美的原因了，它是真的知道开发人员需要什么，跟我大声说——SSH！所以默认启动之后就配置好了ssh访问，可以执行lima命令进入虚拟机。\n注意：这是因为前面我们没有指定虚拟机的名字，所以默认是default，所以执行lima也就不需要指定名字了。如果你想启动多个虚拟机，相应的命令可以替换成limactl start debian、lima debian这样。\n这时候更有意思的来了，进虚拟机之后它会把你当前在宿主机的位置带进虚拟机，如果你不小心去了别的目录，还想回到宿主机的家目录，只需要cd /Users/your_name/path/to/your/location/就行了，别提多贴心了。\n现在你可以开心地使用虚拟机了，是不是很有WSL的味道？\nOne More Thing 考虑以下两种情况\n在Apple Silicon的芯片上运行X86_64的Linux虚拟机 在Ryzontosh上运行虚拟机 QEMU表示：我太难了！！！\n但还是可以的，第一种情况你在前面编辑配置的时候可能已经看到了，配置文件中有几项带默认值的，这里简单列一下\nkey value vmType 默认QEMU就行了，vz性能更差 | os | 也不用动，你既然找它肯定是要用Linux |\n| arch |关键就是这里了，Apple Silicon的机器默认肯定都选的是aarch64，但如果确实需要X86的虚拟机，可以改成\nx86_64\n第二种情况，需要手动给QEMU指定运行参数，把加上 QEMU_SYSTEM_X86_64=\u0026quot;QEMU-system-x86_64 -cpu max -machine q35\u0026quot;环境变量再执行limactl start就可以了，但运行速度会比较慢就是了。\n那么好奇的小朋友就会问了，这是为什么呢？\n先看下GPT对于QEMU和KVM的区别的说明\nQEMU（Quick Emulator）和 KVM（Kernel-based Virtual Machine）之间的关系是相互补充的，它们一起在Linux上提供了一种高效的虚拟化解决方案。\nQEMU是一个通用的开源机器模拟器和虚拟器。它可以执行硬件虚拟化，使您能够运行一个操作系统的完整副本（称为客户机或虚拟机）在另一个操作系统上。QEMU可以在用户模式下运行，提供软件仿真虚拟化，但这通常会导致较慢的性能。\nKVM是Linux内核的一部分，它允许Linux将自身转换为一个类型1（裸金属）的虚拟机监控器。KVM需要处理器支持硬件虚拟化扩展（Intel的VT-x或AMD的AMD-V）。当KVM用于QEMU时，它提供硬件辅助虚拟化，显著提高虚拟机的性能，特别是对于CPU密集型应用程序。\nKVM本身不执行任何模拟，它依赖于用户空间程序（如QEMU）来设置虚拟机的环境、虚拟硬件等。QEMU在使用KVM时，负责设备模拟，而CPU密集型任务则由KVM在硬件虚拟化扩展的帮助下运行，这样可以近乎本地速度执行虚拟机。\n简单来说，QEMU用于模拟硬件，而KVM让QEMU利用CPU扩展来提供更快的虚拟化性能。如果无法使用KVM（例如在不支持硬件虚拟化的平台上），QEMU仍然可以独立工作，只是性能会有所下降。\n可以理解为Hypervisor.Framework就是macOS平台上的KVM，而在Ryzentosh上，无法访问Hypervisor.Framework，所以就只能完全依赖QEMU自己模拟和计算了，经过一层转化自然就慢了。\n","date":"2024-04-05T23:11:59+08:00","image":"https://techinsight.pages.dev/images/covers/Lima-for-linux-virtual-machine-on-macOS.png","permalink":"https://techinsight.pages.dev/p/%E4%BD%BF%E7%94%A8lima%E7%AE%A1%E7%90%86macos%E4%B8%8A%E7%9A%84linux%E8%99%9A%E6%8B%9F%E6%9C%BA/","title":"使用Lima管理macOS上的Linux虚拟机"},{"content":"使用CMake的小伙伴可能大部分都是在（类）Unix环境下，所以一般都是用Unix Makefile作为默认后端，我这两天在修改一个多年前的C项目，把它从一坨Automake、Autoconf中拯救出来，改成了CMake的形式，清爽多了。\n为什么这么说呢？是因为原先的方式就是把项目本身的代码放在根目录，然后几个依赖放在和自己的代码同一级的文件夹里，对于这几个依赖也是动态链接的方式，所以就需要先手动按顺序编译、安装这些依赖，然后再编译自己的代码。而改成CMake之后就变成静态依赖了，编译过程简化不少。\n但问题也来了，我发现每次编译都要花挺长时间，那么有没有办法加速呢？这时候我想到了在很多开源项目都会用到的Ninja，于是了解了下，发现像Chromium、Android（部分）都是用它编译的，主打一个增量编译，说白了就是只编译修改了的部分，它能够更智能地决定哪些部分需要重新构建，从而减少不必要的编译。\n使用方法 首先当然是安装了，在Debian上的名字叫ninja-build，其他系统可以参考这个名字。\n正常使用CMake是这样的：\n1 2 3 4 mkdir build cd build cmake .. cmake --build 而对比使用Ninja作为状态后端，流程是这样的\n1 2 3 4 mkdir build cd build cmake -G \u0026#34;Ninja\u0026#34; cmake --build 区别就在于加了-G \u0026quot;Ninja\u0026quot;，一定注意这里要大写，否则你可以试试。\n由于代码量比较少，所以比较下来区别不大，但公司的那个C项目，实际使用时还是有点明显的。\n查了一下Help，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Generators The following generators are available on this platform (* marks default): Green Hills MULTI = Generates Green Hills MULTI files (experimental, work-in-progress). * Unix Makefiles = Generates standard UNIX makefiles. Ninja = Generates build.ninja files. Ninja Multi-Config = Generates build-\u0026lt;Config\u0026gt;.ninja files. Watcom WMake = Generates Watcom WMake makefiles. CodeBlocks - Ninja = Generates CodeBlocks project files. CodeBlocks - Unix Makefiles = Generates CodeBlocks project files. CodeLite - Ninja = Generates CodeLite project files. CodeLite - Unix Makefiles = Generates CodeLite project files. Eclipse CDT4 - Ninja = Generates Eclipse CDT 4.0 project files. Eclipse CDT4 - Unix Makefiles= Generates Eclipse CDT 4.0 project files. Kate - Ninja = Generates Kate project files. Kate - Unix Makefiles = Generates Kate project files. Sublime Text 2 - Ninja = Generates Sublime Text 2 project files. Sublime Text 2 - Unix Makefiles = Generates Sublime Text 2 project files. 所以-G其实就是指定generator，默认是Unix Makefiles。\n再进一步 前面mkdir build \u0026amp;\u0026amp; cd build这些步骤其实有点初级，可以改成这样的\n1 2 cmake -G \u0026#34;Ninja\u0026#34; -B build cmake --build build 第一行的意思是指定Ninja作为generator，指定build作为编译用的目录，如果build目录不存在就会自动创建。\n第二行的意思是在build目录里执行编译过程。\n这就更适合自己写一个编译脚本来执行了。\n总结 Ninja的优势主要表现在以下几个方面：\n速度：Ninja被设计为比Make更快。Ninja的设计重点是实现高性能，这意味着它能够更快地开始编译过程并更高效地执行构建。 简化的构建文件：Ninja的构建文件通常比Makefiles更加简单和易于理解。这是因为CMake负责生成这些文件，而Ninja仅仅执行它们。 并行构建：虽然Make也支持并行构建（例如通过make -j 参数），但Ninja通常在并行构建方面更加高效和智能，它会自动推断出最优的任务数来使用所有可用的处理器核心。 更好的构建进度估计：Ninja提供了更准确的构建进度信息，这对于长时间的构建过程来说非常有用。 更少的重新构建：Ninja更智能地处理构建文件的生成，从而避免了一些不必要的重新构建，这可能发生在Makefiles中。 快速的无操作构建：当没有任何东西需要构建时（即所有目标都是最新的），Ninja可以更快地确定没有工作要做，并立即完成构建过程。 总的来说，虽然Make和Ninja都支持增量构建，但Ninja在执行构建任务时通常会更快，尤其是对于大型项目。这不仅节省了开发者的时间，也提高了构建系统的整体效率。对于有大量源文件的复杂项目，采用Ninja可能会显著减少构建的时间。\n","date":"2024-04-04T22:00:30+08:00","image":"https://techinsight.pages.dev/images/covers/CMake-with-Ninja-cover.png","permalink":"https://techinsight.pages.dev/p/%E4%BD%BF%E7%94%A8ninja%E4%BD%9C%E4%B8%BAcmake%E7%9A%84%E5%90%8E%E7%AB%AF/","title":"使用Ninja作为CMake的后端"},{"content":"什么是 CMake？它能干什么？ CMake 是一个开源的跨平台自动化构建系统，它使用平台和编译器独立的配置文件来生成标准的构建文件，这使得开发者可以使用统一的方法来管理项目的构建过程。CMake的配置文件通常命名为 CMakeLists.txt，在这个文件中，开发者可以定义项目的源码文件、依赖关系、目标（如可执行文件和库）、编译选项以及其他构建参数。\nCMake的主要优势在于它的跨平台能力：它可以生成适用于多种平台和IDE的构建配置，包括Unix的Makefiles、Microsoft Visual Studio的解决方案文件、Apple的Xcode项目等。这样，使用CMake的项目可以轻松地在不同的开发环境中编译和运行，无需修改构建配置。\n简单来说，CMake让复杂的构建过程变得简单化，提供一种高效、可扩展的方式来自动化构建过程，支持大型项目和多平台编译。\n一个最简单的 CMake 项目 只包含必要元素的 CMakeLists.txt 一个最简单的CMakeLists.txt文件通常包含以下几个基本元素：\ncmake_minimum_required: 指定运行此项目所需的最低版本的CMake。 project: 定义了项目的名称和可选的版本号。 add_executable 或 add_library: 添加一个可执行文件或者库到项目中，并指定它的源文件。 下面是一个最简单的示例：\n1 2 3 4 cmake_minimum_required(VERSION 3.0) # 设置CMake的最低版本要求 project(HelloWorld) # 定义项目名称 add_executable(HelloWorld main.c) # 添加一个可执行文件 \u0026#34;HelloWorld\u0026#34;，源文件是 \u0026#34;main.c\u0026#34; 这个 CMakeLists.txt 文件定义了一个简单的项目，其中包含一个可执行文件 HelloWorld，它是由单个源文件 main.c 编译而成的。当你运行CMake时，它会根据这个配置文件生成适合你系统的构建文件（比如Unix的Makefile或者Visual Studio的项目文件）。然后你可以使用相应的构建系统来编译和链接你的程序。\n最简单的 HelloWorld 1 2 3 4 5 6 #include \u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;hello world!\\n\u0026#34;); return 0; } 构建这个项目 创建 build 目录 在 CMakeLists.txt 所在的目录下，创建一个 build 目录。\n1 mkdir build \u0026amp;\u0026amp; cd build 生成对应当前系统的构建文件 1 cmake .. 为什么要费这劲呢？ 你可能已经看到了，我们先创建了 build 目录，然后在它里面执行的是 cmake ..，也就是说 cmake 需要的文件在 build 的上层目录，那是不是直接在它的上层目录执行 cmake . 也是可以的呢？\n是的，但是这样做其实是个最佳实践，好处在于在必要时可以直接删除 build 目录，而不会对项目产生影响，否则 CMake 生成的文件散落在你的代码中，一定不是你想看到的结果。\n这个命令通常在一个空的构建目录（通常是项目根目录的子目录，如 build）中执行。这是一种被推荐的外部构建方法，可以避免在源代码目录中生成构建文件。. 表示当前目录，而 .. 表示当前目录的父目录，通常这个父目录包含顶层的 CMakeLists.txt 文件。\n执行 cmake .. 会：\n检查系统环境。 根据顶层 CMakeLists.txt 文件来确定如何编译项目的源代码。 生成对应于当前系统的构建文件（如Makefiles或者Visual Studio解决方案等）。 CMake 怎么知道要生成什么平台的文件呢？ CMake 确定使用哪种具体构建工具（如 Make 或 Visual Studio）的过程是在初次配置构建系统时发生的，也就是在你运行 cmake 命令来生成构建文件时。这个决定基于两个主要因素：\n可用的构建工具：CMake 会检测你的系统上安装了哪些构建工具。例如，如果你在 Windows 上且安装了 Visual Studio，CMake 默认会生成 Visual Studio 解决方案文件。如果你在 Linux 或 macOS 上，通常默认生成 Makefile。 用户指定的生成器：用户可以通过 -G 选项显式指定使用哪个生成器，也就是构建系统类型。例如，即使在 Windows 上，你也可以通过 -G \u0026quot;Unix Makefiles\u0026quot; 选项告诉 CMake 生成 Makefile，使用 Make 工具进行构建，而不是使用 Visual Studio。类似地，如果你想要使用其他类型的构建系统，如 Ninja，你也可以通过 -G \u0026quot;Ninja\u0026quot; 来指定。 当运行 cmake 命令没有指定 -G 选项时，CMake 会根据它检测到的环境和默认优先顺序选择一个生成器。一旦构建文件被生成，运行 cmake --build . 命令时，CMake 会使用相应的构建工具来编译项目，而不需要用户关心具体是使用 Make、Visual Studio 还是其他工具。这种抽象化的好处是，你可以用相同的命令在不同的平台和环境下构建你的项目。\n生成最终的可执行文件或库文件 1 cmake --build . 这个命令是用来实际编译和链接程序的。在你已经生成了构建系统文件后（例如，使用 cmake ..），可以使用此命令来启动构建过程。\n执行 cmake --build . 会：\n根据当前目录中的构建系统文件（Makefile、Visual Studio解决方案文件等）来编译源代码。 生成最终的可执行文件或库文件。 . 在这里也表示当前目录，它应该是包含了构建系统文件的目录。这条命令的优点是它抽象了具体的构建工具（make、ninja、msbuild等），使得构建过程与构建系统无关。\n执行完这一步，就生成了最终的可执行文件（或库文件，如果你写的是一个库的话）。在这个例子里可执行文件的文件名是 HelloWorld，因为 CMakeLists.txt 文件的第二行 project(HelloWorld) 里已经定义了。\n如果只是这么简单，就没必要使用 CMake 了，直接 gcc 都能满足了，所以下面我们来看如何用 CMake 来管理依赖。\n管理一个库项目 有时候我们需要写的项目是一个让其他项目依赖的库，就不能像上面生成一个可执行文件，而是生成一个 .a 或 .so 文件，下面是一个最简单的例子。完成这个例子之后我们会对两种情况做一个简单的对比。\n只包含必要元素的 CMakeLists.txt 1 2 3 4 5 6 7 8 cmake_minimum_required(VERSION 3.27) project(my_math) # 创建一个明为my_math的静态库 add_library(my_math include/my_math/my_math.h src/my_math.c) # 指定库的头文件搜索路径 target_include_directories(my_math PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include) [!QUESTION] 什么是静态库？是不是还有动态库？\n在CMake中，当你使用 add_library 命令创建一个库时，SHARED 和 STATIC 关键字用来指定库是动态链接的还是静态链接的：\nSTATIC：创建一个静态库。静态库通常是一个包含多个对象文件的单一归档文件（在Windows上通常是 .lib 文件，在Unix-like系统上是 .a 文件）。当静态库被链接到一个可执行文件时，库中的代码会被复制到最终的可执行文件中。静态链接发生在编译时，一旦完成，可执行文件不再需要静态库文件。静态库的优点是最终的可执行文件是自包含的，不依赖外部的动态库。缺点是如果多个程序都使用同一个静态库，它们各自都会包含一份库代码的副本，这会导致冗余和更大的程序体积。\nSHARED：创建一个动态链接库（在Windows上是 .dll 文件，在Unix-like系统上是 .so 文件）。当动态库被链接到一个可执行文件时，并不是将代码复制到可执行文件中，而是在程序运行时由操作系统动态地加载和链接。这意味着程序在运行时需要能够找到这个动态库文件。动态库的优点是多个程序可以共享同一份库代码，节省空间，并可以在不重新编译程序的情况下更新库代码（只要接口没有改变）。缺点是可能会遇到库版本不匹配或找不到库文件的运行时错误。\n在实际应用中，这两种类型的库各有用处：\n如果你希望你的程序易于部署，不想处理动态库可能带来的复杂性，你可能会选择静态链接。这样你的程序可以在没有额外依赖的情况下运行。 如果你希望你的应用程序能够共享公共代码并且轻量级，或者希望能够独立于应用程序更新你的库，那么你可能会选择动态链接。 在 CMake 中，你可以根据需要选择使用 STATIC 或 SHARED 关键字来构建你的库，或者使用 add_library 命令不带任何关键字来创建类型取决于构建类型的库（默认情况下可能是静态的，但可以通过 CMake 变量来控制）。\n一个简单的库的实现 include/my_math/my_math.h\n1 2 3 4 5 6 #ifndef MY_MATH_HEADER #define MY_MATH_HEADER int max(int, int); #endif // !MY_MATH_HEADER 为什么要有 `#Ifndef`、`#define`、`#endif` 这些？ 简单讲就是为了避免重复引用。\n先判断 #ifndef 也就是如果没有定义过这个 HEADER，那反过来讲如果定义过这个 HEADER 呢？是不是就执行后面所有的语句了？\n然后如果进入了，说明就是还没有引入过这个文件，就定义这个常量，这也帮助了第一步的判断\n最后是 #endif，说明整个 #ifndef 结束了\n1 2 3 4 5 6 7 8 9 #include \u0026#34;my_math/my_math.h\u0026#34; int max(int a, int b) { if (a \u0026gt; b) { return a; } return b; } 这是整个项目的所有文件，读者可以对照一下\n1 2 3 4 5 6 7 8 ├── CMakeLists.txt ├── include │ └── my_math │ └── my_math.h └── src └── my_math.c 4 directories, 3 files 构建这个项目 1 mkdir build \u0026amp;\u0026amp; cd build \u0026amp;\u0026amp; cmake .. \u0026amp;\u0026amp; cmake --build . 执行上述命令之后会生成很多文件，但目前我们需要关注的只有 libmy_math.a，这就是其他项目需要依赖的库了。\n管理依赖 下面看如何让 HelloWorld 项目依赖 my_math 项目。\n首先要明确的是，CMake 并不能像 Maven/Cargo 那样直接从互联网下载你需要的包，而是需要你自己下载了包之后放在项目依赖的目录下，然后编辑 CMakeLists.txt，让你的项目能够识别到这个依赖。这更像早期使用 Ant 编译 Java 的时代吧。\n最简单的依赖 HelloWorld 的结构暂时不变，但需要创建一个 include 目录来存放依赖，先把 lib-demo 的所有代码放在 HelloWorld 的 include 目录下。\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.27) project(HelloWorld) add_executable(HelloWorld src/main.c) add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/include/my_math) target_link_libraries(HelloWorld my_math) target_include_directories(HelloWorld PRIVATE include/my_math/include) `Target_include_directories` 的第二个参数 `PRIVATE` 是什么意思？ 这个命令为目标（可执行文件或库）指定包含目录，第二个参数可选值有 3 个：PUBLIC、PRIVATE 和 INTERFACE，这决定了包含目录的范围和传播行为：\nPRIVATE：指定的包含目录仅用于这个目标的构建，并且不会传递给依赖这个目标的其他目标。如果你有一个目标（如一个库或可执行文件），而这个目标的头文件仅在源文件中内部使用，没有在任何对外的头文件中使用，则应该将这些内部使用的头文件目录标记为 PRIVATE。\nPUBLIC：指定的包含目录既用于这个目标的构建，也会传递给依赖这个目标的其他目标。使用 PUBLIC 意味着连接到这个库的任何目标也将自动添加这些包含目录到它们的包含目录列表中。如果你的库的公共头文件需要某些路径才能被找到，那么这些路径应该被标记为 PUBLIC。\nINTERFACE：指定的包含目录不用于这个目标的构建，但会传递给依赖这个目标的其他目标。这通常用在只包含头文件的接口库上，这种情况下构建目标本身并不需要这些包含目录，但是使用该目标的其他目标需要。\n简而言之，PRIVATE 意味着仅用于构建当前目标，PUBLIC 意味着既用于构建当前目标也用于依赖它的目标，而 INTERFACE 意味着仅用于依赖当前目标的其他目标。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include \u0026#34;my_math/my_math.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; int main() { int a = 10, b = 5; int c = max(a, b); if (c != 10) { printf(\u0026#34;max is not correct\\n\u0026#34;); } else { printf(\u0026#34;c is %d, and max is included\\n\u0026#34;, c); } printf(\u0026#34;hello world!\\n\u0026#34;); return 0; } 接下来怎么让这个代码运行起来，这里就不再赘述了。\n嵌套的依赖 前面其实已经完整讲述了一个项目和它的依赖应该如何组织了，其实更深层的依赖管理和上面描述的也没有什么区别，比如上面的 my_math 库又依赖了别的库比如 his_math，这时你有两个选择：\n如果 HelloWorld 也使用到了 his_math，那你应该把它放在 HelloWorld 项目的 include 里，这样两个项目都可以使用它。 如果只有 my_math 用它，那就可以把它放在 my_math 的 include 里，具体的做法和上面的并无其他不同。 单元测试 没错，CMake 还能做单元测试。\n在 my_math 项目里添加一个 tests 目录，把 CMakeLists.txt 改一下，判断是否存在 tests 目录，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cmake_minimum_required(VERSION 3.27) project(my_math) # 设置C标准 set(CMAKE_C_STANDARD 11) set(CMAKE_C_STANDARD_REQUIRED True) # 创建一个明为my_math的静态库 add_library(${PROJECT_NAME} include/my_math/my_math.h src/my_math.c) add_subdirectory(third_party/his_math) target_link_libraries(${PROJECT_NAME} his_math) if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/tests/CMakeLists.txt) enable_testing() add_subdirectory(tests) include(CTest) endif() # 指定库的头文件搜索路径 target_include_directories(${PROJECT_NAME} PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include) 这里我把检测到 tests/CMakeLists.txt 之后的操作全部写在一个 if 里了。\n然后 tests 目录本身也需要一个 CMakeLists.txt，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cmake_minimum_required(VERSION 3.27) project(my_math_tests) # 设置C标准 set(CMAKE_C_STANDARD 11) set(CMAKE_C_STANDARD_REQUIRED True) enable_testing() add_executable(${PROJECT_NAME} test_my_math.c) # 指定库的头文件搜索路径 target_include_directories(${PROJECT_NAME} PUBLIC ${CMAKE_SOURCE_DIR}/include) target_link_libraries(${PROJECT_NAME} his_math my_math) add_test(NAME my_math_tests COMMAND ${PROJECT_NAME}) 这里要注意的点还是挺多的，比如前面写的都是 ${CMAKE_CURRENT_SOURCE_DIR}/include，这里却是 ${CMAKE_SOURCE_DIR}/include。\n其实这个单元测试就是一个的 executable，加了一个通过 ctest 命令启动的入口。\n然后写一个 test_my_math.c，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026#34;my_math/my_math.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; void test_max() { int result = max(1, 2); if (result == 2) { printf(\u0026#34;test_max passed.\\n\u0026#34;); } else { printf(\u0026#34;test_max failed. Expected 2 but got %d.\\n\u0026#34;, result); } } int main() { test_max(); // 这里可以添加更多的测试 return 0; // 如果有测试失败，你可能想要返回非零值 } 这时候去 my_math 的 build 目录里执行 cmake .. \u0026amp;\u0026amp; cmake --build . \u0026amp;\u0026amp; ctest ，就会发现单元测试已经可以执行了。\n1 2 3 4 5 6 7 8 ❯ ctest Test project /Users/frost/workspace/private/cmake-tutorial/simple-demo/third_party/my_math/build Start 1: my_math_tests 1/1 Test #1: my_math_tests .................... Passed 0.39 sec 100% tests passed, 0 tests failed out of 1 Total Test time (real) = 0.39 sec ","date":"2023-01-06T22:17:27+08:00","image":"https://techinsight.pages.dev/p/learn-cmake-in-30-min/cover_hu_628e564716127f4.png","permalink":"https://techinsight.pages.dev/p/learn-cmake-in-30-min/","title":"Learn Cmake in 30 Min"},{"content":"公司发的2020款M1芯片MacBook Pro前几天升级了macOS 13.0.1，噩梦开始了。\n是从官网看到新版本的macOS更新了更好用的Spotlight、邮件等app，觉得是挺好用的新特性，加上已经更新了一个小的bugfix版本，就更新了，结果遇到了以下问题\n无端突然黑屏隔几秒后回到登录界面，输入密码登录后提示WindowServer crash，一大堆没什么用的信息 Spotlight反应明显慢亿拍，这是我每天用几十次的功能，结果现在变成了唤起之后输入等5秒左右才显示在输入框里，然后开始搜索，所以是反向升级了 这是非常影响工作的，几乎变成不可用的状态了，毕竟一崩溃所有正在用的窗口全都异常关闭，好在没出现数据异常丢失的情况。而且最常用的功能反应慢了那么多。\n问了周围几个同事，有的很早之前就更新了也没有出现这个问题，我就纳闷了，后来发现好像他们都不是M1芯片。\nReddit上扒了一些帖子，发现遇到这个问题的还不在少数，而且目前看起来13.1Beta2也没有解决。好在有网友发现了问题的根源，按照这个帖子删除以下两个文件\n1 2 ~/Library/Preferences/ByHost/com.apple.windowserver.displays.\u0026lt;LONG HEX STRING\u0026gt;.plist /Library/Preferences/com.apple.windowserver.displays.plist 重启，就好了。不过这只是解决了黑屏崩溃的问题，并没有解决Spotlight反应慢的问题。\n其实中间想着降级来着，但不知道为什么已经回不到出厂的10.12 Monterey版本了，重新装了一遍13.0.1（已经备份的数据的情况），数据没有丢，问题也没有解决。好在现在这个最严重的问题解决了，就记录一下。\n","date":"2022-11-29T23:28:48+08:00","permalink":"https://techinsight.pages.dev/p/windowserver-crash-on-macos-13/","title":"WindowServer Crash on Macos 13"},{"content":"用Filebeat收集日志一直有一个痛点，就是不知道要什么时候才能把要收集的文件删除。\n之前的解决方案是定时删除文件，但删除的时候其实也不保证已经收集了，所以肯定是有概率会删除正在收集中的文件，这时候其实文件句柄正在被filebeat持有，所以删除也不能释放空间。\n简单的搜索就发现了这篇文章FileBeats -Are there any ways we can delete the log files after file beat harvest the data to logstash 。\n总结下来原理很简单，就是比较registry中的文件对应的offset，如果offset和当前文件的size相同，就表示已经收集完成（只是表示已经收集到文件最后的位置，并不表示文件不再继续写入了），这时候就可以删除文件了。\n而基于之前升级filebeat的经验，我们把所有filebeat实例配置都放在同一个目录下，我们的代码就只需要便利这个目录下所有的data/registry/filebeat/log.json文件，分析其中的数据即可。这里放一个简单的例子\n这个文件虽然是叫log.json，但其实内容是多行json，也就是每行是一个json串，这里要注意一下。\n1 2 {\u0026#34;k\u0026#34;:\u0026#34;filebeat::logs::native::361-2055\u0026#34;,\u0026#34;v\u0026#34;:{\u0026#34;timestamp\u0026#34;:[698286230,1641268968],\u0026#34;ttl\u0026#34;:-1,\u0026#34;type\u0026#34;:\u0026#34;log\u0026#34;,\u0026#34;FileStateOS\u0026#34;:{\u0026#34;inode\u0026#34;:361,\u0026#34;device\u0026#34;:2055},\u0026#34;source\u0026#34;:\u0026#34;path/to/log.log\u0026#34;,\u0026#34;offset\u0026#34;:153040051,\u0026#34;identifier_name\u0026#34;:\u0026#34;native\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;native::361-2055\u0026#34;,\u0026#34;prev_id\u0026#34;:\u0026#34;\u0026#34;}} {\u0026#34;op\u0026#34;:\u0026#34;set\u0026#34;,\u0026#34;id\u0026#34;:8250639157} 这里面需要处理的就是v.source和v.offset。处理过程中需要注意很多异常，但多数都是可以忽略的，主要是因为可能这个文件已经不存在了，但是registry信息还在这个log.json里面。\n不得不说用golang处理这种问题简直是“干净又卫生”，比用shell处理降低了非常多的心智负担，而且非常快速就能解决问题。在Mac上写的代码打一个linux的包放在/etc/cron.hourly目录下就能每小时执行一次了。\n","date":"2022-01-04T11:46:03+08:00","permalink":"https://techinsight.pages.dev/p/filebeat-%E8%87%AA%E5%8A%A8%E5%88%A0%E9%99%A4%E5%B7%B2%E7%BB%8F%E6%94%B6%E9%9B%86%E5%AE%8C%E7%9A%84%E6%96%87%E4%BB%B6/","title":"Filebeat 自动删除已经收集完的文件"},{"content":"终于看到Flink承认自己在背压高的时候Checkpoint慢的事实了。甚至详细介绍的文章都才只写了第一篇。\n关于Unaligned Checkpoint（非对齐检查点）的详细介绍官网上已经有很多了，前段时间刚发布了系列文章的第一篇\nFrom Aligned to Unaligned Checkpoints - Part 1: Checkpoints, Alignment, and Backpressure。其中明确提到了以下内容\nDespite all these great properties, Flink’s checkpointing method has an Achilles Heel: the speed of a completed checkpoint is determined by the speed at which data flows through the application. When the application backpressures, the processing of checkpoints is backpressured as well (Appendix 1 recaps what is backpressure and why it can be a good thing). In such cases, checkpoints may take longer to complete or even time out completely.\n之前一直觉得Flink在流式计算领域是神一样的存在，没有缺点。但实际用了之后才发现就这一点就够喝一壶了。所谓流式数据其实就是（没有边界的）消息队列了，那么消息队列的一大用途就是削峰填谷，好了，这里面的_消峰_就是在流量高峰的时候能以其极高的性能扛住压力，保证在数据压力降下来之前数据的不丢失。没错，Kafka在这里扛住了，但Flink掉链子了。\n所谓背压（有的叫反压，原文Back Pressure），对于数据源（DataSource）来说，其实就是下游的消费能力不足，导致上游数据无法完成整个流程（从数据源流入数据汇DataSink），具体到Kafka的这个场景来说就是业务处理的流程慢。\n正常来说，我们是希望当数据流量大的时候系统能加快处理，比如设计处理能力是1000tps，实际平时只有300tps，那么当流量上来时我们是期望它能按设计处理能力消费数据，让数据高峰尽快消散的，但实际情况是当数据量增大时，处理能力从300tps变成了2tps。\n是的，堆积越多处理越慢。反过来处理越慢，堆积越快。陷入了死循环。\n上面文章里也说了，导致这个结果的原因并不是真的是业务代码处理的慢，确确实实就是在背压出现时，Checkpoint变慢了。所以在新版本推出了非对齐检查点模式。\n这里有一个Inflight-data的概念，我理解就是新的检查点方式是把每个TaskManager中处理的数据都快照下来了，也不用管水位线什么的，直接搞起，完成一个删除上一个，带来的效果就是完成检查点的速度和背压没有太直接的关系了，实际的使用也验证了这一点。但和预期还是有稍稍的不同，从Kafka监控来看，按照之前对齐检查点方式，每个检查点完成后立即就能看到监控上的消费波峰，但非对齐检查点的完成和波峰就没有直接关系，不过它起码比对齐检查点好在不会在数据流量高峰到来时全部超时，导致系统瘫痪。\n带来的好处直观而明显，但不方便之处也是有的。\n对检查点存储后端的压力会非常大。 之前每个检查点大小是24K左右，而改成新的方式后就达到了200MB左右，对IO的压力增加可想而知，不过由于我们用的是rocksdb后端，所以这个压力可以承受。\n这种情况下自动创建的检查点不能用来扩容/缩容。 由于没有对齐，就没办法做内部的rescale，重启前后的TaskManager数量必须一致。但好在可以通过人工生成SavePoint的方式来创建一个完整的保存点，用保存点保证重启过程的数据不丢失。\n总结 总之Flink的这个新功能还是非常有用的，在使用这个功能之前数据量增大的时候只能祈祷它不超时，然而总是事与愿违。\n看文档说后面的目标是把非对齐检查点作为默认的检查点模式，从目前看还有很长的路要走。\n","date":"2021-02-04T22:55:48+08:00","permalink":"https://techinsight.pages.dev/p/flink%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7unaligned-checkpoints/","title":"Flink的新特性——Unaligned Checkpoints"},{"content":"我们都自称 CRUD boy 了，没有数据库拿什么 CRUD？\n实验环境 简单起见，我们在本地搭建一个 MySQL 服务，使用brew install mysql即可安装，之后通过brew services start mysql即可启动 MySQL 服务。\n相应的还可以通过brew services list查看运行中的服务，通过brew services stop mysql来停止 mysql 服务。如果是首次安装还需要执行mysql_secure_installation来初始化密码等。\n建库建表 1 2 3 4 5 6 7 create database happyhacker; use happyhacker; create table employee ( `id` int unsigned auto_increment primary key, `name` varchar(40) not null default \u0026#39;\u0026#39;, `age` tinyint unsigned not null default 0 ) engine=innodb default charset=utf8mb4; JDBC 简单来讲 JDBC 就是一套操作数据库的 API（interface），它是不包含实现的（implementation）。各数据库供应商（如 MySQL 等）提供驱动（实现）来完成对数据库的操作。\nJDBC 提供了两个功能：\nJDBC API 提供 Java 应用和 JDBC Manager 之间的通信 JDBC driver 提供了 JDBC Manager 和数据库驱动之间的通信 我们先来感受一下使用原生的 JDBC 怎样操作数据库。\n建立连接（Connection） 1 Connection connection = DriverManager.getConnection(\u0026#34;jdbc:mysql://127.0.0.1:3306/happyhacker\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;12345678\u0026#34;); 创建语句（Statement） 1 2 Statement stmt = connection.createStatement(); String sql = \u0026#34;select * from employee\u0026#34;; 执行语句并返回结果（ResultSet） 1 ResultSet rs = stmt.executeQuery(sql); 对结果集进行迭代取出数据 1 2 3 4 5 6 7 while (rs.next()) { int id = rs.getInt(\u0026#34;id\u0026#34;); String name = rs.getString(\u0026#34;name\u0026#34;); int age = rs.getInt(\u0026#34;age\u0026#34;); System.out.printf(\u0026#34;id: %d,\\t name: %s,\\t age: %d\\n\u0026#34;, id, name, age); } 关闭连接和语句 1 2 stmt.close(); connection.close(); 从上面的例子中也可以看出，通过 JDBC 对数据库的操作分为以下步骤：\n建立连接（Connection） 建立语句（Statement） 执行语句并返回结果（ResultSet） 对结果集进行迭代取出数据 下面列出的完整代码包含对数据库的增删改查\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 package fun.happyhacker.springbootdemo; import java.sql.*; public class JDBCTest { public static void main(String[] args) { jdbcTest(); } private static void jdbcTest() { Connection connection = null; Statement stmt = null; try { Class.forName(\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;); connection = DriverManager.getConnection(\u0026#34;jdbc:mysql://127.0.0.1:3306/happyhacker\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;12345678\u0026#34;); stmt = connection.createStatement(); String selectSql = \u0026#34;select * from employee\u0026#34;; ResultSet rs = stmt.executeQuery(selectSql); while (rs.next()) { int id = rs.getInt(\u0026#34;id\u0026#34;); String name = rs.getString(\u0026#34;name\u0026#34;); int age = rs.getInt(\u0026#34;age\u0026#34;); System.out.printf(\u0026#34;id: %d,\\t name: %s,\\t age: %d\\n\u0026#34;, id, name, age); } String insertSql = \u0026#34;insert into employee (`name`, `age`) values (\u0026#39;John\u0026#39;, 13)\u0026#34;; long effectedRows1 = stmt.executeUpdate(insertSql, new String[]{\u0026#34;id\u0026#34;}); if (effectedRows1 \u0026gt; 0) { System.out.println(\u0026#34;insert ok\u0026#34;); } else { System.out.println(\u0026#34;insert failed\u0026#34;); } String updateSql = \u0026#34;update employee set `name` = \u0026#39;Tam\u0026#39; where id = 3\u0026#34;; long effectedRows2 = stmt.executeUpdate(updateSql); if (effectedRows2 \u0026gt; 0) { System.out.println(\u0026#34;update ok\u0026#34;); } else { System.out.println(\u0026#34;update failed\u0026#34;); } String deleteSql = \u0026#34;delete from employee where id = 5\u0026#34;; long effectedRows3 = stmt.executeUpdate(deleteSql); if (effectedRows3 \u0026gt; 0) { System.out.println(\u0026#34;delete ok\u0026#34;); } else { System.out.println(\u0026#34;delete failed\u0026#34;); } rs.close(); stmt.close(); connection.close(); } catch (SQLException e) { e.printStackTrace(); } finally { if (stmt != null) { try { stmt.close(); } catch (SQLException throwables) { throwables.printStackTrace(); } } if (connection != null) { try { connection.close(); } catch (SQLException throwables) { throwables.printStackTrace(); } } } } } 不要忘了需要在pom.xml中添加如下依赖\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.20\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 注意其中的Class.forName(\u0026quot;com.mysql.cj.jdbc.Driver\u0026quot;)这句，老的文档都会写这句，但其实使用新版的 MySQL Driver 的话这个已经不需要了，因为新版本有这个\n连接池 前面我们看到Statement和Connection都是可以复用的，但这仅仅局限在一个方法中。想让一个连接在整个应用中都可用，就要把它做成全局变量。\n最简单的做法就是这样\n1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.jdbc; import java.sql.Connection; import java.sql.DriverManager; import java.sql.SQLException; public class JDBCConnection { public static Connection getConnection() throws SQLException { return DriverManager.getConnection(\u0026#34;jdbc:mysql://127.0.0.1:3306/happyhacker\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;12345678\u0026#34;); } } 这样在需要用它的地方直接调用JDBCConnection.getConnection()就可以了。但要注意用完之后可不要调用connection.close()，因为一旦 close 就无法再被别人使用了。\n再往下需要考虑的东西就很多了，比如\n用完之后不立即 close，那什么时候 close？ 一个连接用多少次 close？ 应用高并发的情况下不可能只用一个连接，多个连接如何保持？ 连接异常断开后如何自动重连？ 想想就觉得很复杂，好在这个问题已经有很完善的解决方案了。早前比较流行的有Apache 的 DBCP，老牌的 C3P0，打败它们俩的 BoneCP，后起之秀 Hikari CP等等，而 Hikari CP 得益于其优异的性能傲视群雄，目前已经是 Springboot 默认的连接池框架，搞的 BoneCP 的作者都公开声称不再更新了，建议大家去用 HikariCP。BoneCP 的 github 主页介绍只有这样一段了\nBoneCP BoneCP is a Java JDBC connection pool implementation that is tuned for high performance by minimizing lock contention to give greater throughput for your applications. It beats older connection pools such as C3P0 and DBCP but should now be considered deprecated in favour of HikariCP.\n这是得多心灰意冷啊。所以我们直接 HikariCP 就行了。对了还有一个国产的阿里出品 Druid，说实话用了那么多开源的东西之后，对阿里家开源的东西还是要谨慎一些，这个东西文档不丰富，用户遇到了问题反馈了得不到回应。\n最简单的数据库配置 最简单的配置就是使用 Springboot 内置的模板配置了，使用数据库需要引入一个依赖即可\n1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 引入这个依赖就不用再引用 HikariCP 了。\n然后在配置文件中加入这些\n1 2 3 4 5 spring.datasource.type=com.zaxxer.hikari.HikariDataSource spring.datasource.hikari.driver-class-name=com.mysql.cj.jdbc.Driver spring.datasource.hikari.username=root spring.datasource.url=jdbc:mysql://localhost:3306/happyhacker spring.datasource.hikari.password=12345678 接下来就可以直接注入DataSource了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 package fun.happyhacker.springbootdemo.controller; import fun.happyhacker.springbootdemo.hikari.Employee; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import javax.sql.DataSource; import java.sql.Connection; import java.sql.PreparedStatement; import java.sql.ResultSet; import java.sql.SQLException; import java.util.ArrayList; import java.util.List; @RestController public class EmployeeController { @Autowired private DataSource ds; @GetMapping(\u0026#34;/list\u0026#34;) public String list() { List\u0026lt;Employee\u0026gt; employees = new ArrayList\u0026lt;\u0026gt;(); try (Connection connection = ds.getConnection(); PreparedStatement ps = connection.prepareStatement(\u0026#34;select * from employee\u0026#34;); ResultSet rs = ps.executeQuery(); ) { Employee employee = new Employee(); while (rs.next()) { employee.setId(rs.getInt(\u0026#34;id\u0026#34;)); employee.setName(rs.getString(\u0026#34;name\u0026#34;)); employee.setAge(rs.getInt(\u0026#34;age\u0026#34;)); employees.add(employee); } } catch (SQLException throwables) { throwables.printStackTrace(); } return employees.toString(); } } 这里提一个小问题，访问这个接口你会发现你的响应是这样的\n1 [Employee(id=8, name=John, age=13), Employee(id=8, name=John, age=13), Employee(id=8, name=John, age=13), Employee(id=8, name=John, age=13), Employee(id=8, name=John, age=13), Employee(id=8, name=John, age=13), Employee(id=8, name=John, age=13)] 这和我们理解的 Json 是有区别的，更多信息可以参考实现 toJson() 方法\nJPA和MyBatis JPA本质就是Hibernate，是一个“轻量级”的ORM框架（至于是不是真的轻量，得看跟谁比）。上手还是非常简单的。\nJPA入门——增删改查 引入依赖 1 2 3 4 5 6 7 8 9 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.20\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 添加配置 application.properties 1 2 3 4 5 6 7 8 spring.datasource.type=com.zaxxer.hikari.HikariDataSource spring.datasource.hikari.driver-class-name=com.mysql.cj.jdbc.Driver spring.datasource.hikari.username=root spring.datasource.url=jdbc:mysql://localhost:3306/happyhacker?charset=utf8mb4 spring.datasource.hikari.password=12345678 spring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialect spring.jpa.show-sql=true spring.jpa.hibernate.ddl-auto=validate 创建实体类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package fun.happyhacker.springbootdemo.jpa.entity; import lombok.Data; import javax.persistence.*; @Entity @Table(name = \u0026#34;employee\u0026#34;) @Data public class Employee { @Id @GeneratedValue private Long id; @Column(length = 32) private String name; private Integer age; } 注意：上面的表名大小写不需区分，可以全部大写\n创建Repository接口 1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.jpa.repository; import fun.happyhacker.springbootdemo.jpa.entity.Employee; import org.springframework.data.jpa.repository.JpaRepository; import org.springframework.stereotype.Repository; import java.util.List; @Repository public interface EmployeeRepository extends JpaRepository\u0026lt;Employee, Long\u0026gt; { } 验证功能 有了上面这些配置，就可以体验JPA带给你的强大功能了。\n先看在Controller类中测试。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 package fun.happyhacker.springbootdemo.controller; import fun.happyhacker.springbootdemo.hikari.ExtensionMethods; import fun.happyhacker.springbootdemo.jpa.entity.Employee; import fun.happyhacker.springbootdemo.jpa.repository.EmployeeRepository; import lombok.experimental.ExtensionMethod; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; @RestController @ExtensionMethod({ExtensionMethods.class}) public class EmployeeController { @Autowired private EmployeeRepository employeeRepository; @GetMapping(\u0026#34;/list\u0026#34;) public String list() { Employee john = new Employee(); john.setAge(20); john.setName(\u0026#34;John\u0026#34;); Employee lam = new Employee(); lam.setName(\u0026#34;lam\u0026#34;); lam.setAge(30); employeeRepository.save(john); employeeRepository.save(lam); Employee first = employeeRepository.getOne(1L); first.setName(\u0026#34;happyhacker\u0026#34;); employeeRepository.save(first); employeeRepository.deleteById(2L); return employeeRepository.findAll().toString(); } } 在看junit中的测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 package fun.happyhacker.springbootdemo.jpa.repository; import fun.happyhacker.springbootdemo.jpa.entity.Employee; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import java.util.ArrayList; import java.util.List; import java.util.Optional; import static org.junit.jupiter.api.Assertions.assertEquals; import static org.junit.jupiter.api.Assertions.assertFalse; @SpringBootTest class EmployeeRepositoryTest { @Autowired private EmployeeRepository employeeRepository; @Test void crud() { Employee john = new Employee(); john.setAge(20); john.setName(\u0026#34;John\u0026#34;); Employee lam = new Employee(); lam.setName(\u0026#34;lam\u0026#34;); lam.setAge(30); Employee johnWithId = employeeRepository.save(john); assertEquals(john.getName(), johnWithId.getName()); Employee lamWithId = employeeRepository.save(lam); assertEquals(lam.getName(), lamWithId.getName()); lamWithId.setAge(45); Employee oldLam = employeeRepository.save(lamWithId); assertEquals(45, oldLam.getAge()); employeeRepository.delete(oldLam); Optional\u0026lt;Employee\u0026gt; employeeOptional = employeeRepository.findById(2L); assertFalse(employeeOptional.isPresent()); List\u0026lt;Employee\u0026gt; list = employeeRepository.findAll(); List\u0026lt;Employee\u0026gt; expected = new ArrayList\u0026lt;\u0026gt;(); expected.add(johnWithId); assertEquals(expected, list); } } JPA处理联合主键 JPA处理联合主键主要有两种方式，用法有区别，最终生成的SQL语句也有区别，具体用哪种方式主要看场景。\n在典型的RBAC权限控制系统中会有很多联合主键的情况，下面来取一个简单的例子说明。\nRole是角色，包含id RoleAccount是角色和员工的映射，其中包含roleId和accountId @IdClass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package fun.happyhacker.springbootdemo.jpa.idclass.entity; import lombok.Data; import javax.persistence.Entity; import javax.persistence.GeneratedValue; import javax.persistence.Id; @Entity @Data public class Role { @Id @GeneratedValue private Integer id; private String name; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package fun.happyhacker.springbootdemo.jpa.idclass.entity; import lombok.Data; import javax.persistence.Entity; import javax.persistence.GeneratedValue; import javax.persistence.Id; @Entity @Data public class Account { @Id @GeneratedValue private Integer id; private String name; private Integer age; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package fun.happyhacker.springbootdemo.jpa.idclass.entity; import lombok.Data; import javax.persistence.Entity; import javax.persistence.Id; import javax.persistence.IdClass; @Entity @Data @IdClass(RoleAccountId.class) public class RoleAccount { @Id private Integer roleId; @Id private Integer accountId; } 1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.jpa.idclass.entity; import lombok.Data; import java.io.Serializable; @Data public class RoleAccountId implements Serializable { private Integer roleId; private Integer accountId; } 注意：联合主键类需要实现Serializable接口\n1 2 3 4 5 6 7 8 package fun.happyhacker.springbootdemo.jpa.idclass.repository; import fun.happyhacker.springbootdemo.jpa.idclass.entity.RoleAccount; import fun.happyhacker.springbootdemo.jpa.idclass.entity.RoleAccountId; import org.springframework.data.jpa.repository.JpaRepository; public interface RoleAccountRepository extends JpaRepository\u0026lt;RoleAccount, RoleAccountId\u0026gt; { } 1 2 3 4 5 6 7 package fun.happyhacker.springbootdemo.jpa.idclass.repository; import fun.happyhacker.springbootdemo.jpa.idclass.entity.Role; import org.springframework.data.jpa.repository.JpaRepository; public interface RoleRepository extends JpaRepository\u0026lt;Role, Integer\u0026gt; { } 1 2 3 4 5 6 7 package fun.happyhacker.springbootdemo.jpa.idclass.repository; import fun.happyhacker.springbootdemo.jpa.idclass.entity.Account; import org.springframework.data.jpa.repository.JpaRepository; public interface AccountRepository extends JpaRepository\u0026lt;Account, Integer\u0026gt; { } 下面是单元测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 package fun.happyhacker.springbootdemo.jpa.idclass.repository; import fun.happyhacker.springbootdemo.jpa.idclass.entity.Account; import fun.happyhacker.springbootdemo.jpa.idclass.entity.Role; import fun.happyhacker.springbootdemo.jpa.idclass.entity.RoleAccount; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; @SpringBootTest class RoleAccountRepositoryTest { @Autowired private RoleRepository roleRepository; @Autowired private AccountRepository accountRepository; @Autowired private RoleAccountRepository roleAccountRepository; @Test void idClassTest() { Role admin = new Role(); admin.setId(1); admin.setName(\u0026#34;admin\u0026#34;); Role role1 = new Role(); role1.setId(2); role1.setName(\u0026#34;role1\u0026#34;); Role role2 = new Role(); role2.setId(3); role2.setName(\u0026#34;role2\u0026#34;); roleRepository.save(admin); roleRepository.save(role1); roleRepository.save(role2); Account john = new Account(); john.setAge(30); john.setName(\u0026#34;john\u0026#34;); john.setId(4); accountRepository.save(john); RoleAccount roleAccount = new RoleAccount(); roleAccount.setRoleId(admin.getId()); roleAccount.setAccountId(john.getId()); roleAccountRepository.save(roleAccount); } } @Embeddable和@EmbeddedId 和上面相同的代码就再贴了，只贴不同的文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 package fun.happyhacker.springbootdemo.jpa.embeddable.entity; import lombok.Data; import javax.persistence.EmbeddedId; import javax.persistence.Entity; @Entity @Data public class RoleAccount { @EmbeddedId private RoleAccountId roleAccountId; } 1 2 3 4 5 6 7 8 9 10 11 12 13 package fun.happyhacker.springbootdemo.jpa.embeddable.entity; import lombok.Data; import javax.persistence.Embeddable; import java.io.Serializable; @Data @Embeddable public class RoleAccountId implements Serializable { private Integer roleId; private Integer accountId; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 package fun.happyhacker.springbootdemo.jpa.embeddable.repository; import fun.happyhacker.springbootdemo.jpa.embeddable.entity.Account; import fun.happyhacker.springbootdemo.jpa.embeddable.entity.Role; import fun.happyhacker.springbootdemo.jpa.embeddable.entity.RoleAccount; import fun.happyhacker.springbootdemo.jpa.embeddable.entity.RoleAccountId; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; @SpringBootTest class RoleAccountRepositoryTest { @Autowired private RoleRepository roleRepository; @Autowired private AccountRepository accountRepository; @Autowired private RoleAccountRepository roleAccountRepository; @Test void idClassTest() { Role admin = new Role(); admin.setId(1); admin.setName(\u0026#34;admin\u0026#34;); Role role1 = new Role(); role1.setId(2); role1.setName(\u0026#34;role1\u0026#34;); Role role2 = new Role(); role2.setId(3); role2.setName(\u0026#34;role2\u0026#34;); roleRepository.save(admin); roleRepository.save(role1); roleRepository.save(role2); Account john = new Account(); john.setAge(30); john.setName(\u0026#34;john\u0026#34;); john.setId(4); accountRepository.save(john); RoleAccount roleAccount = new RoleAccount(); RoleAccountId roleAccountId = new RoleAccountId(); roleAccountId.setRoleId(admin.getId()); roleAccountId.setAccountId(john.getId()); roleAccount.setRoleAccountId(roleAccountId); roleAccountRepository.save(roleAccount); } } 小结 从上面的比较可以看出，最终使用的时候差别不大，都可以对联合主键进行很好的控制。区别在于，如果RoleAccountId类是从别人的jar包中引用的，你无法修改其中的内容，那么就无法添加@Embeddable注解，而这时候用@IdClass就比较简单了。除此之外的其他情况，我认为使用@Embeddable会更清楚一些。\nMyBatis——基础用法 MyBatis是面向数据表的，而JPA（Hibernate）则是面向领域对象的，前者更接近数据库，而后者更接近业务。JPA的JpaRepository提供了非常多的内置方法，但面对国内互联网公司灵活多变的需求，这些内置方法还是不够多。这也是为什么MyBatis在国内比较流行的原因，据我了解很多人在用国内的一个封装——MyBatis-Plus。同时，在做这个datalink项目的过程中，我也越来越发现现在这种方式的弊端（复杂、添加schema需要做的事情太多等等），所以以下的MyBatis的例子我们就使用MyBatis-Plus替代。\n引入依赖 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.4.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 创建表 1 2 3 4 5 6 7 8 9 10 DROP TABLE IF EXISTS user; CREATE TABLE user ( id BIGINT(20) NOT NULL COMMENT \u0026#39;主键ID\u0026#39;, name VARCHAR(30) NULL DEFAULT NULL COMMENT \u0026#39;姓名\u0026#39;, age INT(11) NULL DEFAULT NULL COMMENT \u0026#39;年龄\u0026#39;, email VARCHAR(50) NULL DEFAULT NULL COMMENT \u0026#39;邮箱\u0026#39;, PRIMARY KEY (id) ); 填充数据 1 2 3 4 5 6 INSERT INTO user (id, name, age, email) VALUES (1, \u0026#39;Jone\u0026#39;, 18, \u0026#39;test1@baomidou.com\u0026#39;), (2, \u0026#39;Jack\u0026#39;, 20, \u0026#39;test2@baomidou.com\u0026#39;), (3, \u0026#39;Tom\u0026#39;, 28, \u0026#39;test3@baomidou.com\u0026#39;), (4, \u0026#39;Sandy\u0026#39;, 21, \u0026#39;test4@baomidou.com\u0026#39;), (5, \u0026#39;Billie\u0026#39;, 24, \u0026#39;test5@baomidou.com\u0026#39;); 如果你自己测试用内存数据库h2，这些sql就不需要提前执行，只需要把建表语句schema.sql和数据语句data.sql放在resource目录下，添加以下两行配置即可\n1 2 spring.datasource.schema=classpath:database/h2/schema.sql spring.datasource.data=classpath:database/h2/data.sql 添加配置 1 2 3 spring.datasource.url=jdbc:mysql://localhost:3306/happyhacker?charset=utf8mb4 spring.datasource.hikari.username=root spring.datasource.hikari.password=12345678 创建实体类 1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.mybatis.entity; import lombok.Data; @Data public class User { private Long id; private String name; private Integer age; private String email; } 创建Mapper 1 2 3 4 5 6 7 8 package fun.happyhacker.springbootdemo.mybatis.mapper; import fun.happyhacker.springbootdemo.mybatis.entity.User; import org.apache.ibatis.annotations.Mapper; @Mapper public interface UserMapper { } 添加方法 接下来就可以在UserMapper类中添加增删改查的方法了。\n在UserMapper.java中添加相关方法 1 2 3 4 5 6 7 8 9 package fun.happyhacker.springbootdemo.mybatis.mapper; import fun.happyhacker.springbootdemo.mybatis.entity.User; import org.apache.ibatis.annotations.Mapper; @Mapper public interface UserMapper extends BaseMapper { List\u0026lt;User\u0026gt; queryByNameEquals(String userName); } 在resources/fun/happyhacker/springbootdemo/mybatis/mapper中添加UserMapper.xml，内容如下 1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34;\u0026gt; \u0026lt;mapper namespace=\u0026#34;fun.happyhacker.springbootdemo.mybatis.mapper.UserMapper\u0026#34;\u0026gt; \u0026lt;resultMap id=\u0026#34;user\u0026#34; type=\u0026#34;fun.happyhacker.springbootdemo.mybatis.entity.User\u0026#34;\u0026gt; \u0026lt;id column=\u0026#34;id\u0026#34; property=\u0026#34;id\u0026#34; javaType=\u0026#34;int\u0026#34;/\u0026gt; \u0026lt;result column=\u0026#34;name\u0026#34; property=\u0026#34;name\u0026#34; javaType=\u0026#34;String\u0026#34;/\u0026gt; \u0026lt;result column=\u0026#34;age\u0026#34; property=\u0026#34;age\u0026#34; javaType=\u0026#34;int\u0026#34;/\u0026gt; \u0026lt;/resultMap\u0026gt; \u0026lt;select id=\u0026#34;queryByNameEquals\u0026#34; parameterType=\u0026#34;java.lang.String\u0026#34; resultMap=\u0026#34;user\u0026#34; resultType=\u0026#34;fun.happyhacker.springbootdemo.mybatis.entity.User\u0026#34;\u0026gt; SELECT * FROM user WHERE `name` = #{userName, jdbcType=VARCHAR} \u0026lt;/select\u0026gt; \u0026lt;/mapper\u0026gt; 注意：UserMapper.xml和UserMapper.java所在的包名一定要相同。\n这样就可以以同样的方式调用userMapper.queryByNameEquals(name)方法了。\n单元测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package fun.happyhacker.springbootdemo.mybatis.mapper; import fun.happyhacker.springbootdemo.mybatis.entity.User; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import java.util.List; import static org.junit.jupiter.api.Assertions.assertEquals; @SpringBootTest class UserMapperTest { @Autowired private UserMapper userMapper; @Test void testSelect() { List\u0026lt;User\u0026gt; userList = userMapper.queryByNameEquals(\u0026#34;Jone\u0026#34;); assertEquals(1, userList.size()); } } 如果不想在每个xxxMapper上面写@Mapper注解，可以在启动类上加@MapperScan(\u0026quot;fun.happyhacker.springbootdemo.mybatis.mapper\u0026quot;)，让系统自动扫描\nMyBatis——MyBatis-Plus 上节虽然用的是MyBatis-Plus，但并没有用到任何它独有的特性，下面来介绍一下。\n内置方法 前面我们自己写了一个 List\u0026lt;User\u0026gt; queryByNameEquals(String userName);，但实际上这种增删改查的方法已经内置在BaseMapper\u0026lt;T\u0026gt;中了，多数时候我们只需要让UserMapper扩展BaseMapper\u0026lt;User\u0026gt;即可。\n代码生成 这个和原生MyBatis的代码生成功能差不多，多出的功能在于它能生成controller、service层的代码，虽然都是空的。而且能生成带lombok的的代码。\n这一点其实和国外团队的思路是不同的，他们可能更多的考虑职责分离，克制的加入带有个人主观偏好的功能，但国人开发的MyBatis-Plus则不同，更多从实际开发角度出发考虑问题，而不太关注设计模式。这也很像诸多的Android定制版，国内的各种定制版你可能说它臃肿、不简洁，但你不能说它不好用。\n添加依赖 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-generator\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.4.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 编写CodeGenerator.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 package fun.happyhacker.springbootdemo.mybatis; import com.baomidou.mybatisplus.core.exceptions.MybatisPlusException; import com.baomidou.mybatisplus.core.toolkit.StringUtils; import com.baomidou.mybatisplus.generator.AutoGenerator; import com.baomidou.mybatisplus.generator.config.DataSourceConfig; import com.baomidou.mybatisplus.generator.config.GlobalConfig; import com.baomidou.mybatisplus.generator.config.PackageConfig; import com.baomidou.mybatisplus.generator.config.StrategyConfig; import com.baomidou.mybatisplus.generator.config.rules.NamingStrategy; import java.util.Scanner; public class CodeGenerator { public static void main(String[] args) { AutoGenerator mpg = new AutoGenerator(); GlobalConfig gc = new GlobalConfig(); String projectPath = System.getProperty(\u0026#34;user.dir\u0026#34;) + \u0026#34;/springboot-demo\u0026#34;; gc.setOutputDir(projectPath + \u0026#34;/src/main/java\u0026#34;); gc.setAuthor(\u0026#34;happyhacker\u0026#34;); gc.setOpen(false); mpg.setGlobalConfig(gc); DataSourceConfig dsc = new DataSourceConfig(); dsc.setUrl(\u0026#34;jdbc:mysql://localhost:3306/happyhacker?charset=utf8mb4\u0026#34;); dsc.setDriverName(\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;); dsc.setUsername(\u0026#34;root\u0026#34;); dsc.setPassword(\u0026#34;12345678\u0026#34;); mpg.setDataSource(dsc); PackageConfig pc = new PackageConfig(); pc.setModuleName(scanner(\u0026#34;模块名\u0026#34;)); pc.setParent(\u0026#34;fun.happyhacker.mybatis\u0026#34;); mpg.setPackageInfo(pc); // InjectionConfig cfg = new InjectionConfig() { // @Override // public void initMap() { // // } // }; // String templatePath = \u0026#34;/templates/mapper.xml.vm\u0026#34;; // List\u0026lt;FileOutConfig\u0026gt; focList = new ArrayList\u0026lt;\u0026gt;(); // focList.add(new FileOutConfig(templatePath) { // @Override // public String outputFile(TableInfo tableInfo) { // return projectPath + \u0026#34;/src/main/resources/mapper/\u0026#34; + pc.getModuleName() + \u0026#34;/\u0026#34; + tableInfo.getEntityName() + \u0026#34;Mapper\u0026#34; + StringPool.DOT_XML; // } // }); // cfg.setFileOutConfigList(focList); // mpg.setCfg(cfg); // // TemplateConfig templateConfig = new TemplateConfig(); // templateConfig.setXml(null); // mpg.setTemplate(templateConfig); StrategyConfig strategyConfig = new StrategyConfig(); strategyConfig.setNaming(NamingStrategy.underline_to_camel); strategyConfig.setColumnNaming(NamingStrategy.underline_to_camel); // strategyConfig.setSuperEntityClass(\u0026#34;父类Entity，没有可以不用设置\u0026#34;); strategyConfig.setEntityLombokModel(true); strategyConfig.setRestControllerStyle(true); // strategyConfig.setSuperControllerClass(\u0026#34;父类Controller，没有就不用设置\u0026#34;); strategyConfig.setSuperEntityColumns(\u0026#34;id\u0026#34;); strategyConfig.setTablePrefix(pc.getModuleName() + \u0026#34;_\u0026#34;); mpg.setStrategy(strategyConfig); mpg.execute(); } private static String scanner(String tip) { Scanner scanner = new Scanner(System.in); System.out.println(\u0026#34;请输入 \u0026#34; + tip + \u0026#34;：\u0026#34;); if (scanner.hasNext()) { String ipt = scanner.next(); if (StringUtils.isNotBlank(ipt)) { return ipt; } } throw new MybatisPlusException(\u0026#34;请输入正确的 \u0026#34; + tip + \u0026#34;！\u0026#34;); } } 测试就不再赘述了。\n小结 数据库方面总的来说有JPA和MyBatis两种选择，一般来说国内用MyBatis比较多，这和我们在设计系统时先从数据库开始考虑的思维方式有关系，因为JPA更适应DDD（领域驱动设计）的开发方式，而这种方式在我们实际项目开发中几乎不存在。不过在我看来其实MyBatis-Plus提供的BaseMapper和JPA提供的JpaRepository作用其实差不多，而且后者多数时候更好用。\n虽说我个人更倾向于使用JPA，但考虑到国内用户的使用习惯和招聘难度，加上之前项目选择React而不是Vue引起大部分同事的抵触，最终还是决定使用MyBatis。\n实际项目中的数据库配置 实际的项目不会像上面那样简单，需要考虑更多的问题\n多数据库实例，肯定就不能用spring.datasource.type这种配置了 主从分离 得益于dynamic-datasource，我们就不用再费劲的自己通过代码配置数据源和主从分离了，而是简单的通过增加配置即可实现。\n回顾 首先让我们来回顾一下前面的经验，要实现一个简单的CRUD，需要如下的class/interface\nEntity -\u0026gt; User Mapper -\u0026gt; UserMapper Service -\u0026gt; UserService ServiceImpl -\u0026gt; UserServiceImpl 在UserService中，可能存在List\u0026lt;User\u0026gt; allUsers()和int createUser(User user)两个方法，如下\n1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.mybatis.service; import fun.happyhacker.springbootdemo.mybatis.entity.User; import java.util.List; public interface UserService { List\u0026lt;User\u0026gt; listAllUsers(); int createUser(User user); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 package fun.happyhacker.springbootdemo.mybatis.service.impl; import com.baomidou.dynamic.datasource.annotation.DS; import com.baomidou.mybatisplus.core.conditions.query.QueryWrapper; import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl; import fun.happyhacker.springbootdemo.mybatis.entity.User; import fun.happyhacker.springbootdemo.mybatis.mapper.UserMapper; import fun.happyhacker.springbootdemo.mybatis.service.UserService; import org.springframework.stereotype.Service; import java.util.List; import java.util.Objects; @Service public class UserServiceImpl extends ServiceImpl\u0026lt;UserMapper, User\u0026gt; implements UserService { @Override @DS(\u0026#34;account_slave\u0026#34;) public List\u0026lt;User\u0026gt; listAllUsers() { QueryWrapper\u0026lt;User\u0026gt; wrapper = new QueryWrapper\u0026lt;\u0026gt;(); wrapper.gt(\u0026#34;id\u0026#34;, 0); return baseMapper.selectList(wrapper); } @Override @DS(\u0026#34;account_master\u0026#34;) public int createUser(User user) { if (Objects.isNull(user)) { return 0; } return baseMapper.insert(user); } } 你可能已经注意到了这里面的@DS注解，它就是用来配置主从分离的。要实现上述的功能我们需要引入以下依赖\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dynamic-datasource-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; dynamic-datasource的配置比较多，用yaml写会更方便一些\n添加数据源相关的配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 spring: datasource: dynamic: hikari: connection-timeout: 5000 leak-detection-threshold: 5000 max-lifetime: 30000 primary: account_master strict: false datasource: account_master: url: jdbc:mysql://192.168.99.101:3306/account username: account password: account! driver-class-name: com.mysql.cj.jdbc.Driver account_slave: url: jdbc:mysql://192.168.99.101:3307/account username: account_r password: account! driver-class-name: com.mysql.cj.jdbc.Driver 通过这个配置，我们可以很简单的配置主从分离。但这个方法有个致命的问题——分表。它提供了一个TableNameHandler可以用来根据传入的SQL来选择相应的表，看起来很美好，但实际上很难实现，看下代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 protected String changeTable(String sql) { TableNameParser parser = new TableNameParser(sql); List\u0026lt;TableNameParser.SqlToken\u0026gt; names = new ArrayList\u0026lt;\u0026gt;(); parser.accept(names::add); StringBuilder builder = new StringBuilder(); int last = 0; for (TableNameParser.SqlToken name : names) { int start = name.getStart(); if (start != last) { builder.append(sql, last, start); String value = name.getValue(); TableNameHandler handler = tableNameHandlerMap.get(value); if (handler != null) { builder.append(handler.dynamicTableName(sql, value)); } else { builder.append(value); } } last = name.getEnd(); } if (last != sql.length()) { builder.append(sql.substring(last)); } return builder.toString(); } 可以看到，它提供的方法是接收PreparedStatement中的sql部分，通俗点说就是还没有替换变量的SQL，比如我们要通过其中的uid字段以uid%128作为表名的后缀，通过这种方式就很难实现了。所以下一步就要祭出大杀器Apache ShardingSphere，它提供了针对分库分表的一整套解决方案。在实际应用中，我们只关心ShardingSphere-JDBC即可。\nShardingSphere-JDBC 在开始介绍之前，首先设定一个场景。\n假设我们的项目是要处理订单，由于订单很多，需要不仅要分表还要分库。为了简化问题，我们就分成2个库，每个库4张表，每个库一主一从。\norders_0 主库，包含表t_orders_0, t_orders_2, t_orders_4, t_orders_8 orders_1 主库，包含表 t_orders_1, t_orders_3, t_orders_5, t_orders_7 slave_0 从库，包含表t_orders_0, t_orders_2, t_orders_4, t_orders_8 slave_1从库，包含表 t_orders_1, t_orders_3, t_orders_5, t_orders_7 根据user_id分库，库名后缀为user_id%2，后缀可选范围为0和1 根据user_id分表，表名后缀为user_id%8，后缀可选范围为0-7 为了演示不需要分表的场景，添加一个t_order_config表，位于orders_0主库和orders_0从库 启动数据库服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 version: \u0026#34;3.3\u0026#34; services: db-3306: image: mysql:latest restart: always ports: - \u0026#34;3306:3306\u0026#34; volumes: - \u0026#34;3306:/var/lib/mysql:rw\u0026#34; environment: MYSQL_ALLOW_EMPTY_PASSWORD: \u0026#34;yes\u0026#34; MYSQL_ROOT_PASSWORD: \u0026#34;root\u0026#34; db-3307: image: mysql:latest restart: always ports: - \u0026#34;3307:3306\u0026#34; volumes: - \u0026#34;3307:/var/lib/mysql:rw\u0026#34; environment: MYSQL_ALLOW_EMPTY_PASSWORD: \u0026#34;yes\u0026#34; MYSQL_ROOT_PASSWORD: \u0026#34;root\u0026#34; db-3308: image: mysql:latest restart: always ports: - \u0026#34;3308:3306\u0026#34; volumes: - \u0026#34;3308:/var/lib/mysql:rw\u0026#34; environment: MYSQL_ALLOW_EMPTY_PASSWORD: \u0026#34;yes\u0026#34; MYSQL_ROOT_PASSWORD: \u0026#34;root\u0026#34; db-3309: image: mysql:latest restart: always ports: - \u0026#34;3309:3306\u0026#34; volumes: - \u0026#34;3309:/var/lib/mysql:rw\u0026#34; environment: MYSQL_ALLOW_EMPTY_PASSWORD: \u0026#34;yes\u0026#34; MYSQL_ROOT_PASSWORD: \u0026#34;root\u0026#34; volumes: \u0026#34;3306\u0026#34;: \u0026#34;3307\u0026#34;: \u0026#34;3308\u0026#34;: \u0026#34;3309\u0026#34;: 使用上面的docker-compose.yml文件，启动数据库服务\n1 docker-compose up 得到4个数据库端口。对应关系如下表\n端口 库名 主/从 3306 orders_0 主 3307 orders_1 主 3308 slave_0 从 3309 slave_1 从 建表 在3306和3308端口执行以下SQL语句\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 create database orders_0; use orders_0; SET NAMES utf8mb4; SET FOREIGN_KEY_CHECKS = 0; -- ---------------------------- -- Table structure for t_orders_0 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_0`; CREATE TABLE `t_orders_0` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for t_orders_2 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_2`; CREATE TABLE `t_orders_2` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for t_orders_4 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_4`; CREATE TABLE `t_orders_4` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for t_orders_6 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_6`; CREATE TABLE `t_orders_6` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for order_config -- ---------------------------- DROP TABLE IF EXISTS `t_order_config`; CREATE TABLE `t_order_config` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;编号\u0026#39;, `pay_timeout` int(11) DEFAULT NULL COMMENT \u0026#39;支付超时时间;单位：分钟\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单配置表\u0026#39;; SET FOREIGN_KEY_CHECKS = 1; 在3307和3309端口执行以下SQL语句\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 create database orders_1; use orders_1; SET NAMES utf8mb4; SET FOREIGN_KEY_CHECKS = 0; -- ---------------------------- -- Table structure for t_orders_1 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_1`; CREATE TABLE `t_orders_1` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=400675304294580226 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for t_orders_3 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_3`; CREATE TABLE `t_orders_3` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for t_orders_5 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_5`; CREATE TABLE `t_orders_5` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for t_orders_7 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_7`; CREATE TABLE `t_orders_7` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; SET FOREIGN_KEY_CHECKS = 1; 配置依赖 1 2 3 4 5 6 7 8 9 10 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.4.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.shardingsphere\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;sharding-jdbc-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 编写相应的POJO Entity 1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.mybatis.orders.entity; import com.baomidou.mybatisplus.annotation.TableName; import lombok.Data; @Data @TableName(value = \u0026#34;t_orders\u0026#34;) public class Order { private Integer id; private Integer userId; } 1 2 3 4 5 6 7 8 9 10 11 12 13 package fun.happyhacker.springbootdemo.mybatis.orders.entity; import com.baomidou.mybatisplus.annotation.TableName; import lombok.Data; import java.time.LocalDateTime; @Data @TableName(value = \u0026#34;t_order_config\u0026#34;) public class OrderConfig { private Integer id; private LocalDateTime pay_timeout; } Mapper 1 2 3 4 5 6 7 8 9 package fun.happyhacker.springbootdemo.mybatis.orders.mapper; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import fun.happyhacker.springbootdemo.mybatis.orders.entity.OrderConfig; import org.apache.ibatis.annotations.Mapper; @Mapper public interface OrderConfigMapper extends BaseMapper\u0026lt;OrderConfig\u0026gt; { } 1 2 3 4 5 6 7 8 9 10 package fun.happyhacker.springbootdemo.mybatis.orders.mapper; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import fun.happyhacker.springbootdemo.mybatis.orders.entity.Order; import org.apache.ibatis.annotations.Mapper; @Mapper public interface OrderMapper extends BaseMapper\u0026lt;Order\u0026gt; { Order selectByUserId(Integer userId); } XML Mapper 1 2 3 4 5 6 7 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34;\u0026gt; \u0026lt;mapper namespace=\u0026#34;fun.happyhacker.springbootdemo.mybatis.orders.mapper.OrderMapper\u0026#34;\u0026gt; \u0026lt;select id=\u0026#34;selectByUserId\u0026#34; parameterType=\u0026#34;Integer\u0026#34; resultType=\u0026#34;Order\u0026#34;\u0026gt; select * from t_orders where user_id = #{userId} order by id desc limit 1 \u0026lt;/select\u0026gt; \u0026lt;/mapper\u0026gt; Service 和 ServiceImpl 1 2 3 4 5 6 7 package fun.happyhacker.springbootdemo.mybatis.orders.service; import com.baomidou.mybatisplus.extension.service.IService; import fun.happyhacker.springbootdemo.mybatis.orders.entity.OrderConfig; public interface OrderConfigService extends IService\u0026lt;OrderConfig\u0026gt; { } 1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.mybatis.orders.service; import com.baomidou.mybatisplus.extension.service.IService; import fun.happyhacker.springbootdemo.mybatis.orders.entity.Order; import org.apache.ibatis.annotations.Param; public interface OrderService extends IService\u0026lt;Order\u0026gt; { Order selectByUserId(@Param(\u0026#34;userId\u0026#34;) Integer userId); Order selectAndUpdate(@Param(\u0026#34;userId\u0026#34;) Integer userId); } 1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.mybatis.orders.service.impl; import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl; import fun.happyhacker.springbootdemo.mybatis.orders.entity.OrderConfig; import fun.happyhacker.springbootdemo.mybatis.orders.mapper.OrderConfigMapper; import fun.happyhacker.springbootdemo.mybatis.orders.service.OrderConfigService; import org.springframework.stereotype.Service; @Service public class OrderConfigServiceImpl extends ServiceImpl\u0026lt;OrderConfigMapper, OrderConfig\u0026gt; implements OrderConfigService { } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package fun.happyhacker.springbootdemo.mybatis.orders.service.impl; import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl; import fun.happyhacker.springbootdemo.mybatis.orders.entity.Order; import fun.happyhacker.springbootdemo.mybatis.orders.mapper.OrderMapper; import fun.happyhacker.springbootdemo.mybatis.orders.service.OrderService; import lombok.extern.log4j.Log4j2; import org.springframework.stereotype.Service; import org.springframework.transaction.annotation.Transactional; @Service @Log4j2 public class OrderServiceImpl extends ServiceImpl\u0026lt;OrderMapper, Order\u0026gt; implements OrderService { @Override public Order selectByUserId(Integer userId) { return baseMapper.selectByUserId(userId); } @Transactional @Override public Order selectAndUpdate(Integer userId) { Order aOrder = new Order(); aOrder.setUserId(20); baseMapper.insert(aOrder); return baseMapper.selectByUserId(20); } } 应用配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 server: port: 8080 spring: shardingsphere: datasource: names: master0, master1, slave0, slave1 master0: type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.cj.jdbc.Driver jdbcUrl: jdbc:mysql://192.168.99.101:3306/orders_0?charset=utf8mb4\u0026amp;allowPublicKeyRetrieval=true\u0026amp;useSSL=false username: root password: root master1: type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.cj.jdbc.Driver jdbcUrl: jdbc:mysql://192.168.99.101:3307/orders_1?charset=utf8mb4\u0026amp;allowPublicKeyRetrieval=true\u0026amp;useSSL=false username: root password: root slave0: type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.cj.jdbc.Driver jdbcUrl: jdbc:mysql://192.168.99.101:3308/orders_0?charset=utf8mb4\u0026amp;allowPublicKeyRetrieval=true\u0026amp;useSSL=false username: root password: root slave1: type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.cj.jdbc.Driver jdbcUrl: jdbc:mysql://192.168.99.101:3309/orders_1?charset=utf8mb4\u0026amp;allowPublicKeyRetrieval=true\u0026amp;useSSL=false username: root password: root sharding: tables: t_orders: key-generator: column: id type: SNOWFLAKE actual-data-nodes: ds0.t_orders_$-\u0026gt;{[0,2,4,6]}, ds1.t_orders_$-\u0026gt;{[1,3,5,7]} table-strategy: inline: algorithm-expression: t_orders_$-\u0026gt;{user_id % 8} sharding-column: user_id database-strategy: inline: algorithm-expression: ds$-\u0026gt;{user_id % 2} sharding-column: user_id t_order_config: actual-data-nodes: ds0.t_order_config master-slave-rules: ds0: master-data-source-name: master0 slave-data-source-names: slave0 ds1: master-data-source-name: master1 slave-data-source-names: slave1 default-database-strategy: inline: sharding-column: user_id algorithm-expression: master$-\u0026gt;{user_id % 2} props: log: show: true # 在日志中打印执行过程 sql: show: true # 打印Logic SQL和 Actual SQL，非常方便排查问题 mybatis-plus: configuration: map-underscore-to-camel-case: true # 类属性中的驼峰字段到数据库中的下划线字段 global-config: db-config: id-type: none logic-delete-value: 1 logic-not-delete-value: 0 mapper-locations: classpath*:mapper/*.xml # 搜索resources/mapper/目录下的XML文件作为Mapper type-aliases-package: fun.happyhacker.springbootdemo.mybatis.orders # 使XML Mapper中可以直接使用短类名，而不需要包名 上面的配置文件展示了需要和不需要分库分表的情况下的不同配置，理清楚之后也很清晰。一定要注意的是database-strategy的值引用的是spring.shardingsphere.datasource.names中的值，而不是实际的库名。\n*-strategy.algorithm-expression是Groovy脚本，本身Apache ShardingSphere支持两种写法\nmaster${user_id % 2} master$-\u0026gt;{user_id % 2} 但第一种方式在Spring环境中会被认为user_id % 2是一个变量名而不是表达式，所以在Spring环境中需要使用第二种方式。\n事务 上面的代码中有一个方法标记了@Transactional，表明是支持事务的。在本例中，插入数据之后后面的方法也会去相应的主库中查找对应的数据而不是去从库中查，你可以把@Transactional注解移除验证一下。\n总结 关于数据库的所有内容到这里就结束了，我们从最基础的JDBC开始学习，了解了主流的JPA和MyBatis的基础用法，以及在使用这些框架时实际遇到的问题，后面引入了MyBatis-Plus来消除模板代码，使用内置的方法减少代码量。\n在介绍主从分离和分库分表时我们遇到了挑战，使用Dynamic-Datasource不能解决问题，因此我们引入了Apache ShardingSphere来解决。问中给出了非常详细的代码，希望对你的学习有帮助。\n","date":"2021-02-04T22:43:50+08:00","permalink":"https://techinsight.pages.dev/p/springboot%E4%BD%BF%E7%94%A8shardingsphere%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E5%BA%93/","title":"Springboot使用Shardingsphere配置数据库"},{"content":"Java 中 POJO 的toString()方法和我们预期的 JSON 格式不符合，而如果直接覆盖它写一个生成 JSON 的又不合适，因为当需要那种格式的时候就没得用的，所以本着各司其职的原则，我们来实现一个toJson()方法。\n序列化库选择 我比较熟悉的就是 Jackson 和 Gson，其中 Gson 的使用较为简单，所以这里用 Gson 来实现。其实主要原因是 Jackson 在序列化对向的时候会抛出异常，而 Gson 就不会。这里简单验证一下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package fun.happyhacker.json; import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.ObjectMapper; import com.google.gson.Gson; public class JsonTest { public static void main(String[] args) { Employee employee = new Employee(); employee.setAge(10); employee.setId(1); employee.setName(\u0026#34;John\u0026#34;); ObjectMapper objectMapper = new ObjectMapper(); String jackson = \u0026#34;\u0026#34;; try { jackson = objectMapper.writeValueAsString(employee); } catch (JsonProcessingException e) { e.printStackTrace(); } System.out.println(jackson); String gson = new Gson().toJson(employee); System.out.println(gson); System.out.println(new GsonBuilder().serializeNulls().create().toJson(employee)); } } 1 2 3 {\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:null,\u0026#34;age\u0026#34;:10} {\u0026#34;id\u0026#34;:1,\u0026#34;age\u0026#34;:10} {\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:null,\u0026#34;age\u0026#34;:10} 所以，起码在序列化 JSON 这方面，可以认为两个库的作用是一致的，但 Gson 用起来很简单。\n给每个 POJO 添加toJson方法 所以是不是就要人肉给每个 POJO 添加这个方法了，比如上面提到的Employee\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package fun.happyhacker.json; import com.google.gson.GsonBuilder; import lombok.Data; @Data public class Employee { private int id; private String name; private int age; public String toJson() { return new GsonBuilder().serializeNulls().create().toJson(this); } } 虽说是可行，但明显有太多了模板代码可以消除了。\n利用 lombok 其实这个方法我都是在 lombok 相关的帖子下看到的，考虑到目前 lombok 并没有提供类似@ToJson这种注解，那么可以利用它的ExtensionMethod来实现。\n首先创建一个扩展方法集\n1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.json; import com.google.gson.GsonBuilder; import java.io.Serializable; public class Extensions { public static \u0026lt;T extends Serializable\u0026gt; String toJson(T t) { return new GsonBuilder().serializeNulls().create().toJson(t); } } 这里面我加了一点小的限制，让POJO 类必须要可以序列化才能使用toJson()方法。\n然后在需要调用 toJson() 方法的所在的类上加上 @ExtensionMethod 注解\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package fun.happyhacker.json; import lombok.experimental.ExtensionMethod; @ExtensionMethod({Extensions.class}) public class JsonTest { public static void main(String[] args) { Employee employee = new Employee(); employee.setAge(10); employee.setId(1); employee.setName(\u0026#34;John\u0026#34;); System.out.println(employee.toJson()); } } 这时你会发现 IDEA 识别不了这个toJson()方法，但没关系，它是可以正常执行的，这是 IDEA 的 lombok 扩展不支持而已。在这一点上 Eclipse 已经领先了，虽然在其他所有方面 Eclipse 都是惨遭碾压。\n简单讲就是 lombok 在编译期把employee.toJson()这个方法改写成了new GsonBuilder().serializeNulls().create().toJson(employee)，这也解释了为什么Extensions中的方法需要是静态的。\n总结 虽说是解决了一部分问题，但我觉得这个问题解决的不够优雅，按我们正常的思维，这个注解是应该加到Employee这个类上的，而不是加到调用它的类上。所以就很尴尬了。不知道能不能参与到 lombok 官方项目中，给它加上这个功能。\n经过简单的搜索发现这个问题已经被讨论过无数遍了，核心问题是核心开发者认为这个和项目设计之初的目标不符合（我觉得这点站不住脚），他们拿toString()来做比较，说本来每个类也都有toString，只是没有实现，但几乎没有类存在toJson这个方法。其实本质上还是因为实现起来太复杂了，没有一个轻量级、高性能的 JSON 序列化库可以用，虽然 Jackson 和 Gson 都可以达成目的，但他们认为都太重了。\n所以这已经不是技术问题了，而是哲学问题。可能这些“库”作者压根不能理解我们应用开发者的痛点吧。\n","date":"2021-01-21T17:53:05+08:00","permalink":"https://techinsight.pages.dev/p/%E5%AE%9E%E7%8E%B0-tojson-%E6%96%B9%E6%B3%95/","title":"实现 toJson() 方法"},{"content":"我们知道 Java1.8 推荐使用 LocalDate和LocalDateTime类来处理日期和时间，但之前的版本是用GregorianCalendar。之前一直以为只是易用性上的差别，没想到还有一个神坑。\n事情经过 今天发现应用崩了，查日志发现是接收到了一个Sat Jan 01 00:00:00 +0800 0001这样的时间，当我用\n1 LocalDateTime.parse(\u0026#39;Sat Jan 01 00:00:00 +0800 0001\u0026#39;, DateTimeFormatter.ofPattern(\u0026#34;EEE MMM dd HH:mm:ss Z yyyy\u0026#34;)); 来解析的时候报了这样一个错\n1 2 3 4 java.time.format.DateTimeParseException: Text \u0026#39;Sat Jan 01 00:00:00 +0800 0001\u0026#39; could not be parsed: Conflict found: Field DayOfWeek 1 differs from DayOfWeek 6 derived from 0001-01-01 at java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920) ~[?:1.8.0_231] at java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1855) ~[?:1.8.0_231] at java.time.LocalDateTime.parse(LocalDateTime.java:492) ~[?:1.8.0_231] 看起来像是星期一和星期六有冲突，然后我就去百度公元0001年 1 月 1 日到底是星期几，得到的答案有的说是星期一，有的说是星期六，当然是找不到这样一个日历的，网上能找到的日历最多也就到 1900 年，这就勾起了我的兴趣，于是在想这个日期肯定不是人为填写的，那么就只能是因为生成这个时间的方法和我现在用的解析这个时间的方法有出入。业务方很早之前就开始用 Java 了，而我们是最近刚开始用，所以他们即便可能现在用的是 Java8，里面的很多写法应该还是保留了更早的方式，所以我就验证了一下这个日期\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package fun.happyhacker.springbootdemo; import java.time.LocalDate; import java.time.Month; import java.util.Calendar; import java.util.Date; import java.util.GregorianCalendar; public class DateTest { public static void main(String[] args) { Date birthDay = new GregorianCalendar(1, Calendar.JANUARY, 1).getTime(); System.out.println(birthDay); LocalDate birthDay1 = LocalDate.of(1, Month.JANUARY, 1); System.out.println(birthDay1.getDayOfWeek()); } } 执行一下，神奇的事情发生了\n1 2 Sat Jan 01 00:00:00 CST 1 MONDAY 也就是说在GregorianCalendar认为这天是星期六，而LocalDateTime认为这天是星期一。\n总结 从这件事儿得出的结论就是，从 JDK7 升级到 JDK8 的过程中，不光要注意什么语法的问题，说不定还会出现这种历史遗留问题。虽然这个日期比较特殊，但保不齐还有其他的特殊情况。\n","date":"2021-01-21T16:33:43+08:00","permalink":"https://techinsight.pages.dev/p/java%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%80%E4%B8%AA%E5%9D%91/","title":"Java日期函数的一个坑"},{"content":"Oracle JDK 现在收费了，macOS 上安装个 JDK 还挺麻烦。\nTL；DR 简单来讲，可以直接运行brew search openjdk\n这样搜索出来的是 Oracle 发布的 OpenJDK。其中openjdk就是最新版本的，带@的就是指定版本的，其中 8 和 11 是 LTS 版本，所以可以拥有姓名，至于其他的短期版本，这里就干脆也没有了。\nAdoptOpenJDK 这个名字有点长，其实是 Eclipse 基金会在维护的发行版，它和 Oracle OpenJDK 的关系有点类似于 MIUI 和 Andriod AOSP 的关系，功能上应该是一样的，不过添加了一些特色的功能，如图所示\n它提供了不同的垃圾收集器和所有的版本号，社区应该也比较流行吧（好吧，其实更多人还是会去下载 Oracle JDK，只是它的免费的 JDK1.8 永远的停留在了8u231）。\n参考这个https://github.com/AdoptOpenJDK/homebrew-openjdk\n强烈推荐使用这个脚本\n1 2 3 4 5 jdk() { version=$1 export JAVA_HOME=$(/usr/libexec/java_home -v\u0026#34;$version\u0026#34;); java -version } 可以让你在不同版本的 JDK 中自由切换。\n","date":"2021-01-14T16:28:04+08:00","permalink":"https://techinsight.pages.dev/p/%E9%80%9A%E8%BF%87-homebrew-%E5%AE%89%E8%A3%85-jdk/","title":"通过 homebrew 安装 JDK"},{"content":"自从知道了Optional的用法，我现在基本上能不用if/else就不用了，但现在发现这东西还是有一些局限性。\n例子使用的两个类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package fun.happyhacker.optional; public class Person { private int age; private String name; private Book book; public int getAge() { return age; } public void setAge(int age) { this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Book getBook() { return book; } public void setBook(Book book) { this.book = book; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package fun.happyhacker.optional; public class Book { private String author; public String getAuthor() { return author; } public void setAuthor(String author) { this.author = author; } public String getIsbn() { return isbn; } public void setIsbn(String isbn) { this.isbn = isbn; } private String isbn; } 正常的使用场景 1 2 3 4 5 6 Person person = new Person(); person.setAge(20); person.setName(\u0026#34;Frost\u0026#34;); Book book = Optional.ofNullable(person.getBook()).orElse(new Book()); System.out.println(Optional.ofNullable(book.getAuthor()).orElse(\u0026#34;anonymous\u0026#34;)); 很明显这会输出anonymous，但也很明显它的适用场景太局限。\n不能支持的场景 在上面的例子中，我需要调用两次Optional.ofNullable方法才能完成最终的判断，那如果我想这么用呢？\n1 2 3 4 5 Person person = new Person(); person.setAge(20); person.setName(\u0026#34;Frost\u0026#34;); String author = Optional.ofNullable(person.getBook().getAuthor()).orElse(\u0026#34;anonymous\u0026#34;); System.out.println(author); 这时就会报错了\n1 2 Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException at fun.happyhacker.optional.OptionalTest.main(OptionalTest.java:15) 也就是说Optional.ofNullable的参数中的空值还是不能调别的方法，如果调了还是会抛出异常。\n如果只能这么用真就没什么意思了。\n查看源码发现也就真的这样了\n调用顺序就是1、2、3，其中没有对任何可能发生的NPE做捕获。\n","date":"2020-12-23T22:27:03+08:00","permalink":"https://techinsight.pages.dev/p/java-optional-api-examples/","title":"Java Optional Api Examples"},{"content":"macOS上用Vim觉得很卡顿，不流畅，终于找到原因了。\n有两个相关设置\n这两个把Key Repeat设置到最快，把Delay Until Repeat设置到最短，但实际上这么设置之后还是不够，再小就不能通过配置页面设置了，只能通过命令行设置了。我觉得合适的配置是这样的\n1 2 defaults write -g InitialKeyRepeat -int 10 defaults write -g KeyRepeat -int 1 这样之后再操作就明显流畅多了。\n","date":"2020-11-21T22:48:25+08:00","permalink":"https://techinsight.pages.dev/p/macos%E8%AE%BE%E7%BD%AEkey-repeat%E5%92%8Cdelay-until-repeat/","title":"macOS设置Key Repeat和Delay Until Repeat"},{"content":"Java日志真是一个过于复杂的问题了，花了大量的时间在这个本不应该花时间的地方。\n开篇 据说上古时期Java标准库内并没有日志类，所以社区就搞出来了log4j，后来Apache想着把log4j合并到JDK中去，但这Java官方肯定不想让别人制定标准而自己做别人标准的遵守者，所以就搞了java.util.logging，后来就越发混乱了，Apache就又搞了个Commons Logging，这应该就是最早的“日志门面”了，它和后来的slf4j解决的问题是一样的。\n想象一下这个场景，你的项目依赖了两个包，其中一个依赖log4j，另外一个依赖jul，但很显然你是想把项目中所有日志放在一起管理的，如果没有一个统一的框架去管理，是很难维护一个大型项目的。\n问题初现 那么问题来了，历史原因，别人的包就是不想换日志框架，那不能改变别人就只能改变自己了，所以我们有2个选择：\n把这两个实现选一个作为真正的实现，通过某种方式将另一个日志框架的调用转发到这个真正的实现上 这两个我谁都不用，我要用第三个，所有第三方的库写的日志都转发到我自己用的这个日志框架上 这就是所谓的bridge了，具体来说如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package fun.happyhacker; import java.util.logging.Logger; public class JULTest { private static final Logger LOG = Logger.getLogger(JULTest.class.getCanonicalName()); public static void main(String[] args) { log(); } public static void log() { LOG.info(\u0026#34;hello log from JUL\u0026#34;); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package fun.happyhacker; import org.apache.log4j.Logger; public class Log4jTest { private static final Logger LOG = Logger.getLogger(Log4jTest.class); public static void main(String[] args) { log(); } public static void log() { LOG.info(\u0026#34;hello log from log4j\u0026#34;); } } 神奇的桥接 据说jul的性能比较差，我不想用，那么就需要把所有对它的调用转发到log4j上，但实际上没有所谓的jul-to-log4j ，但我们可以用jul-to-slf4j，然后再把slf4j的实现绑定到log4j，问题开始变得复杂了。简单画一下\n就现在的例子来说，加上\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jul-to-slf4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-log4j12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 这些依赖再执行JULTest类，发现还是一样的结果。。。。这是怎么回事儿呢。。。原来它不能自动桥接，还需要做一些改动。这里我们是将JULTest作为依赖使用的，也就是假设我们不能改变它的源码，所以只能改我们自己的应用。我们的应用原本是这样的\n1 2 3 4 5 6 7 8 9 10 11 12 package fun.happyhacker; import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class App { private static final Logger LOG = LoggerFactory.getLogger(App.class); public static void main(String[] args) { JULTest.log(); } } 执行的结果如下\n还需要改成这样\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package fun.happyhacker; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.slf4j.bridge.SLF4JBridgeHandler; import java.util.logging.LogManager; public class App { private static final Logger LOG = LoggerFactory.getLogger(App.class); static { LogManager.getLogManager().reset(); SLF4JBridgeHandler.install(); } public static void main(String[] args) { JULTest.log(); } } 官方文档专门强调了一点，jul到slf4j是有很大的性能损失的（本来jul的性能就差，这么一搞就更差了）因为其他的日志框架到slf4j的桥接都是通过重新实现相应的接口来完成的，但因为双亲委派机制的限制，我们是无法重新实现java.util.logging中的接口的，所以实际它是把jul中的LogRecord替换成了slf4j中的等价对象，这层转换在日志这个本不应该消耗太多资源的场景下就消耗了太多的资源。\n好消息是并没有太多的应用使用jul。所以啊，即便你是官方的实现，也不一定那么受欢迎。\n延伸 多数情况可能是要用log4j-over-slf4j来将对log4j的直接调用转调到slf4j-api上，然后再通过其他的日志框架，比如logback来写日志。这里可以简单的追踪以下调用过程。\n1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker; import org.apache.log4j.Logger; public class Log4jTest { private static final Logger LOG = Logger.getLogger(Log4jTest.class); public static void main(String[] args) { LOG.info(\u0026#34;hello from log4j\u0026#34;); } } 配合相应的log4j.properties就能有相应的输出。那么如果我不想改这块代码，而想让它直接通过logback输出，就需要引入另外两个依赖\n1 2 3 4 5 6 7 8 9 10 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;ch.qos.logback\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;logback-classic\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-over-slf4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.25\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 这时在上述代码的第9行打个断点进入debug模式，走到第一步Category.java，就会发现上面会有一个提示，如图所示\n但当你真的选了第二个源码文件的时候它又会有另一个提示\n这说明它在执行的时候还是用的log4j包中的class文件，而不是log4j-over-slf4j。这时候把依赖中的log4j去掉（实际项目中应该是exclude掉）。再次执行就会发现没有这个提示了，取而代之的是代码的调用直接进入了log4j-over-slf4j中的，打开这个包的源码你就会发现它的包结构和log4j是一致的\n所以其实就是用log4j-over-slf4j中的类“偷偷”的替换了log4j中的类，其实已经变成了基于slf4j中的实现。\n更实际的情况 上面讲述的这种情况很少见，但更常见的是什么呢？其中一个就是配置Spymemcached的日志级别。更复杂的场景在下面。\nSpringBoot可能是现在最常见的应用类型了，我们知道它默认的日志框架是logback。 而我这里还有一个基于Flink的应用，二者要复用一部分代码，也就是说需要在Flink应用里创建一个Spring容器。Spring用的是logback，Flink用的是log4j，又需要在二者中选择一个了。\n但这个问题其实又没有那么复杂，因为他们其实都是通过slf4j写日志的，也就是说我们没有log4j-to-slf4j或者logback-to-slf4j这种转换，而只需要选择一个实现即可。\n正常使用logback的应用会引入logback-classic包，它提供了slf4j-api和log4j-over-slf4j等依赖，所以当logback-classic和slf4j-log4j12同时存在时就会出现下面的报错\n也就是说现在classpath中存在了两个LoggerFactoryBinder的实现，我们要做的就是把它们中的屏蔽一个。比如如果你不想用logback，只需要在spring相关依赖中添加\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.1.RELEASE\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;ch.qos.logback\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;logback-classic\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 相应的，如果要用logback而不是log4j，就把slf4j-log4j12排除即可。\n一点提示 前面我们说到了，如果想把直接调用log4j的请求转发到slf4j-api上，再根据实际情况决定最终底层用哪个日志框架。而slf4j-log4j12就是那个决定让slf4j使用log4j写日志的包，那么如果这两个包同时出现又会怎么样呢？\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-log4j12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-over-slf4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 再执行App，就会像下面这样崩溃了\n简单说就是出现了死循环，要出现栈溢出错误了。道理很简单，这里不再赘述了。\n总结 这篇基本就把Java日志里出现的几种情况解释清楚了。下面会开一篇具体说说这几种日志框架各自的配置方式。\n","date":"2020-11-21T15:21:05+08:00","permalink":"https://techinsight.pages.dev/p/java%E6%97%A5%E5%BF%97%E6%BC%94%E5%8C%96%E6%A2%B3%E7%90%86/","title":"Java日志演化梳理"},{"content":"用这个net.spy.memcached最恶心的事情就是它的日志了，不管三七二十一先打印一组红色的INFO级别日志。\n之前也没有研究这个原因，最近在总结日志相关的坑，就把这里详细看了一下。\n观察现象 首先是这个日志是红色的，INFO级别，而且在log4j.properties中添加\n1 2 log4j.logger.net.spy.memcached=ERROR, console log4j.logger.addivitity.*=false 是不起作用的。\n分析原因 由于之前对log4j的配置文件也不太熟悉，所以一直想着是自己的配置文件没有写对导致的，而忽略了其他原因。昨天弄明白了如果要改变某个指定的包的日志配置就是这样做，所以就确定了配置文件没有问题。那么原因就只可能是这个包记录日志根本就没有用log4j。那它用的是啥呢？不想翻文档就只能debug了。\n首先找到记日志的地方，打个断点，执行程序，一步一步往下跟，找到这里\n很明显具体拿到的Logger类就是在这里决定的了，这段简单来说就是如果系统没有设置net.spy.log.LoggerImpl属性，就用默认的DefaultLogger，实际上跟到这里确实也发现就是没有设置这个属性，从而className拿到的是个空，所以也就没有log4j什么事儿了。\n改造方案 既然知道了问题的根源，那么我们就设置一下这个属性就行了，它是从System.getProperty()方法获取的，那么我们就从System.setProperty()方法设置它。那么要设置成什么呢？打开这个net.spy.log目录，就会发现它提供了几个默认的实现\n很明显我们要找的就是net.spy.memcached.compat.log.Log4jLogger，所以只需要在程序入口加上这行\n1 System.setProperty(\u0026#34;net.spy.log.LoggerImpl\u0026#34;, \u0026#34;net.spy.memcached.compat.log.Log4JLogger\u0026#34;); 这个很可能是slf4j这种日志门面出现之前的一种自己实现的方案，而slf4j-api/log4j-api就是解决这个问题的了。\n即可。加上之后再运行程序就会发现颜色已经和其他的日志一样了。\n总结 只要把问题回归到我们会的问题上，后面的问题就很容易解决了，前面已经配置了log4j.properties，所以当然也就可以方便的控制它的日志级别了。\n","date":"2020-11-21T14:14:29+08:00","permalink":"https://techinsight.pages.dev/p/%E9%85%8D%E7%BD%AEspymemcached%E7%9A%84%E6%97%A5%E5%BF%97%E7%BA%A7%E5%88%AB/","title":"配置Spymemcached的日志级别"},{"content":"用默认的archetype太烦了，每次都要改很多东西，不如自己创建一个。\n为什么要自己定义archetype 当然是因为懒。我一般是用maven-archetype-quickstart创建项目，但每次创建完之后还要改一通，比如JDK版本号、maven插件等等，这些东西本身其实不应该花太多时间去记的。所以自己定义一个就好了。\n定义archetype的步骤 生成模板 从maven-archetype-quickstart创建一个正常的maven项目 这个maven项目是什么不重要，重要的是它会帮你生成一个标准的pom.xml文件。\n修改pom.xml文件 比如你几乎所有的项目都需要依赖log4j2这套日志套件，那么你就需要这两个依赖\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.14.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.14.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 那么你就可以在这个生成的pom.xml文件中加上这两个依赖。\n执行mvn archetype:create-from-project 这会在target目录中生成一个新的maven项目，进入这个项目，执行mvn clean install，你就把新创建的archetype安装到本地了。\n使用新创建的模板 执行mvn archetype:generate -DarchetypeCatalog=local，会产生一个交互\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ➜ my-project mvn archetype:generate -DarchetypeCatalog=local [INFO] Scanning for projects... [INFO] [INFO] ---------------------\u0026lt; fun.happyhacker:my-project \u0026gt;--------------------- [INFO] Building my-project 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] \u0026gt;\u0026gt;\u0026gt; maven-archetype-plugin:3.1.2:generate (default-cli) \u0026gt; generate-sources @ my-project \u0026gt;\u0026gt;\u0026gt; [INFO] [INFO] \u0026lt;\u0026lt;\u0026lt; maven-archetype-plugin:3.1.2:generate (default-cli) \u0026lt; generate-sources @ my-project \u0026lt;\u0026lt;\u0026lt; [INFO] [INFO] [INFO] --- maven-archetype-plugin:3.1.2:generate (default-cli) @ my-project --- [INFO] Generating project in Interactive mode [INFO] No archetype defined. Using maven-archetype-quickstart (org.apache.maven.archetypes:maven-archetype-quickstart:1.0) Choose archetype: 1: local -\u0026gt; org.apache.flink:flink-quickstart-java (flink-quickstart-java) 2: local -\u0026gt; org.apache.maven.archetypes:maven-archetype-quickstart (quickstart) 3: local -\u0026gt; fun.happyhacker:my-quickstart-1.8-archetype (my-quickstart-1.8) 4: local -\u0026gt; fun.happyhacker:archetype-template-archetype (archetype-template) Choose a number or apply filter (format: [groupId:]artifactId, case sensitive contains): 2: 这时就可以在其中找到刚刚创建的archetype了，在这里就是第4个，所以在交互界面填4，就可以继续选择域名和项目名称，生成一个新的maven项目，在生成的项目中你就会发现你刚刚在第2步中添加的两个依赖已经包含在里面了。\n同样的道理，也可以在pom.xml中添加相关的maven插件，以使从模板创建的项目更符合平时的需求。\n","date":"2020-11-18T23:03:17+08:00","permalink":"https://techinsight.pages.dev/p/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84archetype/","title":"创建一个自定义的Archetype"},{"content":"刚看完《Java高并发编程详解——多线程与架构设计》的第10章，实验一下类似Tomcat的热加载的方法。\n这一章的主要内容如下图\n背景 讲到基于类的加载和卸载来实现功能热更新时，我就想到了Tomcat可以检测webapps中的war包的变化来重新加载新的应用，所以就通过本章介绍的内容扩展一下，自己实现一个热加载功能。\n需求 当指定目录下的class文件发生变化时，系统能及时感知并重新加载。\n实现 指定一个存放class文件的目录，如/tmp/classloader/ 在其中放一个class文件，如Child.class，开始时只有一个walk()方法 系统每隔1秒中检测一次该目录中文件是否更新，如果更新了则重新加载 看是否可以通过反射拿到新类的方法列表 源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 package fun.happyhacker; import java.io.ByteArrayOutputStream; import java.io.IOException; import java.nio.file.Files; import java.nio.file.Path; import java.nio.file.Paths; public class MyClassLoader extends ClassLoader { private static final Path CLASS_DIR = Paths.get(\u0026#34;/tmp/classloader/\u0026#34;); private final Path classDir; public MyClassLoader() { super(); this.classDir = CLASS_DIR; } public MyClassLoader(String classDir) { this.classDir = Paths.get(classDir); } public MyClassLoader(ClassLoader parent) { super(parent); this.classDir = CLASS_DIR; } public MyClassLoader(ClassLoader parent, String classDir) { super(parent); this.classDir = Paths.get(classDir); } @Override protected Class\u0026lt;?\u0026gt; findClass(String name) throws ClassNotFoundException { byte[] classBytes = this.readClassBytes(name); if (classBytes.length == 0) { throw new ClassNotFoundException(\u0026#34;Can not load the class \u0026#34; + name); } return this.defineClass(name, classBytes, 0, classBytes.length); } private byte[] readClassBytes(String name) throws ClassNotFoundException { String classPath = name.replace(\u0026#34;.\u0026#34;, \u0026#34;/\u0026#34;); Path classFullPath = classDir.resolve(Paths.get(classPath + \u0026#34;.class\u0026#34;)); if (!classFullPath.toFile().exists()) { throw new ClassNotFoundException(\u0026#34;The class \u0026#34; + name + \u0026#34; not found\u0026#34;); } try (ByteArrayOutputStream baos = new ByteArrayOutputStream()) { Files.copy(classFullPath, baos); return baos.toByteArray(); } catch (IOException e) { throw new ClassNotFoundException(\u0026#34;load the class \u0026#34; + name + \u0026#34; occur error.\u0026#34;, e); } } @Override public String toString() { return \u0026#34;MyClassLoader\u0026#34;; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 package fun.happyhacker; import java.lang.reflect.InvocationTargetException; import java.lang.reflect.Method; import java.nio.file.Path; import java.nio.file.Paths; import java.util.concurrent.TimeUnit; public class Daemon { private static final String CLASS_PATH = \u0026#34;/tmp/classloader/\u0026#34;; private static final String CLASS_NAME = \u0026#34;a.b.c.Child\u0026#34;; private static final int CHECK_INTERVAL = 5; private long lastModified; private ClassLoader classLoader; private Class\u0026lt;?\u0026gt; klass; public static void main(String[] args) throws InterruptedException, ClassNotFoundException, NoSuchMethodException, InvocationTargetException, InstantiationException, IllegalAccessException { Daemon daemon = new Daemon(); daemon.listen(); } private void run() throws ClassNotFoundException, IllegalAccessException, InstantiationException, NoSuchMethodException, InvocationTargetException { if (classLoader == null) { classLoader = new MyClassLoader(); } if (klass == null) { klass = classLoader.loadClass(CLASS_NAME); } Class\u0026lt;?\u0026gt; child = klass; System.out.println(child.getClassLoader()); Object instance = child.newInstance(); System.out.println(instance); Method[] methods = child.getDeclaredMethods(); for (Method method : methods) { Method m = child.getMethod(method.getName()); m.invoke(instance); } } private void listen() throws InterruptedException, ClassNotFoundException, NoSuchMethodException, InvocationTargetException, InstantiationException, IllegalAccessException { while (true) { Path classPath = Paths.get(CLASS_PATH); Path classFullPath = classPath.resolve(Paths.get(CLASS_NAME.replace(\u0026#34;.\u0026#34;, \u0026#34;/\u0026#34;) + \u0026#34;.class\u0026#34;)); System.out.println(\u0026#34;class path \u0026#34; + classFullPath.toFile().getAbsolutePath()); long newLastModified = classFullPath.toFile().lastModified(); System.out.println(\u0026#34;exists \u0026#34; + classFullPath.toFile().exists()); System.out.println(\u0026#34;new last modified: \u0026#34; + newLastModified); if (newLastModified \u0026gt; lastModified) { reload(); run(); } lastModified = newLastModified; TimeUnit.SECONDS.sleep(CHECK_INTERVAL); } } private void reload() { classLoader = null; klass = null; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package a.b.c; public class Child { public void walk() { System.out.println(\u0026#34;I can walk\u0026#34;); } public void speak() { System.out.println(\u0026#34;I can talk\u0026#34;); } public void write() { System.out.println(\u0026#34;I can write\u0026#34;); } } 在/tmp/classloader/a/b/c中修改Child.java，修改之后执行javac Child.java会引起Child.class文件的变化，从而触发系统自动重新加载新的class文件，执行新的方法。\n总结 对于Tomcat会更复杂一些，但也就是把加载一个单独的class文件升级成加载一个war包，原理是一样的。\n不过我不太理解的是，为什么Tomcat不能实现热更新，也就是为什么不能像Nginx那样有一段时间是新老服务共存，等已经连接到老的服务上的请求完成之后再停掉老服务呢？这个答案只能从Tomcat的源码中寻找了。\n","date":"2020-10-27T22:04:17+08:00","permalink":"https://techinsight.pages.dev/p/jvm%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/","title":"JVM类加载器"},{"content":"Springboot的这个profiles的问题真是让人头疼。\n这个问题在小版本之间来瞎改，又没有明确的说明，不知道浪费了多少人的时间。\n首先明确一点，通过\n1 export SPRING_PROFILES_ACTIVE=prod,web 这种方式从始至终都是可行的。而改变的是\n1 mvn spring-boot:run -Dspring.profiles.active=prod,web 这种方式。亲测在springboot 2.3.1已经完全不起作用了。所以保险的方法就是前面提到的第一种。可以这么做\n1 export SPRING_PROFILES_ACTIVE=prod,web \u0026amp;\u0026amp; mvn spring-boot:run ","date":"2020-10-22T16:32:44+08:00","permalink":"https://techinsight.pages.dev/p/springboot-profiles-active/","title":"Springboot Profiles Active"},{"content":"更新完黑苹果之后发现进BIOS的时候多了一个选项，看起来很奇怪。\n其实是因为在安装Windows或者Windows进行系统更新时，会覆盖/EFI/BOOT/BOOTx64.efi文件，这个配置就是为了保护这个文件不被修改的，具体的原因我没有研究，因为暂时还没有安装Windows。（其实是因为安装不上，在Mac上创建的WindowsInstaller总是无法启动，不是刚需也就没有再继续研究了）\n多了个OpenCore，经过在Reddit上发帖求助，发现原因可能是更新到0.5.9的时候复制了一个配置来 Misc -\u0026gt; Security -\u0026gt; BootProtect，现在的配置是Bootstrap，改成none即可，但如果安装了Windows系统，每当windows系统更新时就会破坏OpenCore的启动顺序。\n所以这其实是OpenCore的保护机制，也就不难理解它是一个【启动安全】选项的原因了。不过我暂时没有安装Windows，所以也用不到这个选项。不过明白了，也就不再纠结了，暂时就不改它了，免得将来装了Windows但忘了这个事儿，净是给自己挖坑。\n","date":"2020-06-14T15:49:21+08:00","permalink":"https://techinsight.pages.dev/p/%E9%BB%91%E8%8B%B9%E6%9E%9Cbootmenu/","title":"黑苹果bootmenu"},{"content":"装黑苹果的时候是10.15.3，后来经历了两次官方的更新，但我又不太清楚需要的更新过程，现在更新成功之后记录一下。\n1. 更新EFI 下载最新的OpenCorePkg，参考这里把主要的几个文件更新一下，把EFI更新到系统硬盘之后重启看看能不能正常启动，能正常启动后再进行第二步。\n我首次安装的时候是OpenCore0.5.6，更新到0.5.7的时候有一个不兼容的配置，如下图所示\n而正好重命名的这个是必须项，所以就需要自己把FwRuntimeServices.efi删除，把新的OpenRuntime.efi放进来，打开ProperTree，重新加载config.plist\n2. 更新macOS 这就是上述这里没有说清楚的地方，因为实在是没什么好说的。就按正常的系统更新流程直接点更新即可！\n3. 更新成功 ","date":"2020-06-14T10:12:22+08:00","permalink":"https://techinsight.pages.dev/p/%E9%BB%91%E8%8B%B9%E6%9E%9C%E6%9B%B4%E6%96%B0%E5%8D%87%E7%BA%A7/","title":"黑苹果更新升级"},{"content":"作为黑苹果用户，不知道直接更新系统会发生什么不可预知的问题，所以还是尽量避免升级。\n设置方法 1. 在系统偏好设置中关闭自动更新 2. 在终端执行以下命令 1 2 3 sudo softwareupdate --ignore \u0026#34;macOS Catalina\u0026#34; defaults write com.apple.systempreferences AttentionPrefBundleIDs 0 killall Dock 3. 补充 最近的不知道哪次更新又带来了一个问题，执行上面的命令的时候会报\n1 2 3 4 5 6 7 8 9 10 11 Password: Ignored updates: ( ) Software Update can only ignore updates that are eligible for installation. If the label provided to ignore is not in the above list, it is not eligible to be ignored. Ignoring software updates is deprecated. The ability to ignore individual updates will be removed in a future release of macOS. 也就是说苹果以后要把这个选项去掉了，可以参考一下这个文章Apple’s has brought back the nagging — you can no longer ignore major macOS updates，看起来不光是我反感这个事儿，全世界都觉得苹果一点都不在乎用户的感受啊，和微信有点像。\n总结 macOS和iPhone的升级速度快（指最新版本的更新率高）的原因就是这么不停提醒吧，太讨厌了。这种方法亲测有效。\n","date":"2020-06-10T10:52:37+08:00","permalink":"https://techinsight.pages.dev/p/%E5%BD%BB%E5%BA%95%E5%85%B3%E9%97%ADmacos%E7%B3%BB%E7%BB%9F%E5%8D%87%E7%BA%A7%E6%8F%90%E7%A4%BA/","title":"彻底关闭macOS系统升级提示"},{"content":"在macOS上开发Springboot应用时发现应用启动前总是等待5秒钟，体现在应用启动的很慢。\n具体的提示信息因为改完之后找不到了，就是一句提示，说使用了5000 milliseconds，建议macOS用户修改/etc/hosts。但具体改什么就没有提到。\n其实是因为应用启动时会查询域名${hostname}，而macOS上默认是没有配置这个域名的，所以就要等到超时（5秒）才能继续了。知道了问题的原因也就清楚如何解决了，在/etc/hosts中添加以下两行\n1 2 127.0.0.1 ${hostname} ::1 ${hostname} 其中${hostname}要替换成你机器的hostname，要得到它只需要执行hostname命令即可，一般是一个以.local结尾的字符串。\n","date":"2020-05-21T10:41:26+08:00","permalink":"https://techinsight.pages.dev/p/%E9%81%BF%E5%85%8Dspringboot%E5%BA%94%E7%94%A8%E5%90%AF%E5%8A%A8%E5%89%8D5%E7%A7%92%E7%9A%84%E7%AD%89%E5%BE%85/","title":"避免Springboot应用启动前5秒的等待"},{"content":"jar包应该是java应用使用最多的分发形式了，jar包中包含什么东西，怎么创建和执行它呢？\njar包是什么 Jar包全称Java Archive File，是以zip格式打包的一个压缩包，和普通的压缩包最本质的区别是——后缀不一样，一个是.jar，一个是.zip。但区分jar包和其他zip包的本质区别是jar包包含一个META-INF/MANIFEST.MF文件，这个清单文件是包含了以下内容\njar包的版本 创建人 Class-Path类搜索路径 Main-Class属性（表示Main方法的入口） 为什么要用jar包 通常我们写一个HelloWord类之后会做以下操作\n1 2 3 4 5 6 // 保存为HelloWorld.java public class HelloWorld { public static void main(String[] args) { System.out.println(\u0026#34;Hello World!\u0026#34;); } } 1 2 3 4 javac HelloWorld.java # 会生成一个HelloWorld.class文件 java HelloWorld # output Hello World! 这里HelloWorld.class其实才是我们需要的东西，也就是Java所谓的“字节码”，Java的最大卖点Write Once, Run Anywhere的特点就来自这里了。需要注意的是，所谓跨平台，并不是指Java（jdk）本身跨平台，而是由jdk编译而来的class文件跨平台。所以也就需要针对不同平台的jdk了。\n当我们的应用只有一个文件时当然可以这样发布，但这肯定是不可能的。如果应用有其他的依赖，用这种原始的方式将会很难维护，因此，jar包就应运而生了。\n下面简单写一个带有依赖的例子，入口类App，小狗类Dog。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package entity; public class Dog { private String name; private int age; public Dog(String name, int age) { this.name = name; this.age = age; } public String getName() { return name; } public int getAge() { return age; } } 1 2 3 4 5 6 7 8 import entity.Dog; public class App { public static void main(String[] args) { Dog aDog = new Dog(\u0026#34;Bailey\u0026#34;, 11); System.out.println(\u0026#34;I am \u0026#34; + aDog.getName() + \u0026#34; and I am \u0026#34; + aDog.getAge() + \u0026#34; years old\u0026#34;); } } 目录结构如下：\n1 2 3 4 . ├── App.java └── entity └── Dog.java 如果没有jar包，就需要执行如下命令\n1 2 3 4 5 6 7 8 9 10 11 12 ➜ demo javac App.java entity/Dog.java ➜ demo tree . ├── App.class ├── App.java └── entity ├── Dog.class └── Dog.java 1 directory, 4 files ➜ demo java App I am Bailey and I am 11 years old 这时候源码(.java)已经不需要了，删了它们一样可以运行。为了更方便的分发，可以把这些.class文件打包成jar包，并指定运行的入口类\n1 2 3 4 5 6 7 8 9 ➜ demo jar cvfe App.jar App entity/*.class App.class added manifest adding: entity/Dog.class(in = 442) (out= 286)(deflated 35%) adding: App.class(in = 784) (out= 476)(deflated 39%) ➜ demo jar -tf App.jar META-INF/ META-INF/MANIFEST.MF entity/Dog.class App.class 这时候执行java -jar App.jar，输出如下\n1 2 ➜ demo java -jar App.jar I am Bailey and I am 11 years old 这时候把这个压缩包发给别人，别人就可以直接这样执行了。\n添加和修改清单属性 如果第一次打包的时候没有添加MANIFEST文件，jar会自动添加一个META-INF/MANIFEST.MF文件，但其中只包含Manifest-Version和Created-By两个属性，在执行的时候还要在命令行传入入口才可以执行。所以这时如果要给MANIFEST文件添加属性，可以这么做。\n创建一个新的清单文件，比如MANIFEST-ADDITION.MF 将需要添加的属性假如新的清单文件 执行jar ufvm App.jar MANIFEST-ADDITION.MF 1 2 Main-Class: App Author: happyhakcer 然后通过vim打开这个jar包就可以看到其中的清单文件已经更新了。\n另外，如果是打包前就已经手动写好了完整的清单文件，也可以通过jar cfvm的方式直接把清单文件打包进jar包。\n需要注意一点，在执行jar vcfme这种命令时，f m e这三个选项的顺序也决定了后面传入的参数表示的含义，f表示目标文件（jar包的名称），m表示清单文件，e(entry)则表示入口点。\n比如可以是\n1 jar vcfme app.jar MANIFEST.MF App App.class entity/*.class 这里是为了说明问题，如果MANIFEST.MF中已经指定了Main-Class属性，则在执行上面的命令时会因为又通过e选项指定了入口点而执行失败。\njar包的官方标准 现在我们已经有了一个标准的jar包了，下面打开它看看这个清单文件中究竟包含了什么信息\n1 2 3 Manifest-Version: 1.0 Created-By: 1.8.0_252 (AdoptOpenJDK) Main-Class: App 由于entity目录位于App.class所在目录的子目录中，所以无需指定Class-Path\njar包的其他标准 本节其实是为了说明标准的jar包和Springboot打包的FAT JAR的区别。\n通常一个jar包只包含了应用代码（区别于依赖），但Springboot的jar包则动辄几百MB，其实就是因为它把所有的依赖全部都打到jar包里了。一个典型的Springboot应用的jar包的清单文件内容如下\n可以看到，下面有波浪线的其实就是标准的清单文件中不包含的部分。而Springboot能从jar包启动，核心就在于Main-Class配置。关于org.springframework.boot.loader.JarLauncher的原理，详见Springboot启动\njar包和war包的区别 最本质的区别就是war包是一个典型的web包，所谓典型也就是早年间把接口和页面等其他静态资源打包到一起的包，tomcat就是用来运行war包的。由于现在war包用的不多了，这里不再赘述。\n总结 本文简单介绍了jar包的结构和一些简单的应用，主要帮助初学者理解一些概念。\n","date":"2020-05-18T22:18:51+08:00","permalink":"https://techinsight.pages.dev/p/%E8%81%8A%E8%81%8Ajar%E5%8C%85/","title":"聊聊jar包"},{"content":"火狐浏览器有个很奇怪的设定，在Windows下按住Ctrl（macOS下按住Cmd）加上鼠标滚轮会缩放网页。\n有些人可能觉得是个很有用的功能，但我觉得这个功能让我很烦恼，有时候有不自觉的放在Cmd上，鼠标滚动一下网页就变成了200%，原来是可以通过配置修改关闭这项功能的\n在地址栏输入about:config 搜索mousewheel.with_meta.action，把3改成0 如果是windows，则搜索mousewheel.with_control.action，同样把3改成0 改完立即生效，不需要重启浏览器 ","date":"2020-05-16T00:49:00+08:00","permalink":"https://techinsight.pages.dev/p/%E5%85%B3%E9%97%ADfirefox%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%AD%E4%BD%BF%E7%94%A8%E9%BC%A0%E6%A0%87%E7%BC%A9%E6%94%BE%E7%BD%91%E9%A1%B5%E7%9A%84%E5%8A%9F%E8%83%BD/","title":"关闭Firefox浏览器中使用鼠标缩放网页的功能"},{"content":"不知道为什么好像家里的网络对github很不友好，网页能打开，但通过curl访问就有问题，所以安装homebrew时如果使用官方提供的花里胡哨的curl或者wget的方式就安装不上。\n姑且说安装成功了吧，后面安装其他软件的时候又会各种慢。国内中科大(个人认为国内做的最好的开源镜像站点)和清华大学都提供了homebrew的镜像，但比较吊诡的是他们提供的都是post-installation，也就是前提是你已经安装好了brew。殊不知很多人（网络环境）这一步都过不了关了。\n所以我看了一下官方的安装脚本，参考了中科大的文档，把安装脚本中的github地址给改成中科大的，然后加上了brew-cask和brew-core，并把其中的brew update --force命令后面加了个-v，方便在安装的时候可以看到具体的进度。\n脚本正文直接贴出来，有想看github的在这里。\n脚本默认你使用的是zsh，有偏好使用bash的同学可以自行修改\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 #!/bin/bash set -u # First check if the OS is Linux. if [[ \u0026#34;$(uname)\u0026#34; = \u0026#34;Linux\u0026#34; ]]; then HOMEBREW_ON_LINUX=1 fi # On macOS, this script installs to /usr/local only. # On Linux, it installs to /home/linuxbrew/.linuxbrew if you have sudo access # and ~/.linuxbrew otherwise. # To install elsewhere (which is unsupported) # you can untar https://github.com/Homebrew/brew/tarball/master # anywhere you like. if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then HOMEBREW_PREFIX=\u0026#34;/usr/local\u0026#34; HOMEBREW_REPOSITORY=\u0026#34;/usr/local/Homebrew\u0026#34; HOMEBREW_CACHE=\u0026#34;${HOME}/Library/Caches/Homebrew\u0026#34; STAT=\u0026#34;stat -f\u0026#34; CHOWN=\u0026#34;/usr/sbin/chown\u0026#34; CHGRP=\u0026#34;/usr/bin/chgrp\u0026#34; GROUP=\u0026#34;admin\u0026#34; TOUCH=\u0026#34;/usr/bin/touch\u0026#34; else HOMEBREW_PREFIX_DEFAULT=\u0026#34;/home/linuxbrew/.linuxbrew\u0026#34; HOMEBREW_CACHE=\u0026#34;${HOME}/.cache/Homebrew\u0026#34; STAT=\u0026#34;stat --printf\u0026#34; CHOWN=\u0026#34;/bin/chown\u0026#34; CHGRP=\u0026#34;/bin/chgrp\u0026#34; GROUP=\u0026#34;$(id -gn)\u0026#34; TOUCH=\u0026#34;/bin/touch\u0026#34; fi BREW_REPO=\u0026#34;http://mirrors.ustc.edu.cn/brew.git\u0026#34; # TODO: bump version when new macOS is released MACOS_LATEST_SUPPORTED=\u0026#34;10.15\u0026#34; # TODO: bump version when new macOS is released MACOS_OLDEST_SUPPORTED=\u0026#34;10.13\u0026#34; # no analytics during installation export HOMEBREW_NO_ANALYTICS_THIS_RUN=1 export HOMEBREW_NO_ANALYTICS_MESSAGE_OUTPUT=1 # string formatters if [[ -t 1 ]]; then tty_escape() { printf \u0026#34;\\033[%sm\u0026#34; \u0026#34;$1\u0026#34;; } else tty_escape() { :; } fi tty_mkbold() { tty_escape \u0026#34;1;$1\u0026#34;; } tty_underline=\u0026#34;$(tty_escape \u0026#34;4;39\u0026#34;)\u0026#34; tty_blue=\u0026#34;$(tty_mkbold 34)\u0026#34; tty_red=\u0026#34;$(tty_mkbold 31)\u0026#34; tty_bold=\u0026#34;$(tty_mkbold 39)\u0026#34; tty_reset=\u0026#34;$(tty_escape 0)\u0026#34; have_sudo_access() { if [[ -z \u0026#34;${HAVE_SUDO_ACCESS-}\u0026#34; ]]; then /usr/bin/sudo -l mkdir \u0026amp;\u0026gt;/dev/null HAVE_SUDO_ACCESS=\u0026#34;$?\u0026#34; fi if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]] \u0026amp;\u0026amp; [[ \u0026#34;$HAVE_SUDO_ACCESS\u0026#34; -ne 0 ]]; then abort \u0026#34;Need sudo access on macOS!\u0026#34; fi return \u0026#34;$HAVE_SUDO_ACCESS\u0026#34; } shell_join() { local arg printf \u0026#34;%s\u0026#34; \u0026#34;$1\u0026#34; shift for arg in \u0026#34;$@\u0026#34;; do printf \u0026#34; \u0026#34; printf \u0026#34;%s\u0026#34; \u0026#34;${arg// /\\ }\u0026#34; done } chomp() { printf \u0026#34;%s\u0026#34; \u0026#34;${1/\u0026#34;$\u0026#39;\\n\u0026#39;\u0026#34;/}\u0026#34; } ohai() { printf \u0026#34;${tty_blue}==\u0026gt;${tty_bold} %s${tty_reset}\\n\u0026#34; \u0026#34;$(shell_join \u0026#34;$@\u0026#34;)\u0026#34; } warn() { printf \u0026#34;${tty_red}Warning${tty_reset}: %s\\n\u0026#34; \u0026#34;$(chomp \u0026#34;$1\u0026#34;)\u0026#34; } abort() { printf \u0026#34;%s\\n\u0026#34; \u0026#34;$1\u0026#34; exit 1 } execute() { if ! \u0026#34;$@\u0026#34;; then abort \u0026#34;$(printf \u0026#34;Failed during: %s\u0026#34; \u0026#34;$(shell_join \u0026#34;$@\u0026#34;)\u0026#34;)\u0026#34; fi } execute_sudo() { local -a args=(\u0026#34;$@\u0026#34;) if [[ -n \u0026#34;${SUDO_ASKPASS-}\u0026#34; ]]; then args=(\u0026#34;-A\u0026#34; \u0026#34;${args[@]}\u0026#34;) fi if have_sudo_access; then ohai \u0026#34;/usr/bin/sudo\u0026#34; \u0026#34;${args[@]}\u0026#34; execute \u0026#34;/usr/bin/sudo\u0026#34; \u0026#34;${args[@]}\u0026#34; else ohai \u0026#34;${args[@]}\u0026#34; execute \u0026#34;${args[@]}\u0026#34; fi } getc() { local save_state save_state=$(/bin/stty -g) /bin/stty raw -echo IFS= read -r -n 1 -d \u0026#39;\u0026#39; \u0026#34;$@\u0026#34; /bin/stty \u0026#34;$save_state\u0026#34; } wait_for_user() { local c echo echo \u0026#34;Press RETURN to continue or any other key to abort\u0026#34; getc c # we test for \\r and \\n because some stuff does \\r instead if ! [[ \u0026#34;$c\u0026#34; == $\u0026#39;\\r\u0026#39; || \u0026#34;$c\u0026#34; == $\u0026#39;\\n\u0026#39; ]]; then exit 1 fi } major_minor() { echo \u0026#34;${1%%.*}.$(x=\u0026#34;${1#*.}\u0026#34;; echo \u0026#34;${x%%.*}\u0026#34;)\u0026#34; } if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then macos_version=\u0026#34;$(major_minor \u0026#34;$(/usr/bin/sw_vers -productVersion)\u0026#34;)\u0026#34; fi version_gt() { [[ \u0026#34;${1%.*}\u0026#34; -gt \u0026#34;${2%.*}\u0026#34; ]] || [[ \u0026#34;${1%.*}\u0026#34; -eq \u0026#34;${2%.*}\u0026#34; \u0026amp;\u0026amp; \u0026#34;${1#*.}\u0026#34; -gt \u0026#34;${2#*.}\u0026#34; ]] } version_ge() { [[ \u0026#34;${1%.*}\u0026#34; -gt \u0026#34;${2%.*}\u0026#34; ]] || [[ \u0026#34;${1%.*}\u0026#34; -eq \u0026#34;${2%.*}\u0026#34; \u0026amp;\u0026amp; \u0026#34;${1#*.}\u0026#34; -ge \u0026#34;${2#*.}\u0026#34; ]] } version_lt() { [[ \u0026#34;${1%.*}\u0026#34; -lt \u0026#34;${2%.*}\u0026#34; ]] || [[ \u0026#34;${1%.*}\u0026#34; -eq \u0026#34;${2%.*}\u0026#34; \u0026amp;\u0026amp; \u0026#34;${1#*.}\u0026#34; -lt \u0026#34;${2#*.}\u0026#34; ]] } should_install_git() { if [[ $(command -v git) ]]; then return 1 fi } should_install_command_line_tools() { if [[ -n \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then return 1 fi if version_gt \u0026#34;$macos_version\u0026#34; \u0026#34;10.13\u0026#34;; then ! [[ -e \u0026#34;/Library/Developer/CommandLineTools/usr/bin/git\u0026#34; ]] else ! [[ -e \u0026#34;/Library/Developer/CommandLineTools/usr/bin/git\u0026#34; ]] || ! [[ -e \u0026#34;/usr/include/iconv.h\u0026#34; ]] fi } get_permission() { $STAT \u0026#34;%A\u0026#34; \u0026#34;$1\u0026#34; } user_only_chmod() { [[ -d \u0026#34;$1\u0026#34; ]] \u0026amp;\u0026amp; [[ \u0026#34;$(get_permission \u0026#34;$1\u0026#34;)\u0026#34; != \u0026#34;755\u0026#34; ]] } exists_but_not_writable() { [[ -e \u0026#34;$1\u0026#34; ]] \u0026amp;\u0026amp; ! [[ -r \u0026#34;$1\u0026#34; \u0026amp;\u0026amp; -w \u0026#34;$1\u0026#34; \u0026amp;\u0026amp; -x \u0026#34;$1\u0026#34; ]] } get_owner() { $STAT \u0026#34;%u\u0026#34; \u0026#34;$1\u0026#34; } file_not_owned() { [[ \u0026#34;$(get_owner \u0026#34;$1\u0026#34;)\u0026#34; != \u0026#34;$(id -u)\u0026#34; ]] } get_group() { $STAT \u0026#34;%g\u0026#34; \u0026#34;$1\u0026#34; } file_not_grpowned() { [[ \u0026#34; $(id -G \u0026#34;$USER\u0026#34;) \u0026#34; != *\u0026#34; $(get_group \u0026#34;$1\u0026#34;) \u0026#34;* ]] } # USER isn\u0026#39;t always set so provide a fall back for the installer and subprocesses. if [[ -z \u0026#34;${USER-}\u0026#34; ]]; then USER=\u0026#34;$(chomp \u0026#34;$(id -un)\u0026#34;)\u0026#34; export USER fi # Invalidate sudo timestamp before exiting (if it wasn\u0026#39;t active before). if ! /usr/bin/sudo -n -v 2\u0026gt;/dev/null; then trap \u0026#39;/usr/bin/sudo -k\u0026#39; EXIT fi # Things can fail later if `pwd` doesn\u0026#39;t exist. # Also sudo prints a warning message for no good reason cd \u0026#34;/usr\u0026#34; || exit 1 ####################################################################### script if should_install_git; then abort \u0026#34;$(cat \u0026lt;\u0026lt;EOABORT You must install Git before installing Homebrew. See: ${tty_underline}https://docs.brew.sh/Installation${tty_reset} EOABORT )\u0026#34; fi if [[ -n \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then if [[ -n \u0026#34;${CI-}\u0026#34; ]] || [[ -w \u0026#34;$HOMEBREW_PREFIX_DEFAULT\u0026#34; ]] || [[ -w \u0026#34;/home/linuxbrew\u0026#34; ]] || [[ -w \u0026#34;/home\u0026#34; ]]; then HOMEBREW_PREFIX=\u0026#34;$HOMEBREW_PREFIX_DEFAULT\u0026#34; else trap exit SIGINT sudo_output=\u0026#34;$(/usr/bin/sudo -n -l mkdir 2\u0026gt;\u0026amp;1)\u0026#34; sudo_exit_code=\u0026#34;$?\u0026#34; if [[ \u0026#34;$sudo_exit_code\u0026#34; -ne 0 ]] \u0026amp;\u0026amp; [[ \u0026#34;$sudo_output\u0026#34; = \u0026#34;sudo: a password is required\u0026#34; ]]; then ohai \u0026#34;Select the Homebrew installation directory\u0026#34; echo \u0026#34;- ${tty_bold}Enter your password${tty_reset} to install to ${tty_underline}${HOMEBREW_PREFIX_DEFAULT}${tty_reset} (${tty_bold}recommended${tty_reset})\u0026#34; echo \u0026#34;- ${tty_bold}Press Control-D${tty_reset} to install to ${tty_underline}$HOME/.linuxbrew${tty_reset}\u0026#34; echo \u0026#34;- ${tty_bold}Press Control-C${tty_reset} to cancel installation\u0026#34; fi if have_sudo_access; then HOMEBREW_PREFIX=\u0026#34;$HOMEBREW_PREFIX_DEFAULT\u0026#34; else HOMEBREW_PREFIX=\u0026#34;$HOME/.linuxbrew\u0026#34; fi trap - SIGINT fi HOMEBREW_REPOSITORY=\u0026#34;${HOMEBREW_PREFIX}/Homebrew\u0026#34; fi if [[ \u0026#34;$UID\u0026#34; == \u0026#34;0\u0026#34; ]]; then abort \u0026#34;Don\u0026#39;t run this as root!\u0026#34; elif [[ -d \u0026#34;$HOMEBREW_PREFIX\u0026#34; \u0026amp;\u0026amp; ! -x \u0026#34;$HOMEBREW_PREFIX\u0026#34; ]]; then abort \u0026#34;$(cat \u0026lt;\u0026lt;EOABORT The Homebrew prefix, ${HOMEBREW_PREFIX}, exists but is not searchable. If this is not intentional, please restore the default permissions and try running the installer again: sudo chmod 775 ${HOMEBREW_PREFIX} EOABORT )\u0026#34; fi if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then if version_lt \u0026#34;$macos_version\u0026#34; \u0026#34;10.7\u0026#34;; then abort \u0026#34;$(cat \u0026lt;\u0026lt;EOABORT Your Mac OS X version is too old. See: ${tty_underline}https://github.com/mistydemeo/tigerbrew${tty_reset} EOABORT )\u0026#34; elif version_lt \u0026#34;$macos_version\u0026#34; \u0026#34;10.9\u0026#34;; then abort \u0026#34;Your OS X version is too old\u0026#34; elif ! [[ \u0026#34;$(dsmemberutil checkmembership -U \u0026#34;$USER\u0026#34; -G \u0026#34;$GROUP\u0026#34;)\u0026#34; = *\u0026#34;user is a member\u0026#34;* ]]; then abort \u0026#34;This script requires the user $USER to be an Administrator.\u0026#34; elif version_gt \u0026#34;$macos_version\u0026#34; \u0026#34;$MACOS_LATEST_SUPPORTED\u0026#34; || \\ version_lt \u0026#34;$macos_version\u0026#34; \u0026#34;$MACOS_OLDEST_SUPPORTED\u0026#34;; then who=\u0026#34;We\u0026#34; what=\u0026#34;\u0026#34; if version_gt \u0026#34;$macos_version\u0026#34; \u0026#34;$MACOS_LATEST_SUPPORTED\u0026#34;; then what=\u0026#34;pre-release version\u0026#34; else who+=\u0026#34; (and Apple)\u0026#34; what=\u0026#34;old version\u0026#34; fi ohai \u0026#34;You are using macOS ${macos_version}.\u0026#34; ohai \u0026#34;${who} do not provide support for this ${what}.\u0026#34; echo \u0026#34;$(cat \u0026lt;\u0026lt;EOS This installation may not succeed. After installation, you will encounter build failures with some formulae. Please create pull requests instead of asking for help on Homebrew\\\u0026#39;s GitHub, Discourse, Twitter or IRC. You are responsible for resolving any issues you experience while you are running this ${what}. EOS ) \u0026#34; fi fi ohai \u0026#34;This script will install:\u0026#34; echo \u0026#34;${HOMEBREW_PREFIX}/bin/brew\u0026#34; echo \u0026#34;${HOMEBREW_PREFIX}/share/doc/homebrew\u0026#34; echo \u0026#34;${HOMEBREW_PREFIX}/share/man/man1/brew.1\u0026#34; echo \u0026#34;${HOMEBREW_PREFIX}/share/zsh/site-functions/_brew\u0026#34; echo \u0026#34;${HOMEBREW_PREFIX}/etc/bash_completion.d/brew\u0026#34; echo \u0026#34;${HOMEBREW_REPOSITORY}\u0026#34; # Keep relatively in sync with # https://github.com/Homebrew/brew/blob/master/Library/Homebrew/keg.rb directories=(bin etc include lib sbin share opt var Frameworks etc/bash_completion.d lib/pkgconfig share/aclocal share/doc share/info share/locale share/man share/man/man1 share/man/man2 share/man/man3 share/man/man4 share/man/man5 share/man/man6 share/man/man7 share/man/man8 var/log var/homebrew var/homebrew/linked bin/brew) group_chmods=() for dir in \u0026#34;${directories[@]}\u0026#34;; do if exists_but_not_writable \u0026#34;${HOMEBREW_PREFIX}/${dir}\u0026#34;; then group_chmods+=(\u0026#34;${HOMEBREW_PREFIX}/${dir}\u0026#34;) fi done # zsh refuses to read from these directories if group writable directories=(share/zsh share/zsh/site-functions) zsh_dirs=() for dir in \u0026#34;${directories[@]}\u0026#34;; do zsh_dirs+=(\u0026#34;${HOMEBREW_PREFIX}/${dir}\u0026#34;) done directories=(bin etc include lib sbin share var opt share/zsh share/zsh/site-functions var/homebrew var/homebrew/linked Cellar Caskroom Homebrew Frameworks) mkdirs=() for dir in \u0026#34;${directories[@]}\u0026#34;; do if ! [[ -d \u0026#34;${HOMEBREW_PREFIX}/${dir}\u0026#34; ]]; then mkdirs+=(\u0026#34;${HOMEBREW_PREFIX}/${dir}\u0026#34;) fi done user_chmods=() if [[ \u0026#34;${#zsh_dirs[@]}\u0026#34; -gt 0 ]]; then for dir in \u0026#34;${zsh_dirs[@]}\u0026#34;; do if user_only_chmod \u0026#34;${dir}\u0026#34;; then user_chmods+=(\u0026#34;${dir}\u0026#34;) fi done fi chmods=() if [[ \u0026#34;${#group_chmods[@]}\u0026#34; -gt 0 ]]; then chmods+=(\u0026#34;${group_chmods[@]}\u0026#34;) fi if [[ \u0026#34;${#user_chmods[@]}\u0026#34; -gt 0 ]]; then chmods+=(\u0026#34;${user_chmods[@]}\u0026#34;) fi chowns=() chgrps=() if [[ \u0026#34;${#chmods[@]}\u0026#34; -gt 0 ]]; then for dir in \u0026#34;${chmods[@]}\u0026#34;; do if file_not_owned \u0026#34;${dir}\u0026#34;; then chowns+=(\u0026#34;${dir}\u0026#34;) fi if file_not_grpowned \u0026#34;${dir}\u0026#34;; then chgrps+=(\u0026#34;${dir}\u0026#34;) fi done fi if [[ \u0026#34;${#group_chmods[@]}\u0026#34; -gt 0 ]]; then ohai \u0026#34;The following existing directories will be made group writable:\u0026#34; printf \u0026#34;%s\\n\u0026#34; \u0026#34;${group_chmods[@]}\u0026#34; fi if [[ \u0026#34;${#user_chmods[@]}\u0026#34; -gt 0 ]]; then ohai \u0026#34;The following existing directories will be made writable by user only:\u0026#34; printf \u0026#34;%s\\n\u0026#34; \u0026#34;${user_chmods[@]}\u0026#34; fi if [[ \u0026#34;${#chowns[@]}\u0026#34; -gt 0 ]]; then ohai \u0026#34;The following existing directories will have their owner set to ${tty_underline}${USER}${tty_reset}:\u0026#34; printf \u0026#34;%s\\n\u0026#34; \u0026#34;${chowns[@]}\u0026#34; fi if [[ \u0026#34;${#chgrps[@]}\u0026#34; -gt 0 ]]; then ohai \u0026#34;The following existing directories will have their group set to ${tty_underline}${GROUP}${tty_reset}:\u0026#34; printf \u0026#34;%s\\n\u0026#34; \u0026#34;${chgrps[@]}\u0026#34; fi if [[ \u0026#34;${#mkdirs[@]}\u0026#34; -gt 0 ]]; then ohai \u0026#34;The following new directories will be created:\u0026#34; printf \u0026#34;%s\\n\u0026#34; \u0026#34;${mkdirs[@]}\u0026#34; fi if should_install_command_line_tools; then ohai \u0026#34;The Xcode Command Line Tools will be installed.\u0026#34; fi if [[ -t 0 \u0026amp;\u0026amp; -z \u0026#34;${CI-}\u0026#34; ]]; then wait_for_user fi if [[ -d \u0026#34;${HOMEBREW_PREFIX}\u0026#34; ]]; then if [[ \u0026#34;${#chmods[@]}\u0026#34; -gt 0 ]]; then execute_sudo \u0026#34;/bin/chmod\u0026#34; \u0026#34;u+rwx\u0026#34; \u0026#34;${chmods[@]}\u0026#34; fi if [[ \u0026#34;${#group_chmods[@]}\u0026#34; -gt 0 ]]; then execute_sudo \u0026#34;/bin/chmod\u0026#34; \u0026#34;g+rwx\u0026#34; \u0026#34;${group_chmods[@]}\u0026#34; fi if [[ \u0026#34;${#user_chmods[@]}\u0026#34; -gt 0 ]]; then execute_sudo \u0026#34;/bin/chmod\u0026#34; \u0026#34;755\u0026#34; \u0026#34;${user_chmods[@]}\u0026#34; fi if [[ \u0026#34;${#chowns[@]}\u0026#34; -gt 0 ]]; then execute_sudo \u0026#34;$CHOWN\u0026#34; \u0026#34;$USER\u0026#34; \u0026#34;${chowns[@]}\u0026#34; fi if [[ \u0026#34;${#chgrps[@]}\u0026#34; -gt 0 ]]; then execute_sudo \u0026#34;$CHGRP\u0026#34; \u0026#34;$GROUP\u0026#34; \u0026#34;${chgrps[@]}\u0026#34; fi else execute_sudo \u0026#34;/bin/mkdir\u0026#34; \u0026#34;-p\u0026#34; \u0026#34;${HOMEBREW_PREFIX}\u0026#34; if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then execute_sudo \u0026#34;$CHOWN\u0026#34; \u0026#34;root:wheel\u0026#34; \u0026#34;${HOMEBREW_PREFIX}\u0026#34; else execute_sudo \u0026#34;$CHOWN\u0026#34; \u0026#34;$USER:$GROUP\u0026#34; \u0026#34;${HOMEBREW_PREFIX}\u0026#34; fi fi if [[ \u0026#34;${#mkdirs[@]}\u0026#34; -gt 0 ]]; then execute_sudo \u0026#34;/bin/mkdir\u0026#34; \u0026#34;-p\u0026#34; \u0026#34;${mkdirs[@]}\u0026#34; execute_sudo \u0026#34;/bin/chmod\u0026#34; \u0026#34;g+rwx\u0026#34; \u0026#34;${mkdirs[@]}\u0026#34; execute_sudo \u0026#34;$CHOWN\u0026#34; \u0026#34;$USER\u0026#34; \u0026#34;${mkdirs[@]}\u0026#34; execute_sudo \u0026#34;$CHGRP\u0026#34; \u0026#34;$GROUP\u0026#34; \u0026#34;${mkdirs[@]}\u0026#34; fi if ! [[ -d \u0026#34;${HOMEBREW_CACHE}\u0026#34; ]]; then if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then execute_sudo \u0026#34;/bin/mkdir\u0026#34; \u0026#34;-p\u0026#34; \u0026#34;${HOMEBREW_CACHE}\u0026#34; else execute \u0026#34;/bin/mkdir\u0026#34; \u0026#34;-p\u0026#34; \u0026#34;${HOMEBREW_CACHE}\u0026#34; fi fi if exists_but_not_writable \u0026#34;${HOMEBREW_CACHE}\u0026#34;; then execute_sudo \u0026#34;/bin/chmod\u0026#34; \u0026#34;g+rwx\u0026#34; \u0026#34;${HOMEBREW_CACHE}\u0026#34; fi if file_not_owned \u0026#34;${HOMEBREW_CACHE}\u0026#34;; then execute_sudo \u0026#34;$CHOWN\u0026#34; \u0026#34;$USER\u0026#34; \u0026#34;${HOMEBREW_CACHE}\u0026#34; fi if file_not_grpowned \u0026#34;${HOMEBREW_CACHE}\u0026#34;; then execute_sudo \u0026#34;$CHGRP\u0026#34; \u0026#34;$GROUP\u0026#34; \u0026#34;${HOMEBREW_CACHE}\u0026#34; fi if [[ -d \u0026#34;${HOMEBREW_CACHE}\u0026#34; ]]; then execute \u0026#34;$TOUCH\u0026#34; \u0026#34;${HOMEBREW_CACHE}/.cleaned\u0026#34; fi if should_install_command_line_tools \u0026amp;\u0026amp; version_ge \u0026#34;$macos_version\u0026#34; \u0026#34;10.13\u0026#34;; then ohai \u0026#34;Searching online for the Command Line Tools\u0026#34; # This temporary file prompts the \u0026#39;softwareupdate\u0026#39; utility to list the Command Line Tools clt_placeholder=\u0026#34;/tmp/.com.apple.dt.CommandLineTools.installondemand.in-progress\u0026#34; execute_sudo \u0026#34;$TOUCH\u0026#34; \u0026#34;$clt_placeholder\u0026#34; clt_label_command=\u0026#34;/usr/sbin/softwareupdate -l | grep -B 1 -E \u0026#39;Command Line Tools\u0026#39; | awk -F\u0026#39;*\u0026#39; \u0026#39;/^ *\\\\*/ {print \\$2}\u0026#39; | sed -e \u0026#39;s/^ *Label: //\u0026#39; -e \u0026#39;s/^ *//\u0026#39; | sort -V | tail -n1\u0026#34; clt_label=\u0026#34;$(chomp \u0026#34;$(/bin/bash -c \u0026#34;$clt_label_command\u0026#34;)\u0026#34;)\u0026#34; if [[ -n \u0026#34;$clt_label\u0026#34; ]]; then ohai \u0026#34;Installing $clt_label\u0026#34; execute_sudo \u0026#34;/usr/sbin/softwareupdate\u0026#34; \u0026#34;-i\u0026#34; \u0026#34;$clt_label\u0026#34; execute_sudo \u0026#34;/bin/rm\u0026#34; \u0026#34;-f\u0026#34; \u0026#34;$clt_placeholder\u0026#34; execute_sudo \u0026#34;/usr/bin/xcode-select\u0026#34; \u0026#34;--switch\u0026#34; \u0026#34;/Library/Developer/CommandLineTools\u0026#34; fi fi # Headless install may have failed, so fallback to original \u0026#39;xcode-select\u0026#39; method if should_install_command_line_tools \u0026amp;\u0026amp; test -t 0; then ohai \u0026#34;Installing the Command Line Tools (expect a GUI popup):\u0026#34; execute_sudo \u0026#34;/usr/bin/xcode-select\u0026#34; \u0026#34;--install\u0026#34; echo \u0026#34;Press any key when the installation has completed.\u0026#34; getc execute_sudo \u0026#34;/usr/bin/xcode-select\u0026#34; \u0026#34;--switch\u0026#34; \u0026#34;/Library/Developer/CommandLineTools\u0026#34; fi if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]] \u0026amp;\u0026amp; ! output=\u0026#34;$(/usr/bin/xcrun clang 2\u0026gt;\u0026amp;1)\u0026#34; \u0026amp;\u0026amp; [[ \u0026#34;$output\u0026#34; == *\u0026#34;license\u0026#34;* ]]; then abort \u0026#34;$(cat \u0026lt;\u0026lt;EOABORT You have not agreed to the Xcode license. Before running the installer again please agree to the license by opening Xcode.app or running: sudo xcodebuild -license EOABORT )\u0026#34; fi ohai \u0026#34;Downloading and installing Homebrew...\u0026#34; ( cd \u0026#34;${HOMEBREW_REPOSITORY}\u0026#34; \u0026gt;/dev/null || return # we do it in four steps to avoid merge errors when reinstalling execute \u0026#34;git\u0026#34; \u0026#34;init\u0026#34; \u0026#34;-q\u0026#34; # \u0026#34;git remote add\u0026#34; will fail if the remote is defined in the global config execute \u0026#34;git\u0026#34; \u0026#34;config\u0026#34; \u0026#34;remote.origin.url\u0026#34; \u0026#34;${BREW_REPO}\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;config\u0026#34; \u0026#34;remote.origin.fetch\u0026#34; \u0026#34;+refs/heads/*:refs/remotes/origin/*\u0026#34; # ensure we don\u0026#39;t munge line endings on checkout execute \u0026#34;git\u0026#34; \u0026#34;config\u0026#34; \u0026#34;core.autocrlf\u0026#34; \u0026#34;false\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;fetch\u0026#34; \u0026#34;origin\u0026#34; \u0026#34;--force\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;fetch\u0026#34; \u0026#34;origin\u0026#34; \u0026#34;--tags\u0026#34; \u0026#34;--force\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;reset\u0026#34; \u0026#34;--hard\u0026#34; \u0026#34;origin/master\u0026#34; execute \u0026#34;ln\u0026#34; \u0026#34;-sf\u0026#34; \u0026#34;${HOMEBREW_REPOSITORY}/bin/brew\u0026#34; \u0026#34;${HOMEBREW_PREFIX}/bin/brew\u0026#34; execute \u0026#34;cd\u0026#34; \u0026#34;${HOMEBREW_PREFIX}/Homebrew/Library/Taps/homebrew\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;clone\u0026#34; \u0026#34;https://mirrors.ustc.edu.cn/homebrew-cask.git\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;clone\u0026#34; \u0026#34;https://mirrors.ustc.edu.cn/homebrew-core.git\u0026#34; execute \u0026#34;${HOMEBREW_PREFIX}/bin/brew\u0026#34; \u0026#34;update\u0026#34; \u0026#34;--force\u0026#34; \u0026#34;-v\u0026#34; execute \u0026#34;echo\u0026#34; \u0026#34;export\u0026#34; \u0026#34;HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles\u0026#34; \u0026#34;\u0026gt;\u0026gt;\u0026#34; \u0026#34;~/.zshrc\u0026#34; execute \u0026#34;source\u0026#34; \u0026#34;~/.zshrc\u0026#34; ) if [[ \u0026#34;:${PATH}:\u0026#34; != *\u0026#34;:${HOMEBREW_PREFIX}/bin:\u0026#34;* ]]; then warn \u0026#34;${HOMEBREW_PREFIX}/bin is not in your PATH.\u0026#34; fi ohai \u0026#34;Installation successful!\u0026#34; echo # Use the shell\u0026#39;s audible bell. if [[ -t 1 ]]; then printf \u0026#34;\\a\u0026#34; fi # Use an extra newline and bold to avoid this being missed. ohai \u0026#34;Homebrew has enabled anonymous aggregate formulae and cask analytics.\u0026#34; echo \u0026#34;$(cat \u0026lt;\u0026lt;EOS ${tty_bold}Read the analytics documentation (and how to opt-out) here: ${tty_underline}https://docs.brew.sh/Analytics${tty_reset} No analytics data has been sent yet (or will be during this \\`install\\` run). EOS ) \u0026#34; ohai \u0026#34;Homebrew is run entirely by unpaid volunteers. Please consider donating:\u0026#34; echo \u0026#34;$(cat \u0026lt;\u0026lt;EOS ${tty_underline}https://github.com/Homebrew/brew#donations${tty_reset} EOS ) \u0026#34; ( cd \u0026#34;${HOMEBREW_REPOSITORY}\u0026#34; \u0026gt;/dev/null || return execute \u0026#34;git\u0026#34; \u0026#34;config\u0026#34; \u0026#34;--replace-all\u0026#34; \u0026#34;homebrew.analyticsmessage\u0026#34; \u0026#34;true\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;config\u0026#34; \u0026#34;--replace-all\u0026#34; \u0026#34;homebrew.caskanalyticsmessage\u0026#34; \u0026#34;true\u0026#34; ) ohai \u0026#34;Next steps:\u0026#34; echo \u0026#34;- Run \\`brew help\\` to get started\u0026#34; echo \u0026#34;- Further documentation: \u0026#34; echo \u0026#34; ${tty_underline}https://docs.brew.sh${tty_reset}\u0026#34; if [[ -n \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then case \u0026#34;$SHELL\u0026#34; in */bash*) if [[ -r \u0026#34;$HOME/.bash_profile\u0026#34; ]]; then shell_profile=\u0026#34;$HOME/.bash_profile\u0026#34; else shell_profile=\u0026#34;$HOME/.profile\u0026#34; fi ;; */zsh*) shell_profile=\u0026#34;$HOME/.zprofile\u0026#34; ;; *) shell_profile=\u0026#34;$HOME/.profile\u0026#34; ;; esac cat \u0026lt;\u0026lt;EOS - Install the Homebrew dependencies if you have sudo access: ${tty_bold}Debian, Ubuntu, etc.${tty_reset} sudo apt-get install build-essential ${tty_bold}Fedora, Red Hat, CentOS, etc.${tty_reset} sudo yum groupinstall \u0026#39;Development Tools\u0026#39; See ${tty_underline}https://docs.brew.sh/linux${tty_reset} for more information. - Configure Homebrew in your ${tty_underline}${shell_profile}${tty_reset} by running echo \u0026#39;eval \\$(${HOMEBREW_PREFIX}/bin/brew shellenv)\u0026#39; \u0026gt;\u0026gt; ${shell_profile} - Add Homebrew to your ${tty_bold}PATH${tty_reset} eval \\$(${HOMEBREW_PREFIX}/bin/brew shellenv) - We recommend that you install GCC by running: brew install gcc EOS fi ","date":"2020-05-16T00:20:48+08:00","permalink":"https://techinsight.pages.dev/p/%E4%B8%AD%E5%9B%BD%E5%A4%A7%E9%99%86%E5%AE%89%E8%A3%85homebrew%E7%9A%84%E6%9C%80%E4%BC%98%E6%96%B9%E6%B3%95/","title":"中国大陆安装homebrew的最优方法"},{"content":"Java的stream api真是功能强大，但写的时候总是忘，这里简单记录以下。\n概念 经常会搞混淆的应该就是Stream和Collection的区别。从定义上讲，Collection是一个内存数据结构，它包含了这个数据结构拥有的所有元素，每个元素都必须是确定的。也就是说，一个元素在加入到一个集合（Collection）中之前一定是计算好了的、确定的。而Stream从概念上固定的数据结构，它里面的元素可以按需计算。\n差异如下：\n流并不存储其元素。这些元素可能存储在底层的集合中，或者是按需生成的。 流的操作不会修改其数据源。filter方法不会从流中移除元素，而是会生成一个新的流。 流的操作是尽可能惰性的，这意味着直至需要结果时，操作才会执行。 创建Stream的几种方式 1. Stream.Of(val1, val2, val3) 1 2 3 4 5 6 7 public class StreamBuilders { public static void main(String[] args) { Stream\u0026lt;Integer\u0026gt; stream = Stream.of(1,2,3,4,5,6,7,8,9); stream.forEach(p -\u0026gt; System.out.println(p)); // stream.forEach(System.out::println); } } 2. Stream.of(arrayOfElements) 1 2 3 4 5 6 public class StreamBuilders { public static void main(String[] args) { Stream\u0026lt;Integer\u0026gt; stream = Stream.of(new Integer[]{1,2,3,4,5,6,7,8,9}); stream.forEach(System.out::println); } } 3. List.stream() 1 2 3 4 5 6 7 8 9 10 11 public class StreamBuilders { public static void main(String[] args) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; 10; i++) { list.add(i); } Stream\u0026lt;Integer\u0026gt; stream = list.stream(); stream.forEach(System.out::println); } } 4. Stream.generate()或Stream.iterate() 1 2 3 4 5 6 public class StreamBuilders { public static void main(String[] args) { Stream\u0026lt;Date\u0026gt; stream = Stream.generate(Date::new); stream.forEach(System.out::println); } } 5. String chars或String tokens 1 2 3 4 5 6 public class StreamBuilders { public static void main(String[] args) { IntStream stream = \u0026#34;12345_abcde\u0026#34;.chars(); stream.forEach(System.out::println); } } 6. Map通过entrySet().stream() 1 2 3 4 5 6 7 8 9 10 public class StreamBuilders { public static void main(String[] args) { Map\u0026lt;Integer, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; 10; i++) { map.put(i, Character.toString(i+96)); } Stream\u0026lt;Map.Entry\u0026lt;Integer, String\u0026gt;\u0026gt; stream = map.entrySet().stream(); stream.forEach(System.out::println); } } 中间操作和终止操作 1 2 3 4 5 6 7 8 9 10 11 12 13 public class StreamBuilders { private static final List\u0026lt;String\u0026gt; memberNames = new ArrayList\u0026lt;\u0026gt;(); static { memberNames.add(\u0026#34;Amitabh\u0026#34;); memberNames.add(\u0026#34;Shekhar\u0026#34;); memberNames.add(\u0026#34;Aman\u0026#34;); memberNames.add(\u0026#34;Rahul\u0026#34;); memberNames.add(\u0026#34;Shahrukh\u0026#34;); memberNames.add(\u0026#34;Salman\u0026#34;); memberNames.add(\u0026#34;Yana\u0026#34;); memberNames.add(\u0026#34;Lokesh\u0026#34;); } } 中间操作 1 2 3 4 5 6 7 8 9 10 11 public static void main(String[] args) { memberNames.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;A\u0026#34;)) .forEach(System.out::println); memberNames.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;S\u0026#34;)) .map(String::toUpperCase) .forEach(System.out::println); memberNames.stream().sorted() .map(String::toUpperCase) .forEach(System.out::println); } 终止操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 memberNames.forEach(System.out::println); System.out.println(memberNames.stream().map(String::toLowerCase) .collect(Collectors.toList())); boolean b1 = memberNames.stream().anyMatch(s -\u0026gt; s.startsWith(\u0026#34;A\u0026#34;)); System.out.println(b1); boolean b2 = memberNames.stream().allMatch(s -\u0026gt; s.startsWith(\u0026#34;A\u0026#34;)); System.out.println(b2); boolean b3 = memberNames.stream().noneMatch(s -\u0026gt; s.startsWith(\u0026#34;A\u0026#34;)); System.out.println(b3); long count = memberNames.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;S\u0026#34;)).count(); System.out.println(count); Optional\u0026lt;String\u0026gt; reduced = memberNames.stream() .reduce((s1, s2) -\u0026gt; s1 + \u0026#34;#\u0026#34; + s2); reduced.ifPresent(System.out::println); map和flatMap 这两个真是最容易搞混的，不过如果要搞清楚他们的区别也很简单：map就是一个转换，把原来是a的转换成b，原来是List\u0026lt;String\u0026gt;，map之后还是List\u0026lt;String\u0026gt;（其实是stream，是类型没有变），而flatMap会生成一个新的stream把原先的多个stream合并在一起。举例说明下\n1 2 3 4 5 // 例1 List\u0026lt;String\u0026gt; lower = Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;); List\u0026lt;String\u0026gt; upper = lower.stream().map(String::toUpperCase).collect(Collectors.toList()); System.out.println(lower); System.out.println(upper); 从这个例子可以看出，通过map，让这个List\u0026lt;String\u0026gt;中的每个元素都执行了e.toUpperCase()方法，输出的结果就是\n1 2 [a, b, c, d] [A, B, C, D] 下面看flatMap可以实现什么功能\n1 2 3 4 5 6 7 8 // 例2 List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; packed = new ArrayList\u0026lt;\u0026gt;(); packed.add(lower); packed.add(upper); System.out.println(packed); List\u0026lt;String\u0026gt; flat = packed.stream().flatMap(s -\u0026gt; s.stream()).map(String::toUpperCase).collect(Collectors.toList()); System.out.println(flat); 这里必须要注意一点，map()的输出是stream中的一级元素，像例1中的String::toUpperCase显然是输入a，输出A。而对于flatMap而言，它的输入是一个Collection，而输出是一个stream，那怎么形成一个stream呢，像例2中的packed，它就是由两个List\u0026lt;String\u0026gt;组成的，对他们调用stream()方法就让它返回一个stream到flatMap的输出了。\n举个不太恰当的例子，这里有3包牛奶，map方法只能把3包牛奶倒到3个杯子里，而flatMap可以把它们倒到1个杯子里。如何实现呢？当然就是在flatMap中把每个牛奶的袋子撕开，然后倒出来。\n例2的输出如下\n1 2 [[a, b, c, d], [A, B, C, D]] [A, B, C, D, A, B, C, D] 理解到了这一步，基本上就搞清楚了二者的区别了，总之就是记住一定要让flatMap()输出一个stream。\nMap的stream() List输出到stream的方法很容易理解，因为它本身就是一个一个的元素，但Map是分了key和value的，要怎么才能把它转换成stream呢？答案是Map.Entry。可能第一门语言就是Java的同学觉得很理所应当，但熟悉PHP的同学再来理解这个概念就有点对应不上了（因为PHP里基本上不区分List和Map，一切皆为数组）。先看一个PHP的例子\n1 2 3 4 5 6 7 8 $a = [ \u0026#39;a\u0026#39; =\u0026gt; \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39; =\u0026gt; \u0026#39;d\u0026#39;, ]; foreach ($a as $key =\u0026gt; $value) { echo $key . \u0026#39;=\u0026gt;\u0026#39; . $value, \u0026#34;\\n\u0026#34;; } 在这个例子中$key和$value的组合就是Java中Map.Entry的概念了，只不过继续遵循封闭的原则，给二者都配备了对应的方法getKey()和getValue()，也就是说，可以从一个Map的Entry中同时获取当前这个元素的key和value。那么问题又来了，怎么拿到它的Entry呢？通过entrySet()方法。\n对一个Map调用entrySet()方法，相当于新建了一个List，它的元素是这个Map的Entry，这就又回到了List，当然也就可以用stream api了。比如要把一个Map的key和value倒过来（PHP的array_flip方法）\n1 2 3 4 5 6 7 8 9 10 Map\u0026lt;String, String\u0026gt; g = new HashMap\u0026lt;\u0026gt;(); g.put(\u0026#34;k1\u0026#34;, \u0026#34;v1\u0026#34;); g.put(\u0026#34;k2\u0026#34;, \u0026#34;v2\u0026#34;); g.put(\u0026#34;k3\u0026#34;, \u0026#34;v3\u0026#34;); g.put(\u0026#34;k4\u0026#34;, \u0026#34;v4\u0026#34;); g.put(\u0026#34;k5\u0026#34;, \u0026#34;v5\u0026#34;); val h = g.entrySet().stream().collect(Collectors.toMap(Map.Entry::getValue, Map.Entry::getKey)); System.out.println(g); System.out.println(h); 结果如下\n1 2 {k1=v1, k2=v2, k3=v3, k4=v4, k5=v5} {v1=k1, v2=k2, v3=k3, v4=k4, v5=k5} 注意这个方法可能会有异常，因为如果原始输入中不同的key对应了相同的value，就无法生成新map了\n1 2 3 4 5 6 7 8 9 10 11 Exception in thread \u0026#34;main\u0026#34; java.lang.IllegalStateException: Duplicate key v3 (attempted merging values k3 and k4) at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) at java.base/java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1746) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) at fun.happyhacker.java.stream.MapDemo.main(MapDemo.java:56) Map还有一个方法，可以把它的所有key输出到一个List，PHPer肯定又想到了array_keys()，没错，就是keySet()\ncollect 很多时候前面的一堆操作都是要把结果收集起来，这个话题太大了，collect的方式多种多样，这里也说不完，简单说几个最常用的。\ncollect(Collectors.toList()) 这是最简单的，把一个stream中的所有元素按前面的输出收集到一个List中。而这个List的类型，当然就取决于这个stream中的元素了，参考例1的代码即可。\ncollect(Collectors.toMap()) 这个就相对复杂一点了，既然把结果收集成map，那就肯定得设置key和value，比如我们要把一个小写字母的列表和它对应的大写字母分别对应起来，\n1 2 3 4 // 例3 List\u0026lt;String\u0026gt; lower = Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;); val f = lower.stream().collect(Collectors.toMap(e -\u0026gt; e, String::toUpperCase)); System.out.println(f); 输出如下\n1 {a=A, b=B, c=C, d=D} ","date":"2020-04-29T23:23:57+08:00","permalink":"https://techinsight.pages.dev/p/java-stream-api/","title":"Java Stream Api"},{"content":"前段时间写Java一直忙于做工程上的工作，还是得停下来看看理论的东西。\n基本类型 类型 存储需求 取值范围 备注 int 4 Bytes $-2^{31}$ ~ $2^{31}-1$ short 2 Bytes $-2^{15}$ ~ $2^{15}-1$ long 8 Bytes $-2^{63}$ ~ $2^{63}-1$ byte 1 Byte $-2^7$ ~ $2^7-1$ float 4 Bytes 大约 $3.40282347E+38F$（有效位数6～7位） 只有很少场景可以用到float double 8 Bytes 大约 $1.79769313486231570E+308$（有效位数15位） 带小数点的默认是double char - char描述了UTF-16编码中的一个代码单元，尽量不要使用 boolean 1 bit true 和 false 位运算 \u0026amp; and | or ^ xor ~ not 以(n \u0026amp; 0b1000) / 0b1000为例，如果整数n的二进制表示从左到右第4位是1，结果就是1，其余情况则为0。利用\u0026amp;并结合适当的2的幂，可以把其他位mask掉，而只留下其中一位。\n\u0026gt;\u0026gt;和\u0026lt;\u0026lt;运算符可以将位模式左移或者右移，需要建立位模式来完成掩码时，这两个运算符很方便。\n1 \u0026lt;\u0026lt; 3 == 0b1000 == 8\n\u0026gt;\u0026gt;\u0026gt;运算符会用0填充高位，这与\u0026gt;\u0026gt;不同，后者用符号填充高位。不存在\u0026lt;\u0026lt;\u0026lt;运算符。\n输入输出 Scanner就不多说了，只说比较有意思也最可能出现问题的Console.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 package fun.happyhacker.java.basics; import java.io.Console; import java.util.Scanner; public class IO { public static void main(String[] args) { IO io = new IO(); io.console(); } private void scanner() { Scanner in = new Scanner(System.in); System.out.println(\u0026#34;What is your name?\u0026#34;); String name = in.nextLine(); System.out.println(\u0026#34;How old are you?\u0026#34;); int age = in.nextInt(); System.out.println(\u0026#34;Hello \u0026#34; + name + \u0026#34;, you\u0026#39;ll be \u0026#34; + (age + 1) + \u0026#34; next year!\u0026#34;); } private void console() { Console console = System.console(); String username = console.readLine(\u0026#34;User name: \u0026#34;); char[] password = console.readPassword(\u0026#34;Password: \u0026#34;); System.out.println(username); System.out.println(password); } } 然后很开心的执行一下，结果发现报错了\n1 2 3 4 5 Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException at fun.happyhacker.java.basics.IO.console(IO.java:25) at fun.happyhacker.java.basics.IO.main(IO.java:9) Process finished with exit code 1 这个原因是\n\u0026ldquo;If the virtual machine is started from an interactive command line without redirecting the standard input and output streams then its console will exist and will typically be connected to the keyboard and display from which the virtual machine was launched. If the virtual machine is started automatically, for example by a background job scheduler, then it will typically not have a console.\u0026rdquo;\n简单说就是如果它是从命令行直接启动的就没问题，而如果是从一个【后台工作调度器】，其实也就是IDEA的工作线程启动的，就没有console了。所以要执行带有console的应用，就需要javac App.java \u0026amp;\u0026amp; java App了。\n数组 Arrays和List在Java中是不同的数据类型（神奇的是为什么还有一个ArrayList）。。。\nArray数组表示的是【定长的数组，一旦确定了长度就不能再改变了】，而List列表则表示可以改变长度的列表。\nArray的创建方式有很多，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package fun.happyhacker.java.basics; import java.util.Arrays; public class Array { public static void main(String[] args) { // 总之数组就是定长的，要么给定长度，要么给定元素让它自己计算长度 int[] a = new int[10]; int n = 100; int[] b = new int[n]; int[] c = {1,2,3,4,}; int[] d = new int[]{1,2,3,4,}; // 还有一点需要注意，下面的e和f两个变量会共用同一个数组，所以改变其中一个也会改变另外一个 int[] e = {1,2,3,4,}; int[] f = e; f[2] = 5; System.out.println(Arrays.toString(e)); System.out.println(Arrays.toString(f)); // 如果要拷贝数组，则需要用Arrays.copy方法 int[] g = {1,2,3,4,}; int[] h = Arrays.copyOf(g, g.length); h[2] = 8; System.out.println(Arrays.toString(g)); System.out.println(Arrays.toString(h)); } } ","date":"2020-04-21T23:28:34+08:00","permalink":"https://techinsight.pages.dev/p/java%E5%9F%BA%E7%A1%80/","title":"Java基础"},{"content":"Java报错真是多，一不小心单元测试也报错。\n代码是这样的\n1 2 3 4 5 6 7 8 9 @SpringBootTest @Slf4j @RunWith(SpringJUnit4ClassRunner.class) public class MyTestClass { @Test void testShowBatch() { } } 执行mvn test -Dtest=MyTestClass#testShowBatch报错\n1 2 3 4 5 6 7 8 [INFO] Running MyTestClass [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.001 s \u0026lt;\u0026lt;\u0026lt; FAILURE! - in MyTestClass [ERROR] initializationError Time elapsed: 0.001 s \u0026lt;\u0026lt;\u0026lt; ERROR! org.junit.runners.model.InvalidTestClassError: Invalid test class \u0026#39;MyTestClass\u0026#39;: 1. No runnable methods ... 事实上这个并不是问题的关键，当时这个问题的原因在于本地代码上传到远端的目录和我执行的目录不在一个地方，所以报错了。。。但总体上在执行单元测试时的用法和下文说的差不多，junit5不需要@RunWith注解了。\n明显是有runnable methods的啊，原来是因为@RunWith(SpringJUnit4ClassRunner.class)这个注解，它是junit4的用法，加上它，就会查找带有@org.junit.Test注解的方法，也就是所谓的runnable methods。而我这里的看起来是@Test的方法，其实是@org.junit.jupiter.api.Test，是junit5的runnable method，二者不能兼容，所以就出现了上面的错误。\n总结 在测试Springboot应用时，如果你需要Springboot加载才能执行单元测试代码，可以选择使用junit4或者junit5。\n当使用junit4\n需要的类注解是\n1 2 3 @SpringBootTest @Slf4j @RunWith(SpringJUnit4ClassRunner.class) 需要的方法注解是\n1 @org.junit.Test 当使用junit5\n需要的类注解是\n1 2 @SpringBootTest @Slf4j 需要的方法注解是\n1 @org.junit.jupiter.api.Test ","date":"2020-04-08T00:30:39+08:00","permalink":"https://techinsight.pages.dev/p/%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E6%97%B6%E6%8A%A5%E9%94%99%E5%88%86%E6%9E%90%E5%8F%8A%E8%A7%A3%E5%86%B3/","title":"执行单元测试时报错分析及解决"},{"content":"我们用Java是为了让它自动管理内存，然后我的第一个Java项目上线就面临排查线程数过多导致无法创建新线程、内存泄漏导致机器重启各种个样的问题。这和我原本想象的可不一样。\n最近几天比较忙，排查的过程没有有效的记录下来，但其实也不影响最终的结果，因为并不是代码的问题导致的内存泄漏，而是docker的版本问题。\n现象描述 我理解的内存泄漏应该是内存的缓慢增长，但实际上从监控曲线看，它总是每隔几分钟就突增一次，每次增加的量大概是物理机内存总量的15%，所以能在30分钟左右就耗尽物理机的内存，最终docker进程挂掉，内存释放。\n注意，这里的现象是docker服务挂掉，而不是运行tomcat的容器挂掉。这是一个很重要的信号，也是在最早期被忽略的线索。\n同时还发现，当内存占用量不停升高时，重启运行tomcat的容器并不能释放内存。这一点很反常规，因为按我们的理解，tomcat是一个jvm进程，当jvm进程重启时，它对应的内存泄漏应该也被释放才对。这时候我们又被绕进了另外一个误区，去排查native memory了，这里按下不表。\n其实从上面的描述看，最可能的问题是在docker上。使用的docker版本是非常老的1.6.2，java版本是adoptopenjdk 1.8.242，按照官方的说法，在jdk 1.8.131之后，jvm已经可以识别cgroup，即可以感知到自己是在容器中运行，因而不会将物理机的总内存认为是jvm可用的总内存。但还是无法解释我们遇到但问题。一筹莫展之际，我尝试了在宿主机上执行运行tomcat，持续运行了几个小时，内存占用率一直在15%左右，非常稳定，内存泄漏的问题不见了。\n那么基本上就锁定是docker的问题了。升级docker版本到1.13.1，问题解决。\n排查用到的工具 虽然排查的过程走了不少弯路，但基本上能用到的排查工具也都用到了。这里只记录一下大概，用到的时候还是要查对应的文档。\njps\n首先是最基础的jps -lvVm，可以看到整个jvm进程的启动参数，也可以用于验证指定的各种命令行参数是否生效了。\nvisualvm\n这个就是比较高端的分析工具了，最重要的是它能连接远程jvm进程，实时查看堆内存和gc的情况，甚至还有很多插件用于查看更多信息。但对于我们的这个情况来说，只能看到堆内存的占用一直维持在一个正常水平，ygc也很正常，整个内存的使用情况呈锯齿状，能说明堆内存没有泄漏。\nperf\n这是linux内核支持的工具，可以监听指定进程在一段时间内的所有系统调用。\nmat\n全称Memory Analyzer Tool, 可以用于分析生成的hprof文件，这个hprof文件可以是系统崩溃时自动生成的堆内存快照，也可以是由jmap生成。\njmap\n生成可供mat分析的内存快照。\n需要注意的是.hprof文件可能会非常大，最好监控起来，一方面是因为除去人为通过jmap生成的情况之外，都是系统崩溃了才会生成，另一方面是文件太大，很可能会快速占满磁盘空间。\n基本上常用的内存检查工具就这些了，关于这些工具的详细使用方法，最好还是参照官方文档，后面如果再遇到需要查内存泄漏的例子，将会在这里补充一些具体的案例。\n","date":"2020-04-05T23:11:28+08:00","permalink":"https://techinsight.pages.dev/p/java%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/","title":"Java内存泄漏排查记录"},{"content":"用Java开发新项目，遇到了很多之前没见过的问题，时区算是第二头疼的一个。\n现象 数据库中的时间是2020-03-20 00:00:00，但查询时需要2020-03-21 08:00:00才能查到。这明显是差了8个时区。\n排查过程 JDBC连接 搜索该问题，提到的最多的就是在jdbc url中加上时区的配置，应该是Asia/Shanghai和GMT+8(注意encode)都行，我测试的时候两种都没有解决我的问题。\nserverTimezone=GMT%2B8\n数据库服务器的时间 到数据库服务器执行以下命令，得到的输出如下\n1 2 3 4 5 6 7 mysql\u0026gt; show variables like \u0026#39;%time_zone%\u0026#39;; +------------------+--------+ | Variable_name | Value | +------------------+--------+ | system_time_zone | CST | | time_zone | SYSTEM | +------------------+--------+ 测试环境和线上环境都是这样的配置，看起来问题应该不是出在这里。但还是要提以下这个容易产生混淆的地方，CST这个缩写是有歧义的，起码在指时区这一件事情时，就有多种不同的意思\n但其实在mysql服务的时区这件事上其实是没有歧义的，就是指UTC-6:00的中央标准时间。\n服务器本地时间和容器的时间 由于我的代码是在docker中执行的，所以其实更应该关注的是容器中的时间。这时候发现了容器中的时间和宿主机相差了8小时，猜测问题可能是由此引起的。\nJava的LocalDateTime 因为LocalDateTime是和时区无关的时间，jvm默认它就是UTC时间，所以传过来的2020-03-20 00:00:00会被转换成2020-03-19 16:00:00，这就是最上面的问题的答案了。\n可以通过以下方式验证\n1 2 3 4 5 6 7 8 import java.time.LocalDateTime; public class TimeZoneTest { public static void main(String[] args) { System.out.println(LocalDateTime.now()); } } 执行javac TimeZoneTest.java \u0026amp;\u0026amp; java -cp TimeZoneTest，会发现和预期不符合，准确的说是比当前时间慢了8小时。\n1 2 3 4 5 6 7 8 9 10 11 import java.time.LocalDateTime; import java.util.TimeZone; public class TimeZoneTest { public static void main(String[] args) { TimeZone tz = TimeZone.getTimeZone(\u0026#34;GMT+8\u0026#34;); TimeZone.setDefault(tz); System.out.println(LocalDateTime.now()); } } 同样执行javac TimeZoneTest.java \u0026amp;\u0026amp; java -cp TimeZoneTest，就会发现和当前时间一致了。\n解决方案 找到了问题的根本原因，就容易解决了。\nSpringboot 1 2 3 4 5 6 7 8 9 10 11 @SpringBootApplication public class WebApplication { public static void main(String[] args) { SpringApplication.run(WebApplication.class, args); } @PostConstruct void setDefaultTimeZone() { TimeZone.setDefault(TimeZone.getTimeZone(\u0026#34;GMT+8\u0026#34;)); } } 设定Java命令行参数 java -Duser.timezone=GMT+8 TimeZoneTest\n总结 我最终选择的是在jdbc url中添加时区的同时，在SpringBootApplication中设置默认时区，完美的解决了问题。\n","date":"2020-04-01T12:48:50+08:00","permalink":"https://techinsight.pages.dev/p/java%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98%E7%9A%84%E6%8E%92%E6%9F%A5%E5%92%8C%E5%88%86%E6%9E%90/","title":"Java时区问题的排查和分析"},{"content":"mac本身安装了ssh服务，默认情况下不会开机自启。本文记录了开启和停止sshd服务的方法。\n1. 启动sshd服务： sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist\n2. 停止sshd服务： sudo launchctl unload -w /System/Library/LaunchDaemons/ssh.plist\n3. 查看是否启动： sudo launchctl list | grep ssh\n如果看到下面的输出表示成功启动了：\n1 2 $ sudo launchctl list | grep ssh -\t0\tcom.openssh.sshd 为什么需要开启macOS上的sshd服务呢？是因为在本地部署flink或者其他某些集群服务时，默认是要通过ssh协议发送文件的。对，传输到本机也是用ssh服务，所以如果没有开通服务就无法正确部署。\n","date":"2020-03-29T10:57:10+08:00","permalink":"https://techinsight.pages.dev/p/macos%E4%B8%8A%E7%AE%A1%E7%90%86ssh%E6%9C%8D%E5%8A%A1/","title":"macOS上管理SSH服务"},{"content":"用了5年macOS也一直没有用过苹果原生的服务管理、AppleSript等等，总感觉不够直观，好在还有Homebrew这个神器，帮我解决了很多问题。\n对于通过homebrew安装的服务，可以通过其提供的brew services或者服务自带的命令进行管理。这里只记录了两种，其他需要查看启动方法的可以通过brew info命令查看。\nMySQL 1 2 3 4 brew install mysql mysql.server start mysql.server stop ZooKeeper 1 2 3 brew install zookeeper brew services start zookeeper # 后台启动 zkServer start # 前台启动 ","date":"2020-03-29T10:53:04+08:00","permalink":"https://techinsight.pages.dev/p/%E4%BD%BF%E7%94%A8homebrew%E7%AE%A1%E7%90%86macos%E4%B8%8A%E7%9A%84%E6%9C%8D%E5%8A%A1/","title":"使用Homebrew管理macOS上的服务"},{"content":"实在是受不了MBP13孱弱的性能和糟糕的发热，终于下定决心组装一台台式机。最终还是真香。\n本文操作过程基于白苹果，Windows环境的小伙伴可参考其他教程\n本次操作的大部分流程都基于opencore-vanilla-desktop-guide，感谢国内外友人对黑苹果作出的卓越贡献。\n用过两个MacBookPro，最大的感觉就是用起来很方便（我是一个软件开发者），但性能实在是太弱鸡了，严重影响了效率。以至于黑苹果安装完成之后我发现原来的过渡动画竟然不见了。。。对，就是这么明显。\n准备阶段 为了这个黑苹果，我全新配置了一台台式机，配置也是改了又改，最终性能需求战胜了便携性，选择了一台MATX机箱。\n选择配置考虑的问题如下：\n1. CPU 我就是需要一个性能强劲的CPU和足够的内存。本来想着上i7 9700KF，但如果上了这个CPU，就还需要配一块Z390主板，成本太高了就失去了黑苹果的意义了。\n当然AMD YES!!!\n我知道AMD的黑苹果可能遇到更多的问题，本着遇到问题解决问题的态度，还是硬着头皮上了。\n2. 主板 主板也是在华硕的Tuf B450 Pro Gaming和微星的迫击炮MAX之间纠结了很久，最终还是随大流选择了迫击炮。这里提个醒，3600原装的CPU散热器和内存插槽会有冲突，如果不让它和内存插槽有冲突，它就会和朝后的IO接口散热片有冲突。我只在A2和B2两个位置插了内存，所以还好。如果你用塔式散热器可能也不存在这个问题，但我不确定会不会出现其他的问题。\n3. 显卡 推荐的最多的就是RX 580 2304SP，但现在几乎已经买不到全新的满血版本了，一天在京东看到了盈通的RX 580 游戏高手，惊喜的发现竟然是满血版本，而且还是三风扇设计。由于我对显卡的性能要求不高，所以一般也不会满载，功耗什么的应该也没有太大的区别。虽然1119不是什么好价格（毕竟听说之前有蓝宝石999的RX 590 OC），但对比价格高了300块但性能差不多的RX 5500XT还是更合适一点。\n4. 内存 16G应该足够了，尤其是现在内存涨价那么厉害的时候，芝奇Snipper X 3200 8Gx2，迷彩马甲，看起来还可以。\n5. 其他 机箱和硬盘就没啥说的了，硬盘只要不选择三星PM981就行了。而且我这次也没有买硬盘。\n6. 机箱风扇 本来以为机箱风扇并不是必须用品，而且我也不玩游戏，想必机器的负载不会很高，但实际使用中发现玻璃侧板还是有点温热的。想来也是，因为CPU的风扇把热风吹到玻璃侧板上，GPU的风扇往下吹，不管怎么样都没有让热空气往外走的路径。于是在机箱背部和上部各安装了一把12厘米的风扇，效果立竿见影，玻璃侧板一直冷冰冰了。\n配置单 配件 型号 价格 CPU+主板 AMD 3600 + MSI Motar MAX 1919 显卡 盈通RX580游戏高手 1119 内存 芝奇 Snipper X 3200 8GB x 2 598 机箱 先马平头哥M1 159 硬盘 浦科特M6M（2014年的存货）+一块HGST的500G机械盘 0 优盘 闪迪Cruzer Glide 3.0 32GB 29.9 机箱风扇 追风者工包RGB 2把，型号未知 39.8 安装过程 1. 制作安装盘 格式化优盘\n虽然这个看起来是最简单的，但我偏偏是在这里出了问题。\n注意这两个选项，如果你默认选中的是第一个，则在抹掉优盘的时候就找不到分区表的方式，如下图所示\n如果选择了Show All Devices，就如下图所示\n2. 下载必备软件包 1. 下载操作系统 从Mac AppStore下载最新的macOS Catalina 10.15.3，然后执行\n1 sudo /Applications/Install\\ macOS\\ Catalina.app/Contents/Resources/createinstallmedia --volume /Volumes/MyVolume 2. 下载MountEFI 执行./MountEFI.command，会弹出\n正常制作完USB启动盘之后会有一个上图标红的挂载点，选它就行了。然后输入密码，这时在Finder里会看到有一个EFI挂载点，正常是空的。后面配置完OpenCore之后把EFI目录放在里面就行了。\n3. 配置config.plist 这是整个过程最复杂的一步了，但只要按照我在文章开头附上的链接一步步操作，肯定不会有问题。不得不说，那个教程真是严谨。甚至还有一个配置检查器，如果有问题可以提前发现，避免直接上机执行反复重启浪费时间。\n不过这里我就不详细描述了，只把最终我配置好的EFI目录放在这里，供大家直接使用。\n使用优盘启动 优盘插到台式机的USB上，启动时按F11选择启动项，选择那个很明显是优盘的选项。然后就开始跑代码了，之后就一步一步的进行安装过程。\n进入系统之后先不要做任何配置，因为你可能还没有安装成功。这时候系统还只能从优盘启动，如果拔了优盘，就看不到黑苹果的启动项了。\n免优盘启动 其实仔细想下就知道是因为你安装了黑苹果的磁盘没有优盘里面的那个EFI目录（当然不只是目录）。所以问题也就迎刃而解了。\n在新安装的黑苹果执行MountEFI，选择优盘，把优盘里的EFI目录整个复制到桌面上。然后把优盘卸载掉，再挂载你安装了黑苹果的系统盘。这时就看到了系统盘里面的EFI挂载点，然后把刚才放在桌面上的EFI目录放在系统盘的EFI挂载点里。\n这时你的系统就可以不用优盘启动了。\n你可能会问，为什么不能同时挂载两个，直接拖过去呢？放在桌面上这一步看起来很多余。其实上述的文档里专门提到了这一点，因为如果系统同时有两个EFI被挂载了，它可能就懵逼了，为了避免不必要的麻烦，还是事先麻烦一点好。\n不过这时候启动还是会跑代码，我觉得无所谓，就还没有花时间研究。\n总结 现在基本上能正常使用了，几个不太重要的问题还没有解决。\niService不能用，因为没有找到合适的Serial Number，也就是序列号。教程中的序列号生成器生成的序列号，去苹果官网上总是验证不通过，验证不通过也不影响使用AppleID，只是无法使用iMessage等我不用的服务，所以无所谓了。 启动时跑代码，这个应该就是一个DebugVerbose模式，我还没有仔细研究，无关紧要了。实际上只要在config.plist中找到boot-args，把后面的值里面的-v删掉就可以了。 我用的是有线网络，所以蓝牙、无线还未配置，也没有计划研究了。 补充 后来发现一些比较头疼的问题\n无法睡眠、休眠：点击睡眠后总是黑屏1秒后重新唤醒，不知道是不是什么驱动没有装好的问题，而且我也不知道台式机睡眠会是什么样子。。。 机箱前面板的USB2.0接口无法使用：根本不通电，检查了线缆应该是插错，由于没有Windows系统，也就无法验证。不过好在我也用不到这两个插口，所以无所谓了。 ","date":"2020-03-29T10:49:38+08:00","permalink":"https://techinsight.pages.dev/p/amd%E9%BB%91%E8%8B%B9%E6%9E%9C%E7%9A%84%E8%A3%85%E6%9C%BA%E7%BB%8F%E9%AA%8C/","title":"AMD黑苹果的装机经验"},{"content":"开发Java应用的过程中通常需要依赖大量的第三方包，而由于众所周知的原因，我们访问这些资源的速度非常慢，感谢阿里云给我们提供了一个选项可以快速访问这些资源。\n下面分别是使用maven和gradle时的配置。\nmaven 将以下内容写入$HOME/.m2/settings.xml中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \u0026lt;settings xmlns=\u0026#34;http://maven.apache.org/SETTINGS/1.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd\u0026#34;\u0026gt; \u0026lt;!-- \u0026lt;localRepository/\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;interactiveMode/\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;offline/\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;pluginGroups/\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;servers/\u0026gt; --\u0026gt; \u0026lt;mirrors\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;aliyunmaven\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; \u0026lt;name\u0026gt;阿里云公共仓库\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;https://maven.aliyun.com/repository/public\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt; \u0026lt;!-- \u0026lt;proxies/\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;profiles/\u0026gt; --\u0026gt; \u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;jdk-1.8\u0026lt;/id\u0026gt; \u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;jdk\u0026gt;1.8\u0026lt;/jdk\u0026gt; \u0026lt;/activation\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;maven.compiler.compilerVersion\u0026gt;1.8\u0026lt;/maven.compiler.compilerVersion\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt; \u0026lt;!-- \u0026lt;activeProfiles/\u0026gt; --\u0026gt; \u0026lt;/settings\u0026gt; gradle 单个项目 在buile.gradle中添加以下配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 buildscript { repositories { maven { url \u0026#39;https://maven.aliyun.com/repository/google/\u0026#39; } maven { url \u0026#39;https://maven.aliyun.com/repository/jcenter/\u0026#39;} } dependencies { classpath \u0026#39;com.android.tools.build:gradle:2.2.3\u0026#39; // NOTE: Do not place your application dependencies here; they belong // in the individual module build.gradle files } } allprojects { repositories { maven { url \u0026#39;https://maven.aliyun.com/repository/google/\u0026#39; } maven { url \u0026#39;https://maven.aliyun.com/repository/jcenter/\u0026#39;} } } 全局生效 将以下内容写入$HOME/.gradle/init.gradle中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 allprojects{ repositories { def ALIYUN_REPOSITORY_URL = \u0026#39;https://maven.aliyun.com/repository/public/\u0026#39; def ALIYUN_JCENTER_URL = \u0026#39;https://maven.aliyun.com/repository/jcenter/\u0026#39; def ALIYUN_GOOGLE_URL = \u0026#39;https://maven.aliyun.com/repository/google/\u0026#39; def ALIYUN_GRADLE_PLUGIN_URL = \u0026#39;https://maven.aliyun.com/repository/gradle-plugin/\u0026#39; all { ArtifactRepository repo -\u0026gt; if(repo instanceof MavenArtifactRepository){ def url = repo.url.toString() if (url.startsWith(\u0026#39;https://repo1.maven.org/maven2/\u0026#39;)) { project.logger.lifecycle \u0026#34;Repository ${repo.url} replaced by $ALIYUN_REPOSITORY_URL.\u0026#34; remove repo } if (url.startsWith(\u0026#39;https://jcenter.bintray.com/\u0026#39;)) { project.logger.lifecycle \u0026#34;Repository ${repo.url} replaced by $ALIYUN_JCENTER_URL.\u0026#34; remove repo } if (url.startsWith(\u0026#39;https://dl.google.com/dl/android/maven2/\u0026#39;)) { project.logger.lifecycle \u0026#34;Repository ${repo.url} replaced by $ALIYUN_GOOGLE_URL.\u0026#34; remove repo } if (url.startsWith(\u0026#39;https://plugins.gradle.org/m2/\u0026#39;)) { project.logger.lifecycle \u0026#34;Repository ${repo.url} replaced by $ALIYUN_GRADLE_PLUGIN_URL.\u0026#34; remove repo } } } maven { url ALIYUN_REPOSITORY_URL } maven { url ALIYUN_JCENTER_URL } maven { url ALIYUN_GOOGLE_URL } maven { url ALIYUN_GRADLE_PLUGIN_URL } } } ","date":"2020-03-29T00:21:43+08:00","permalink":"https://techinsight.pages.dev/p/%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91%E5%8A%A0%E9%80%9F%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/","title":"使用阿里云加速依赖管理"},{"content":"不得不说Hugo的图片路径支持有些不友好，网上也有很多吐槽。简单说就是即便神级的Markdown编辑器Typora都无法适应Hugo的图片路径。由于Typora需要做日常的工作记录，所以就配置了一下VSCode来支持Hugo。\nHugo支持两种放置本地图片的方式 本地图片是相对网络图片而言，如果你有图床也就无所谓是否相对路径了\ncontent目录下\n例如图片content/a.png，在文章content/post/a.md中引用就需要是![](/../a.png) static目录下\n例如图片static/images/a.png，在文章content/post/a.md中引用就需要是![](/images/a.png) 这里还是想吐槽一下，主要是第一种方式，既然在文章中是这样的写法，其实就已经默认是从【当前文章所在目录】向前查找了，那为什么不能放在当前文章目录下？\n配置VSCode支持两种方式 我还是比较倾向于内容和图片分离，所以就使用上述的第二种方式，方法确定了其实配置方式差别不大。\n依赖工具 VSCode 扩展Paste Image (作者 mushan） 配置步骤 配置图片文件存放路径 Paste Image: Path中配置 ${projectRoot}/static/images/\n配置粘贴到文章中的文本 Paste Image: Insert Pattern中配置 ${imageSyntaxPrefix}/images/${imageFileName}${imageSyntaxSuffix}\n这一点我没有仔细看文档，花费了一些时间。\n效果图如下\n存在的问题 当然这样配置还是解决不了【正常的Markdown】编辑器无法识别图片路径从而导致图片无法渲染的问题。但好在Hugo有一个不错的实时预览功能，弥补了这一点。\n","date":"2020-03-28T23:44:36+08:00","permalink":"https://techinsight.pages.dev/p/%E9%85%8D%E7%BD%AEhugo%E7%9A%84%E5%9B%BE%E7%89%87%E8%B7%AF%E5%BE%84/","title":"配置Hugo的图片路径"},{"content":"本文探讨了Springboot应用使用jar包和war包的区别，以及使用中的一些思考。我刚开始研究Java，一些想法可能不准确，欢迎提出宝贵意见。\n在开发中我们会使用嵌入式的tomcat容器，但实际项目部署中一般不会这么做，事实证明大部分都是这么用的，独立的tomcat部署已经被淘汰了。下面在macOS环境下操作以下步骤：\n由于实验用的黑苹果不支持docker，以下所有操作需要的应用均使用macOS下的homebrew安装。\n创建一个简单的Springboot Web应用 使用Spring Initializr创建一个基础的Springboot应用，只选择Web组件。\n以上就是一个最简单的Springboot应用了。\n在嵌入式tomcat容器中运行Web应用 可以看到，这个应用已经可以在嵌入式tomcat容器中运行了。注意，这里访问的路径是http://localhost:8080/v1/hello/world。\n打包编写完成的war包 在Idea中执行mvn pacakge，然后在target目录中检查生成的war包。\n将war包部署到独立的tomcat服务中 这时候就可以关闭Idea中运行的嵌入式tomcat容器了，因为启动独立tomcat服务时默认端口也是8080，会有冲突导致无法启动。\n可以执行brew services start tomcat来启动web容器。这里为了观察服务的输出，使用前台运行的方式catalina run。\n可以看到tomcat服务已经成功启动，并监听了8080端口。\n访问独立tomcat服务中的应用 将前面target目录中的war包部署到tomcat的webapps目录中。\n可以看到，服务启动后，直接将war包复制到tomcat的工作目录中，服务就会检测到新war包的加入，并自动运行相应的服务。\n这时如果我们还像刚才那样访问http://localhost:8080/v1/hello/world会怎样呢？\n可以看到，是不存在这个路径的。\n问题出在哪儿呢？我们看一下webapps目录下都有哪些东西。\n可以看到，我们是把应用部署在了web容器中，但web容器中却是有多个应用的，所以，访问应用时需要带上应用的名字。那名字是什么呢？当然就是spring-in-tomcat-0.0.1-SNAPSHOT，试一下\n果然可以了。\n访问应用的不同版本 刚才是应用从不存在到存在，tomcat可以自动检测。我们再测试一下是否可以检测文件的变更。\n这里做了一个微小的变化。\n复制完成之后tomcat马上就检测到了文件的更新。\n可以看到，应用更新也无感的完成了。\n是否真的是无感？ 在war包替换期间发生了什么？服务有没有中断呢？再做一个测试\n首先启动30秒的并发请求，然后将重新编辑并打包的war包重新部署，结果发现有大量的非200的返回值。这就证明了并不是“软重启”，而是存在服务中断。那怎么证明不是wrk发起的请求太多，从而导致的服务繁忙呢？在正常情况下再跑一次测试就行了。\n所以，重新部署服务导致服务中断的结论无误。\n从这个结论萌生了另外一个想法，这个访问的路径是带版本号的，这里是0.0.1-SNAPSHOT，那如果我直接加一个0.0.2-SNAPSHOT的版本进来，不就两个都能访问了？然后配合Nginx的反向代理和负载均衡，步进式的切流量，也就同时实现了灰度发布。\n在tomcat前部署nginx反向代理 添加一个如图的配置文件，这时就可以通过nginx访问spring的服务了。不出意外的话，改变nginx的配置并重新reload nginx的过程，服务是不会中断的。\n多次实验结果表明，在并发请求期间reload nginx的server配置，对服务可用性的影响非常小。\n关于Nginx的负载均衡相关内容这里不再过多涉及。\n总结 tomcat会自动加载新加入的war包 tomcat更新同名的新war包时服务会中断 可以利用tomcat可同时运行多个war包的特性提供不同版本的服务 可以利用Nginx反向代理实现服务不中断 可以利用Nginx的负载均衡实现灰度发布 ","date":"2020-03-28T12:56:22+08:00","permalink":"https://techinsight.pages.dev/p/springboot%E4%BD%BF%E7%94%A8%E5%86%85%E7%BD%AE%E5%92%8C%E7%8B%AC%E7%AB%8Btomcat%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%80%9D%E8%80%83/","title":"Springboot使用内置和独立tomcat以及其他思考"}]